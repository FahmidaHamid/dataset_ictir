An increasing fraction of web content is dynamically generated from back-end relational databases.
Even when database content remains unchanged, temporal locality of access cannot be exploited because dynamic content is not cacheable by web browsers or by intermediate caching servers such as Akamai mirrors.
In a multi-tiered architecture, each web request can stress the WAN link between the web server and the database.
This causes user experience to be highly variable because there is no caching to insu- This research was supported by the National Science Foundation (NSF) under grant number CCR-0205266.
Any opinions,  nd-ings, conclusions or recommendations expressed in this material are those of the authors and do not necessarily re ect the views of the NSF or Carnegie Mellon University.
Copyright is held by the International World Wide Web Conference Committee (IW3C2).
Distribution of these papers is limited to classroom use, and personal use by others.
late the client from bursty loads.
Previous attempts in caching dynamic database content have generally weakened transactional semantics [3, 4] or required application modi cations [15, 34].
We report on a new solution that takes the form of a database-agnostic middleware layer called Ganesh.
Ganesh makes no effort to semantically interpret the contents of queries or their results.
Instead, it relies exclusively on cryptographic hashing to detect similarities with previous results.
Hash-based similarity detection has seen increasing use in distributed  le systems [26, 36, 37] for improving performance on low-bandwidth networks.
However, these techniques have not been used for relational databases.
Unlike previous approaches that use generic methods to detect similarity, Ganesh exploits the structure of relational database results to yield superior performance improvement.
One faces at least three challenges in applying hash-based similarity detection to back-end databases.
First, previous work in this space has traditionally viewed storage content as uninterpreted bags of bits with no internal structure.
This allows hash-based techniques to operate on long, contiguous runs of data for maximum effectiveness.
In contrast, relational databases have rich internal structure that may not be as amenable to hash-based similarity detection.
Second, relational databases have very tight integrity and consistency constraints that must not be compromised by the use of hash-based techniques.
Third, the source code of commercial databases is typically not available.
This is in contrast to previous work which presumed availability of source code.
Our experiments show that Ganesh, while conceptually simple, can improve performance signi cantly at bandwidths representative of today s commercial Internet.
On benchmarks modeling multi-tiered web applications, the throughput improvement was as high as tenfold for data-intensive workloads.
For workloads that were not data-intensive, throughput improvements of up to twofold were observed.
Even when bandwidth was not a constraint, Ganesh had low overhead and did not hurt performance.
Our experiments also con rm that exploiting the structure present in database results is crucial to this performance improvement.
As the World Wide Web has grown, many web sites have decentralized their data and functionality by pushing them to the edges of the Internet.
Today, eBusiness systems often use a three-tiered architecture consisting of a front-end web server, an application server, and a back-end database server.
Figure 1 illustrates this architecture.
The  rst two tiers can be replicated close to a concentration of clients at the edge of the Internet.
This improves user experience by lowering end-to-end latency and reducing exposure Ganesh exploits redundancy in the result stream to avoid transmitting result fragments that are already present at the query site.
Redundancy can arise naturally in many different ways.
For example, a query repeated after a certain interval may return a different result because of updates to the database; however, there may be signi cant commonality between the two results.
As another example, a user who is re ning a search may generate a sequence of queries with overlapping results.
When Ganesh detects redundancy, it suppresses transmission of the corresponding result fragments.
Instead, it transmits a much smaller digest of those fragments and lets the query site reconstruct the result through hash lookup in a cache of previous results.
In effect, Ganesh uses computation at the edges to reduce Internet communication.
Our description of Ganesh focuses on four aspects.
We  rst explain our approach to detecting similarity in query results.
Next, we discuss how the Ganesh architecture is completely invisible to all components of a multi-tier system.
We then describe Ganesh s proxy-based approach and the data ow for detecting similarity.
One of the key design decisions in Ganesh is how similarity is detected.
There are many potential ways to decompose a result into fragments.
The optimal way is, of course, the one that results in the smallest possible object for transmission for a given query s results.
Finding this optimal decomposition is a dif cult problem because of the large space of possibilities and because the optimal choice depends on many factors such as the contents of the query s result, the history of recent results, and the cache management algorithm.
When an object is opaque, the use of Rabin  ngerprints [8, 30] to detect common data between two objects has been successfully shown in the past by systems such as LBFS [26] and CASPER [37].
Rabin  ngerprinting uses a sliding window over the data to compute a rolling hash.
Assuming that the hash function is uniformly distributed, a chunk boundary is de ned whenever the lower order bits of the hash value equal some predetermined value.
The number of lower order bits used de nes the average chunk size.
These subdivided chunks of the object become the unit of comparison for detecting similarity between different objects.
As the locations of boundaries found by using Rabin  ngerprints is stochastically determined, they usually fail to align with any structural properties of the underlying data.
The algorithm therefore deals well with in-place updates, insertions and deletions.
However, it performs poorly in the presence of any reordering of data.
Figure 2 shows an example where two results, A and B, consisting of three rows, have the same data but have different sort attributes.
In the extreme case, Rabin  ngerprinting might be unable to  nd any similar data due to the way it detects chunk boundaries.
Fortunately, Ganesh can use domain speci c knowledge for more precise boundary detection.
The information we exploit is that a query s result re ects the structure of a relational database where all data is organized as tables and rows.
It is therefore simple to check for similarity with previous results at two granularities:  rst the entire result, and then individual rows.
The end of a row in a result serves as a natural chunk boundary.
It is important to note that using the tabular structure in results only involves shallow interpretation of the data.
Ganesh does not perform any deeper semantic interpretation such as understanding data types, result schema, or integrity constraints.
Tuning Rabin  ngerprinting for a workload can also be dif cult.
If the average chunk size is too large, chunks can span multiple result rows.
However, selecting a smaller average chunk size increases the amount of metadata required to the describe the results.
Figure 1: Multi-Tier Architecture to backbone traf c congestion.
It can also increase the availability and scalability of web services.
Content that is generated dynamically from the back-end database cannot be cached in the  rst two tiers.
While databases can be easily replicated in a LAN, this is infeasible in a WAN because of the dif cult task of simultaneously providing strong consistency, availability, and tolerance to network partitions [7].
As a result, databases tend to be centralized to meet the strong consistency requirements of many eBusiness applications such as banking,  -nance, and online retailing [38].
Thus, the back-end database is usually located far from many sets of  rst and second-tier nodes [2].
In the absence of both caching and replication, WAN bandwidth can easily become a limiting factor in the performance and scala-bility of data-intensive applications.
Ganesh s focus is on ef cient transmission of results by discovering similarities with the results of previous queries.
As SQL queries can generate large results, hash-based techniques lend themselves well to the problem of ef ciently transferring these large results across bandwidth constrained links.
The use of hash-based techniques to reduce the volume of data transmitted has emerged as a common theme of many recent storage systems, as discussed in Section 8.2.
These techniques rely on some basic assumptions.
Cryptographic hash functions are assumed to be collision-resistant.
In other words, it is computationally intractable to  nd two inputs that hash to the same output.
The functions are also assumed to be one-way; that is,  nding an input that results in a speci c output is computationally infeasible.
Menezes et al. [23] provide more details about these assumptions.
The above assumptions allow hash-based systems to assume that collisions do not occur.
Hence, they are able to treat the hash of a data item as its unique identi er.
A collection of data items effectively becomes content-addressable, allowing a small hash to serve as a codeword for a much larger data item in permanent storage or network transmission.
The assumption that collisions are so rare as to be effectively nonexistent has recently come under  re [17].
However, as explained by Black [5], we believe that these issues do not form a concern for Ganesh.
All communication is between trusted parts of the system and an adversary has no way to force Ganesh to accept invalid data.
Further, Ganesh does not depend critically on any speci c hash function.
While we currently use SHA-1, replacing it with a different hash function would be simple.
There would be no impact on performance as stronger hash functions (e.g.
SHA-256) only add a few extra bytes and the generated hashes are still orders of magnitude smaller than the data items they represent.
No rehashing of permanent storage is required since Ganesh only uses hashing on volatile data.
Back-End Database ServerFront-End Web andApplication ServersWWW 2007 / Track: Performance and ScalabilitySession: Scalable Systems for Dynamic Content312Figure 2: Rabin Fingerprinting vs. Ganesh s Chunking This, in turn, would decrease the savings obtained via its use.
Rabin  ngerprinting also needs two computationally-expensive passes over the data: once to determine chunk boundaries and one again to generate cryptographic hashes for the chunks.
Ganesh only needs a single pass for hash generation as the chunk boundaries are provided by the data s natural structure.
The performance comparison in Section 6 shows that Ganesh s row-based algorithm outperforms Rabin  ngerprinting.
Given that previous work has already shown that Rabin  ngerprinting performs better than gzip [26], we do not compare Ganesh to compression algorithms in this paper.
The key factor in uencing our design was the need for Ganesh to be completely transparent to all components of a typical eBusi-ness system: web servers, application servers, and database servers.
Without this, Ganesh stands little chance of having a signi cant real-world impact.
Requiring modi cations to any of the above components would raise the barrier for entry of Ganesh into an existing system, and thus reduce its chances of adoption.
Preserving transparency is simpli ed by the fact that Ganesh is purely a performance enhancement, not a functionality or usability enhancement.
We chose agent interposition as the architectural approach to realizing our goal.
This approach relies on the existence of a compact programming interface that is already widely used by target software.
It also relies on a mechanism to easily add new code without disrupting existing module structure.
These conditions are easily met in our context because of the popularity of Java as the programming language for eBusiness systems.
The Java Database Connectivity (JDBC) API [32] allows Java applications to access a wide variety of databases and even other tabular data repositories such as  at  les.
Access to these data sources is provided by JDBC drivers that translate between the JDBC API and the database communication mechanism.
Figure 3(a) shows how JDBC is typically used in an application.
As the JDBC interface is standardized, one can substitute one JDBC driver for another without application modi cations.
The JDBC driver thus becomes the natural module to exploit for code interposition.
As shown in Figure 3(b), the native JDBC driver is replaced with a Ganesh JDBC driver that presents the same standardized interface.
The Ganesh driver maintains an in-memory cache of result fragments from previous queries and performs reassembly of results.
At the database, we add a new process called the Ganesh proxy.
This proxy, which can be shared by multiple front-end nodes, consists of two parts: code to detect similarity in result fragments and the original native JDBC driver that communicates with the database.
The use of a proxy at the database makes Ganesh database-agnostic and simpli es prototyping and experimentation.
Ganesh is thus able to work with a wide range of databases and applications, requiring no modi cations to either.
The native JDBC driver shown in Figure 3(a) is a lightweight code component supplied by the database vendor.
Its main func-(a) Native Architecture (b) Ganesh s Interposition-based Architecture Figure 3: Native vs. Ganesh Architecture tion is to mediate communication between the application and the remote database.
It forwards queries, buffers entire results, and responds to application requests to view parts of results.
The Ganesh JDBC driver shown in Figure 3(b) presents the application with an interface identical to that provided by the native driver.
It provides the ability to reconstruct results from compact hash-based descriptions sent by the proxy.
To perform this reconstruction, the driver maintains an in-memory cache of recently-received results.
This cache is only used as a source of result fragments in reconstructing results.
No attempt is made by the Ganesh driver or proxy to track database updates.
The lack of cache consistency does not hurt correctness as a description of the results is always fetched from the proxy   at worst, there will be no performance bene t from using Ganesh.
Stale data will simply be paged out of the cache over time.
The Ganesh proxy accesses the database via the native JDBC driver, which remains unchanged between Figures 3(a) and (b).
The database is thus completely unaware of the existence of the proxy.
The proxy does not examine any queries received from the Ganesh driver but passes them to the native driver.
Instead, the proxy is responsible for inspecting database output received from the native driver, detecting similar results, and generating hash-based encodings of these results whenever enough similarity is found.
While this architecture does not decrease the load on a database, as mentioned earlier in Section 2.1, it is much easier to replicate databases for scalability in a LAN than in a WAN.
To generate a hash-based encoding, the proxy must be aware of what result fragments are available in the Ganesh driver s cache.
One approach is to be optimistic, and to assume that all result fragments are available.
This will result in the smallest possible initial transmission of a result.
However, in cases where there is little overlap with previous results, the Ganesh driver will have to make many calls to the proxy during reconstruction to fetch missing result fragments.
To avoid this situation, the proxy loosely tracks the state of the Ganesh driver s cache.
Since both components are under our control, it is relatively simple to do this without resorting to gray-box techniques or explicit communication for maintaining cache coherence.
Instead, the proxy simulates the Ganesh driver s cache management algorithm and uses this to maintain a list of hashes for which the Ganesh driver is likely to possess the result fragments.
In case of mistracking, there will be no loss of correctness but there will be extra round-trip delays to fetch the missing fragments.
If the client detects loss of synchronization with the proxy, it can ask the proxy to reset the state shared between them.
Also note that the proxy does not need to keep the result fragments themselves, only their hashes.
This allows the proxy to remain scalable even when it is shared by many front-end nodes.
ClientDatabaseWeb and Application ServerNative JDBC DriverWANClientDatabaseGanesh ProxyNative JDBC DriverWANWeb and Application ServerGanesh JDBC DriverWWW 2007 / Track: Performance and ScalabilitySession: Scalable Systems for Dynamic Content313Benchmark Dataset Details






 Table 1: Benchmark Dataset Details   Second, how important is Ganesh s structural similarity detection relative to Rabin  ngerprinting s similarity detection?
  Third, is the overhead of the proxy-based design acceptable?
Our evaluation answers these question through controlled experiments with the Ganesh prototype.
This section describes the benchmarks used, our evaluation procedure, and the experimental setup.
Results of the experiments are presented in Sections 5, 6, and 7.
Our evaluation is based on two benchmarks [18] that have been widely used by other researchers to evaluate various aspects of multi-tier [27] and eBusiness architectures [9].
The  rst benchmark, BBOARD, is modeled after Slashdot, a technology-oriented news site.
The second benchmark, AUCTION, is modeled after eBay, an online auction site.
In both benchmarks, most content is dynamically generated from information stored in a database.
Details of the datasets used can be found in Table 1.
The BBOARD benchmark, also known as RUBBoS [18], models Slashdot, a popular technology-oriented web site.
Slashdot aggregates links to news stories and other topics of interest found elsewhere on the web.
The site also serves as a bulletin board by allowing users to comment on the posted stories in a threaded conversation form.
It is not uncommon for a story to gather hundreds of comments in a matter of hours.
The BBOARD benchmark is similar to the site and models the activities of a user, including read-only operations such as browsing the stories of the day, browsing story categories, and viewing comments as well as write operations such as new user registration, adding and moderating comments, and story submission.
The benchmark consists of three different phases: a short warm-up phase, a runtime phase representing the main body of the workload, and a short cool-down phase.
In this paper we only report results from the runtime phase.
The warm-up phase is important in establishing dynamic system state, but measurements from that phase are not signi cant for our evaluation.
The cool-down phase is solely for allowing the benchmark to shut down.
The warm-up, runtime, and cool-down phases are 2, 15, and 2 minutes respectively.
The number of simulated clients were 400,
 and PHP version and has different datasets; we evaluated Ganesh using the Java Servlets version and the Expanded dataset.
The BBOARD benchmark de nes two different workloads.
The  rst, the Authoring mix, consists of 70% read-only operations and
 tains only read-only operations and does not update the database.
The AUCTION benchmark, also known as RUBiS [18], models eBay, the online auction site.
The eBay web site is used to buy and sell items via an auction format.
The main activities of a user include browsing, selling, or bidding for items.
Modeling the activities on this site, this benchmark includes read-only activities such as browsing items by category and by region, as well as read-write Figure 4: Data ow for Result Handling
 The Ganesh proxy receives database output as Java objects from the native JDBC driver.
It examines this output to see if a Java object of type ResultSet is present.
The JDBC interface uses this data type to store results of database queries.
If a ResultSet object is found, it is shrunk as discussed below.
All other Java objects are passed through unmodi ed.
As discussed in Section 3.1, the proxy uses the row boundaries de ned in the ResultSet to partition it into fragments consisting of single result rows.
All ResultSet objects are converted into objects of a new type called RecipeResultSet.
We use the term  recipe  for this compact description of a database result because of its similarity to a  le recipe in the CASPER  le system [37].
The conversion replaces each result fragment that is likely to be present in the Ganesh driver s cache by a SHA-1 hash of that fragment.
Previously unseen result fragments are retained verbatim.
The proxy also retains hashes for the new result fragments as they will be present in the driver s cache in the future.
Note that the proxy only caches hashes for result fragments and does not cache recipes.
The proxy constructs a RecipeResultSet by checking for similarity at the entire result and then the row level.
If the entire result is predicted to be present in the Ganesh driver s cache, the RecipeResultSet is simply a single hash of the entire result.
Otherwise, it contains hashes for those rows predicted to be present in that cache; all other rows are retained verbatim.
If the proxy estimates an overall space savings, it will transmit the RecipeRe-sultSet.
Otherwise the original ResultSet is transmitted.
The RecipeResultSet objects are transformed back into Re-sultSet objects by the Ganesh driver.
Figure 4 illustrates Re-sultSet handling at both ends.
Each SHA-1 hash found in a RecipeResultSet is looked up in the local cache of result fragments.
On a hit, the hash is replaced by the corresponding fragment.
On a miss, the driver contacts the Ganesh proxy to fetch the fragment.
All previously unseen result fragments that were retained verbatim by the proxy are hashed and added to the result cache.
There should be very few misses if the proxy has accurately tracked the Ganesh driver s cache state.
A future optimization would be to batch the fetch of missing fragments.
This would be valuable when there are many small missing fragments in a high-latency WAN.
Once the transformation is complete, the fully reconstructed ResultSet object is passed up to the application.
Three questions follow from the goals and design of Ganesh:   First, can performance can be improved signi cantly by exploiting similarity across database results?
Object Output StreamConvert ResultSetObject Input StreamConvert ResultSetAll DataRecipe ResultSetAll DataResultSetNetworkGanesh ProxyGanesh JDBC DriverResultSetRecipe Result SetYesYesNoNoGanesh Input StreamGanesh Output StreamWWW 2007 / Track: Performance and ScalabilitySession: Scalable Systems for Dynamic Content314Figure 5: Experimental Setup activities such as bidding for items, buying and selling items, and leaving feedback.
As with BBOARD, the benchmark consists of three different phases.
The warm-up, runtime, and cool-down phases for this experiment are 1.5, 15, and 1 minutes respectively.
We tested Ganesh with four client con gurations where the number of test clients was set to 400, 800, 1200, and 1600.
The benchmark is available in a Enterprise Java Bean (EJB), Java Servlets, and PHP version and has different datasets; we evaluated Ganesh with the Java Servlets version and the Expanded dataset.
The AUCTION benchmark de nes two different workloads.
The  rst, the Bidding mix, consists of 70% read-only operations and
 tains only read-only operations and does not update the database.
Both benchmarks involve a synthetic workload of clients accessing a web server.
The number of clients emulated is an experimental parameter.
Each emulated client runs an instance of the benchmark in its own thread, using a matrix to transition between different benchmark states.
The matrix de nes a stochastic model with probabilities of transitioning between the different states that represent typical user actions.
An example transition is a user logging into the AUCTION system and then deciding on whether to post an item for sale or bid on active auctions.
Each client also models user think time between requests.
The think time is modeled as an exponential distribution with a mean of 7 seconds.
We evaluate Ganesh along two axes: number of clients and WAN bandwidth.
Higher loads are especially useful in understanding Ganesh s performance when the CPU or disk of the database server or proxy is the limiting factor.
A previous study has shown that approximately 50% of the wide-area Internet bottlenecks observed had an available bandwidth under 10 Mb/s [1].
Based on this work, we focus our evaluation on the WAN bandwidth of 5 Mb/s with 66 ms of round-trip latency, representative of severely constrained network paths, and 20 Mb/s with 33 ms of round-trip latency, representative of a moderately constrained network path.
We also report Ganesh s performance at 100 Mb/s with no added round-trip latency.
This bandwidth, representative of an unconstrained network, is especially useful in revealing any potential overhead of Ganesh in situations where WAN bandwidth is not the limiting factor.
For each combination of number of clients and WAN bandwidth, we measured results from the two con gurations listed below:   Native: This con guration corresponds to Figure 3(a).
Native avoids Ganesh s overhead in using a proxy and performing Java object serialization.
  Ganesh: This con guration corresponds to Figure 3(b).
For a given number of clients and WAN bandwidth, comparing these results to the corresponding Native results gives the performance bene t due to the Ganesh middleware system.
The metric used to quantify the improvement in throughput is the number of client requests that can be serviced per second.
The metric used to quantify Ganesh s overhead is the average response time for a client request.
For all of the experiments, the Ganesh driver used by the application server used a cache size of 100,000 items1.
The proxy was effective in tracking the Ganesh driver s cache state; for all of our experiments the miss rate on the driver never exceeded 0.7%.
The experimental setup used for the benchmarks can be seen in Figure 5.
All machines were 3.2 GHz Pentium 4s (with Hyper-Threading enabled.)
With the exception of the database server, all machines had 2 GB of SDRAM and ran the Fedora Core Linux distribution.
The database server had 4 GB of SDRAM.
We used Apache s Tomcat as both the application server that hosted the Java Servlets and the web server.
Both benchmarks used Java Servlets to generate the dynamic content.
The database server used the open source MySQL database.
For the native JDBC drivers, we used the Connector/J drivers provided by MySQL.
The application server used Sun s Java Virtual Machine as the runtime environment for the Java Servlets.
The sysstat tool was used to monitor the CPU, network, disk, and memory utilization on all machines.
The machines were connected by a switched gigabit Ethernet network.
As shown in Figure 5, the front-end web and application server was separated from the proxy and database server by a NetEm router [16].
This router allowed us to control the bandwidth and latency settings on the network.
The NetEm router is a standard PC with two network cards running the Linux Traf c Control and Network Emulation software.
The bandwidth and latency constraints were only applied to the link between the application server and the database for the native case and between the application server and the proxy for the Ganesh case.
There is no communication between the application server and the database with Ganesh as all data  ows through the proxy.
As our focus was on the WAN link between the application server and the database, there were no constraints on the link between the simulated test clients and the web server.
In this section, we address the  rst question raised in Section 4: Can performance can be improved signi cantly by exploiting similarity across database results?
To answer this question, we use results from the BBOARD and AUCTION benchmarks.
We use two metrics to quantify the performance improvement obtainable through the use of Ganesh: throughput, from the perspective of the web server, and average response time, from the perspective of the client.
Throughput is measured in terms of the number of client requests that can be serviced per second.
Figures 6 (a) and (b) present the average number of requests serviced per second and the average response time for these requests as perceived by the clients for BBOARD s Authoring Mix.
As Figure 6 (a) shows, Native easily saturates the 5 Mb/s link.
At 400 clients, the Native solution delivers 29 requests/sec with an average response time of 8.3 seconds.
Native s throughput drops with an increase in test clients as clients timeout due to congestion at the application server.
Usability studies have shown that response times above 10 seconds cause the user to move on to
 their size based on the number of objects.
The size of cache dumps taken at the end of the experiments never exceeded 212 MB.
NetEmRouterGaneshProxyClientsWeb andApplication ServerDatabase ServerWWW 2007 / Track: Performance and ScalabilitySession: Scalable Systems for Dynamic Content315(a) Throughput: Authoring Mix (b) Response Time: Authoring Mix (c) Throughput: Browsing Mix (d) Response Time: Browsing Mix Mean of three trials.
The maximum standard deviation for throughput and response time was 9.8% and 11.9% of the corresponding mean.
Figure 6: BBOARD Benchmark - Throughput and Average Response Time clients.
Even with the 1600 test client con guration Ganesh delivers an acceptable average response time of 8.2 seconds.
other tasks [24].
Based on these numbers, increasing the number of test clients makes the Native system unusable.
Ganesh at
 clients and a  vefold improvement at 1200 clients.
Ganesh s performance drops slightly at 1200 and 1600 clients as the network is saturated.
Compared to Native, Figure 6 (b) shows that Ganesh s response times are substantially lower with sub-second response times at 400 clients.
Figure 6 (a) also shows that for 400 and 800 test clients Ganesh at 5 Mb/s has the same throughput and average response time as Native at 20 Mb/s.
Only at 1200 and 1600 clients does Native at 20 Mb/s deliver higher throughput than Ganesh at 5 Mb/s.
Comparing both Ganesh and Native at 20 Mb/s, we see that Ganesh is no longer bandwidth constrained and delivers up to a twofold improvement over Native at 1600 test clients.
As Ganesh does not saturate the network with higher test client con gurations, at 1600 test clients, its average response time is 0.1 seconds rather than Native s 7.7 seconds.
As expected, there are no visible gains from Ganesh at the higher bandwidth of 100 Mb/s where the network is no longer the bottleneck.
Ganesh, however, still tracks Native in terms of throughput.
Figures 6 (c) and (d) present the average number of requests serviced per second and the average response time for these requests as perceived by the clients for BBOARD s Browsing Mix.
Regardless of the test client con guration, Figure 6 (c) shows that Native s throughput at 5 Mb/s is limited to 10 reqs/sec.
Ganesh at 5 Mb/s with 400 test clients, delivers more than a sixfold increase in throughput.
The improvement increases to over a eleven-fold increase at 800 test clients before Ganesh saturates the network.
Further, Figure 6 (d) shows that Native s average response time of 35 seconds at 400 test clients make the system unusable.
These high response times further increase with the addition of test Due to the data-intensive nature of the Browsing mix, Ganesh at
 Further, as shown in Figure 6 (d), while the average response time for Native at 20 Mb/s is acceptable at 400 test clients, it is unusable with 800 test clients with an average response time of 15.8 seconds.
Like the 5 Mb/s case, this response time increases with the addition of extra test clients.
Ganesh at 20 Mb/s and both Native and Ganesh at 100 Mb/s are not bandwidth limited.
However, performance plateaus out after

 We were surprised by the Native performance from the BBOARD benchmark.
At the bandwidth of 5 Mb/s, Native performance was lower than what we had expected.
It turned out the benchmark code that displays stories read all the comments associated with the particular story from the database and only then did some post-processing to select the comments to be displayed.
While this is exactly the behavior of SlashCode, the code base behind the Slashdot web site, we decided to modify the benchmark to perform some pre- ltering at the database.
This modi ed benchmark, named the Filter Variant, models a developer who applies optimizations at the SQL level to transfer less data.
In the interests of brevity, we only brie y summarize the results from the Authoring mix.
For the Authoring mix, at 800 test clients at 5 Mb/s, Figure 7 (a) shows that Native s throughput increase by 85% when compared to the original benchmark while Ganesh s improvement is smaller at 15%.
Native s performance drops above 800 clients as the test clients time out due to high response times.
The most signi cant gain for Native is seen at 20 Mb/s.
At 1600 test clients, when compared to the original benchmark, Native sees a 73% improvement in throughput and a 77% reduction in average response time.
While
 (b) Response Time: Authoring Mix Mean of three trials.
The maximum standard deviation for throughput and response time was 7.2% and 11.5% of the corresponding mean.
Figure 7: BBOARD Benchmark - Filter Variant - Throughput and Average Response Time Ganesh sees no improvement when compared to the original, it still processes 19% more requests/sec than Native.
Thus, while the optimizations were more helpful to Native, Ganesh still delivers an improvement in performance.
Figures 8 (a) and (b) present the average number of requests serviced per second and the average response time for these requests as perceived by the clients for AUCTION s Bidding Mix.
As mentioned earlier, the Bidding mix consists of a mixture of read and write operations.
The AUCTION benchmark is not as data intensive as BBOARD.
Therefore, most of the gains are observed at the lower bandwidth of 5 Mb/s.
Figure 8 (a) shows that the increase in throughput due to Ganesh ranges from 8% at 400 test clients to 18% with 1600 test clients.
As seen in Figure 8 (b), the average response times for Ganesh are signi cantly lower than Native ranging from a decrease of 84% at 800 test clients to 88% at 1600 test clients.
Figure 8 (a) also shows that with a fourfold increase of bandwidth from 5 Mb/s to 20 Mb/s, Native is no longer bandwidth constrained and there is no performance difference between Ganesh and Native.
With the higher test client con gurations, we did observe that the bandwidth used by Ganesh was lower than Native.
Ganesh might still be useful in these non-constrained scenarios if bandwidth is purchased on a metered basis.
Similar results are seen for the 100 Mb/s scenario.
For AUCTION s Browsing Mix, Figures 8 (c) and (d) present the average number of requests serviced per second and the average response time for these requests as perceived by the clients.
Again, most of the gains are observed at lower bandwidths.
At 5 Mb/s, Native and Ganesh deliver similar throughput and response times with 400 test clients.
While the throughput for both remains the same at 800 test clients, Figure 8 (d) shows that Ganesh s average response time is 62% lower than Native.
Native saturates the link at 800 clients and adding extra test clients only increases the average response time.
Ganesh, regardless of the test client con g-uration, is not bandwidth constrained and maintains the same response time.
At 1600 test clients, Figure 8 (c) shows that Ganesh s throughput is almost twice that of Native.
At the higher bandwidths of 20 and 100 Mb/s, neither Ganesh nor Native is bandwidth limited and deliver equivalent throughput and response times.
Benchmark Orig.
Size Ganesh Size Rabin Size

 SelectSort1 SelectSort2









 Table 2: Similarity Microbenchmarks

 In this section, we address the second question raised in Section 4: How important is Ganesh s structural similarity detection relative to Rabin  ngerprinting-based similarity detecting?
To answer this question, we used microbenchmarks and the BBOARD and AUCTION benchmarks.
As Ganesh always performed better than Rabin  ngerprinting, we only present a subset of the results here in the interests of brevity.
Two microbenchmarks show an example of the effects of data reordering on Rabin  ngerprinting algorithm.
In the  rst micro-benchmark, SelectSort1, a query with a speci ed sort order selects
 query is then repeated with a different sort attribute.
While the same number of rows and the same data is returned, the order of rows is different.
In such a scenario, one would expect a large amount of similarity to be detected between both results.
As Table 2 shows, Ganesh s row-based algorithm achieves a 97.6% reduction while the Rabin  ngerprinting algorithm, with the average chunk size parameter set to 4 KB, only achieves a 1% reduction.
The reason, as shown earlier in Figure 2, is that with Rabin  n-gerprinting, the spans of data between two consecutive boundaries usually cross row boundaries.
With the order of the rows changing in the second result and the Rabin  ngerprints now spanning different rows, the algorithm is unable to detect signi cant similarity.
The small gain seen is mostly for those single rows that are large enough to be broken into multiple chunks.
SelectSort2, another micro-benchmark executed the same queries but increased the minimum chunk size of the Rabin  ngerprinting algorithm.
As can be seen in Table 2, even the small gain from the previous microbenchmark disappears as the minimum chunk size was greater than the average row size.
While one can partially address these problems by dynamically varying the parameters of the Rabin  ngerprinting algorithm, this can be computationally expensive, especially in the presence of changing workloads.
We ran the BBOARD benchmark described in Section 4.1.1 on two versions of Ganesh: the  rst with Rabin  ngerprinting used as
 (b) Response Time: Bidding Mix (c) Throughput: Browsing Mix (d) Response Time: Browsing Mix Mean of three trials.
The maximum standard deviation for throughput and response time was 2.2% and 11.8% of the corresponding mean.
Figure 8: AUCTION Benchmark - Throughput and Average Response Time

 the chunking algorithm and the second with Ganesh s row-based algorithm.
Rabin s results for the Browsing Mix are normalized to Ganesh s results and presented in Figure 9.
As Figure 9 (a) shows, at 5 Mb/s, independent of the test client con guration, Rabin signi cantly underperforms Ganesh.
This happens because of a combination of two reasons.
First, as outlined in Section 3.1, Rabin  nds less similarity as it does not exploit the result s structural information.
Second, this benchmark contained some queries that generated large results.
In this case, Rabin, with a small average chunk size, generated a large number of objects that evicted other useful data from the cache.
In contrast, Ganesh was able to detect these large rows and correspondingly increase the size of the chunks.
This was con rmed as cache statistics showed that Ganesh s hit ratio was roughly three time that of Rabin.
Throughput measurements at 20 Mb/s were similar with the exception of Rabin s performance with 400 test clients.
In this case, Ganesh was not network limited and, in fact, the throughput was the same as 400 clients at 5 Mb/s.
Rabin, however, took advantage of the bandwidth increase from 5 to 20 Mb/s to deliver a slightly better performance.
At 100 Mb/s, Rabin s throughput was almost similar to Ganesh as bandwidth was no longer a bottleneck.
The normalized response time, presented in Figure 9 (b), shows similar trends.
At 5 and 20 Mb/s, the addition of test clients decreases the normalized response time as Ganesh s average response time increases faster than Rabin s.
However, at no point does Rabin outperform Ganesh.
Note that at 400 and 800 clients at 100 Mb/s, Rabin does have a higher overhead even when it is not bandwidth constrained.
As mentioned in Section 3.1, this is due to the fact that Rabin has to hash each ResultSet twice.
The overhead disappears with 1200 and 1600 clients as the database CPU is saturated and limits the performance of both Ganesh and Rabin.
In this section, we address the third question raised in Section 4: Is the overhead of Ganesh s proxy-based design acceptable?
To answer this question, we concentrate on its performance at the higher bandwidths.
Our evaluation in Section 5 showed that Ganesh, when compared to Native, can deliver a substantial throughput improvement at lower bandwidths.
It is only at higher bandwidths that latency, measured by the average response time for a client request, and throughput, measured by the number of client requests that can be serviced per second, overheads would be visible.
Looking at the Authoring mix of the original BBOARD benchmark, there are no visible gains from Ganesh at 100 Mb/s.
Ganesh, however, still tracks Native in terms of throughput.
While the average response time is higher for Ganesh, the absolute difference is in between 0.01 and 0.04 seconds and would be imperceptible to the end-user.
The Browsing mix shows an even smaller difference in average response times.
The results from the  lter variant of the BBOARD benchmarks are similar.
Even for the AUCTION benchmark, the difference between Native and Ganesh s response time at
 to the above results was seen in the  lter variant of the BBOARD benchmark where Ganesh at 1600 test clients added 0.85 seconds to the average response time.
Thus, even for much faster networks where the WAN link is not the bottleneck, Ganesh always delivers throughput equivalent to Native.
While some extra latency is added by the proxy-based design, it is usually imperceptible.
To the best of our knowledge, Ganesh is the  rst system that combines the use of hash-based techniques with caching of database results to improve throughput and response times for applications with dynamic content.
We also believe that it is also the  rst system to demonstrate the bene ts of using structural information for
 (b) Normalized Response Time: Higher is worse For throughput, a normalized result greater than 1 implies that Rabin is better, For response time, a normalized result greater than 1 implies that Ganesh is better.
Mean of three trials.
The maximum standard deviation for throughput and response time was 9.1% and 13.9% of the corresponding mean.
Figure 9: Normalized Comparison of Ganesh vs. Rabin - BBOARD Browsing Mix detecting similarity.
In this section, we  rst discuss alternative approaches to caching dynamic content and then examine other uses of hash-based primitives in distributed systems.
At the database layer, a number of systems have advocated middle-tier caching where parts of the database are replicated at the edge or server [3, 4, 20].
These systems either cache entire tables in what is essentially a replicated database or use materialized views from previous query replies [19].
They require tight integration with the back-end database to ensure a time bound on the propagation of updates.
These systems are also usually targeted towards workloads that do not require strict consistency and can tolerate stale data.
Further, unlike Ganesh, some of these mid-tier caching solutions [2, 3], suffer from the complexity of having to participate in query planing and distributed query processing.
Gao et al. [15] propose using a distributed object replication architecture where the data store s consistency requirements are adapted on a per-application basis.
These solutions require substantial developer resources and detailed understanding of the application being modi ed.
While systems that attempt to automate the partitioning and replication of an application s database exist [34], they do not provide full transaction semantics.
In comparison, Ganesh does not weaken any of the semantics provided by the underlying database.
Recent work in the evaluation of edge caching options for dynamic web sites [38] has suggested that, without careful planning, employing complex of oading strategies can hurt performance.
Instead, the work advocates for an architecture in which all tiers except the database should be of oaded to the edge.
Our evaluation of Ganesh has shown that it would bene t these scenarios.
To improve database scalability, C-JDBC [10], SSS [22], and Ganymed [28] also advocate the use of an interposition-based architecture to transparently cluster and replicate databases at the middleware level.
The approaches of these architectures and Ganesh are complementary and they would bene t each other.
Moving up to the presentation layer, there has been widespread adoption of fragment-based caching [14], which improves cache utilization by separately caching different parts of generated web pages.
While fragment-based caching works at the edge, a recent proposal has proposed moving web page assembly to the clients to optimize content delivery [31].
While Ganesh is not used at the presentation layer, the same principles have been applied in Duplicate Transfer Detection [25] to increase web cache ef ciency as well as for web access across bandwidth limited links [33].
The past few years have seen the emergence of many systems that exploit hash-based techniques.
At the heart of all these systems is the idea of detecting similarity in data without requiring interpretation of that data.
This simple yet elegant idea relies on cryp-tographic hashing, as discussed earlier in Section 2.
Successful applications of this idea span a wide range of storage systems.
Examples include peer-to-peer backup of personal computing  les [11], storage-ef cient archiving of data [29], and  nding similar  les [21].
Spring and Wetherall [35] apply similar principles at the network level.
Using synchronized caches at both ends of a network link, duplicated data is replaced by smaller tokens for transmission and then restored at the remote end.
This and other hash-based systems such as the CASPER [37] and LBFS [26]  lesystems, and Layer-2 bandwidth optimizers such as Riverbed and Peribit use Rabin  n-gerprinting [30] to discover spans of commonality in data.
This approach is especially useful when data items are modi ed in-place through insertions, deletions, and updates.
However, as Section 6 shows, the performance of this technique can show a dramatic drop in the presence of data reordering.
Ganesh instead uses row boundaries as dividers for detecting similarity.
The most aggressive use of hash-based techniques is by systems that use hashes as the primary identi ers for objects in persistent storage.
Storage systems such as CFS [12] and PAST [13] that have been built using distributed hash tables fall into this category.
Single Instance Storage [6] and Venti [29] are other examples of such systems.
As discussed in Section 2.2, the use of cryptographic hashes for addressing persistent data represents a deeper level of faith in their collision-resistance than that assumed by Ganesh.
If time reveals shortcomings in the hash algorithm, the effort involved in correcting the  aw is much greater.
In Ganesh, it is merely a matter of replacing the hash algorithm.
The growing use of dynamic web content generated from relational databases places increased demands on WAN bandwidth.
Traditional caching solutions for bandwidth and latency reduction are often ineffective for such content.
This paper shows that the impact of WAN accesses to databases can be substantially reduced through the Ganesh architecture without any compromise of the database s strict consistency semantics.
The essence of the Ganesh architecture is the use of computation at the edges to reduce communication through the Internet.
Ganesh is able to use crypto-graphic hashes to detect similarity with previous results and send
 interposition to achieve complete transparency: clients, application servers, and database servers are all unaware of Ganesh s presence and require no modi cation.
Our experimental evaluation con rms that Ganesh, while conceptually simple, can be highly effective in improving throughput and response time.
Our results also con rm that exploiting the structure present in database results to detect similarity is crucial to this performance improvement.
