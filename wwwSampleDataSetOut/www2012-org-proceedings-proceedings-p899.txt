Trains of Thought: Generating Information Maps

Dafna Shahaf

Carnegie Mellon University

5000 Forbes Avenue

Pittsburgh, PA

dshahaf@cs.cmu.edu

Carlos Guestrin

Carnegie Mellon University

5000 Forbes Avenue

Pittsburgh, PA

guestrin@cs.cmu.edu

Eric Horvitz

Microsoft Research
One Microsoft Way

Redmond, WA

horvitz@microsoft.com

ABSTRACT
When information is abundant, it becomes increasingly diﬃ-
cult to ﬁt nuggets of knowledge into a sigle coherent picture.
Complex stories spaghetti into branches, side stories, and in-
tertwining narratives. In order to explore these stories, one
needs a map to navigate unfamiliar territory. We propose
a methodology for creating structured summaries of infor-
mation, which we call metro maps. Our proposed algorithm
generates a concise structured set of documents which max-
imizes coverage of salient pieces of information. Most im-
portantly, metro maps explicitly show the relations among
retrieved pieces in a way that captures story development.
We ﬁrst formalize characteristics of good maps and formu-
late their construction as an optimization problem. Then
we provide eﬃcient methods with theoretical guarantees for
generating maps. Finally, we integrate user interaction into
our framework, allowing users to alter the maps to better
reﬂect their interests. Pilot user studies with a real-world
dataset demonstrate that the method is able to produce
maps which help users acquire knowledge eﬃciently.

Categories and Subject Descriptors
H.3.1 [Information Storage and Retrieval]: Content
Analysis and Indexing; H.3.3 [Information Storage and
Retrieval]: Information Search and Retrieval; H.5 [
Information Interfaces and Presentation]

Keywords
Metro maps, Information, Summarization

1.

INTRODUCTION

As data becomes increasingly ubiquitous, users are often
overwhelmed by the ﬂood of information available to them.
Although search engines are eﬀective in retrieving nuggets
of knowledge, the task of ﬁtting those nuggets into a single
coherent picture remains diﬃcult.

We are interested in methods for building more compre-
hensive views that explicitly show the relations among re-
trieved nuggets. We believe that such methods can enable
people to navigate new, complex topics and discover previ-
ously unknown links. We shall focus on the news domain; for
example, the system described in this paper can be used by
a person who wishes to understand the debt crisis in Europe
and its implications.

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2012, April 16–20, 2012, Lyon, France.
ACM 978-1-4503-1229-5/12/04.

Previous news summarization systems with structured out-
put [17, 18, 2] have focused mostly on timeline generation.
However, this style of summarization only works for sim-
ple stories, which are linear in nature. In contrast, complex
stories display a very non-linear structure: stories spaghetti
into branches, side stories, dead ends, and intertwining nar-
ratives. To explore these stories, one needs a map to guide
them through unfamiliar territory.

In this paper, we investigate methods for automatically
creating metro maps of information. Metro maps are con-
cise structured sets of documents maximizing coverage of
salient pieces of information; in addition, the maps make
explicit the various ways each piece relates to the others.
Due to the sparsity of the output, it naturally lends itself
to many visualization techniques. We chose to follow the
metro-map metaphor: a metro map consists of a set of lines
which have intersections or overlaps. Each line follows a
coherent narrative thread; diﬀerent lines focus on diﬀerent
aspects of the story. This visualization allows users to easily
digest information at a holistic level, and also to interact
with the model and make modiﬁcations.

Figure 1 shows a simpliﬁed metro map representing the
debt crisis in Greece. The middle (blue) line details the
chain of events leading from Greece’s debt ‘junk’ status to
the Greek bailout. The L-shaped (red) line is about strikes
and riots in Greece. Both lines intersect at an article about
the austerity plan, since it plays an important role in both
storylines: it was a key precondition for Greece to get bailout
money, but it also triggered many of the strikes.

To the best of our knowledge, the problem of construct-
ing metro maps is novel. We believe that metro maps can
serve as eﬀective tools to help users cope with information
overload in many ﬁelds. For example, maps can be a great
vehicle for scientists exploring the research landscape. Our
main contributions are as follows:

Figure 1: Greek debt crisis: a simpliﬁed metro map

Europe weights possibility of debt default in GreeceEurope commits to action on Greek debtFinance ministers stand ready to help GreeceEurope union moves towards a bailout of GreeceGreece set to release austerity planGreek workers protest austerity planHungary warns of Greek style crisisHungary disclaims earlier comparisons to Greecelabor unionsMerkelausteritybailoutjunk statusprotestsstrikeGermanylabor unionsMerkelWWW 2012 – Session: Web MiningApril 16–20, 2012, Lyon, France899didate set of good lines eﬃciently.

tees to construct good metro maps.

ize criteria characterizing good metro maps.

• We introduce the concept of a metro map and formal-
• We provide eﬃcient methods with theoretical guaran-
• We provide a randomized heuristic to generate a can-
• We integrate user preferences into our framework by
• We conduct promising pilot user studies, comparing
metro maps to Google News and to a state-of-the art
topic detection and tracking system. The results in-
dicate that our method helps users acquire knowledge
more eﬀectively on real-world datasets.

providing an interaction model.

2. CRAFTING AN OBJECTIVE FUNCTION
What are desired properties of a metro map? In the fol-
lowing, we motivate and formalize several (sometimes con-
ﬂicting) criteria. In Section 3, we present a principled ap-
proach to constructing maps that optimize tradeoﬀs among
these criteria. First, we need to formally deﬁne metro maps.
(Metro Map). A metro map M is a
pair (G, Π), where G = (V, E) is a directed graph and Π is
a set of paths in G. We refer to paths as metro lines. Each
e ∈ E must belong to at least one metro line.

Definition 2.1

As an example, the map in Figure 1 includes three metro
lines. Vertices V correspond to news articles, and are de-
noted by docs(M). The lines of Π correspond to aspects of
the story. A key requirement is that each line tells a coher-
ent story: Following the articles along a line should give the
user a clear understanding of evolution of a story.

Coherence is crucial for good maps, but is it suﬃcient as
well? In order put this matter to a test, we found maximally-
coherent lines for the query ‘Bill Clinton’ (using methods of
Section 2.1). The results were discouraging. While the lines
we found were indeed coherent, they were not important.
Many of the lines revolved around narrow topics, such as
Clinton’s visit to Belfast, or his relationship with his reli-
gious leader. Furthermore, as there was no notion of diver-
sity, the lines were very repetitive.

The example suggests that selecting the most coherent
lines does not guarantee a good map. Instead, the key chal-
lenge is balancing coherence and coverage: in addition to
being coherent, lines should also cover topics which are im-
portant to the user.

Finally, a map is more than just a set of lines; there is in-
formation in its structure as well. Therefore, our last prop-
erty is connectivity. The map’s connectivity should con-
vey the underlying structure of the story, and how diﬀerent
aspects of the story interact with each other.

In Sections 2.1-2.3, we formalize coherence, coverage
and connectivity. In Section 2.4, we explore their trade-
oﬀs and combine them into one objective function.
2.1 Coherence

How should we measure coherence of a chain of articles?
We rely on the notion of coherence developed in Connect-
the-Dots (CTD) [16]. In the following, we brieﬂy review this
notion.

In order to deﬁne coherence, a natural ﬁrst step is to
measure similarity between each pair of consecutive articles
along the chain. Since a single poor transition can destroy
the coherence of the entire chain, we measure the strength
of the chain by the strength of its weakest link.

a debt default

• Europe weights possibility
of debt default in Greece
• Why Republicans don’t fear
• Italy; The Pope’s leaning
toward Republican ideas
• Italian-American groups
• Greek workers protest

protest ‘Sopranos’

on Greek debt

• Europe weights possibility
of debt default in Greece
• Europe commits to action
• Europe union moves
• Greece set to release
• Greek workers protest

towards a bailout of Greece

austerity plan

austerity plan

austerity plan

Chain A

Chain B

However, such a simple approach can produce poor chains.
Consider, for example, Chain A. The transitions of Chain
A are all reasonable when examined out of context. For
example, the ﬁrst two articles are about debt default; the
second and third mention Republicans. However, the overall
eﬀect is associative and incoherent. Now, consider Chain B.
This chain has exactly the same endpoints, but it is much
more coherent.

Let us take a closer look at these chains: Figure 2 shows
word patterns along both chains. Bars correspond to ap-
pearance of words in the articles depicted above them. For
example, ‘Greece’ appeared throughout Chain B. It is easy
to spot the associative ﬂow of Chain A in Figure 2. Words
appear for short stretches, often only in two neighbouring
articles. Contrast this with Chain B, where stretches are
longer, and transitions between documents are smoother.
This observation motivates our deﬁnition of coherence.

Figure 2: Word patterns in Chain A (left) and B (right).
Bars correspond to the appearance of a word in the ar-
ticles depicted above them.

We represent documents as feature vectors (for the sake
of presentation, assume features W are words). Given a
chain of articles (d1, ..., dn), we ﬁrst score each transition
di → di+1 by the number of words that both articles share:

Coherence(d1, ..., dn) = min

i=1...n−1

1(w ∈ di ∩ di+1)

(cid:88)

w∈W

However, word appearance alone is too noisy. Articles must
use the exact same words; synonyms (or related words) are
treated as unrelated. Also, all words are treated equally:
the word ‘Greece’ is as important as the word ‘today’.
Therefore, one can replace the indicator function 1(·) with
a notion of importance of feature w in a transition. This
notion takes word co-occurrence into account, reducing the
noise considerably. It also considers the word’s importance
on a corpus level and on a document level (tf-idf). See [16]
for details.

Coherence(d1, ..., dn) = min

i=1...n−1

Importance(w | di, di+1)

(cid:88)

w

This objective guarantees good transitions, but associative
chains like Chain A can still score well. However, these
chains need to use a lot more words in order to achieve this
high score, since many of their transitions use a unique set
of words. On the other hand, coherent chains (like Chain

GreeceEuropeAusterityDebtProtestGreeceEuropeItalyRepublicanProtestGreeceEuropeItalyRepublicanProtestGreeceEuropeAusterityDebtProtestWWW 2012 – Session: Web MiningApril 16–20, 2012, Lyon, France900B) can often be characterized by a small set of words, which
are important throughout many of the transitions.
Therefore, instead of summing Importance(w | di, di+1)
over all features, the problem is transformed into an opti-
mization problem, where the goal is to choose a small set
of words (called ‘active’) and score the chain based on them
alone. Constraints on possible choices (see [16]) enforce a
small number of words and smooth transitions, imitating
the behaviour of Figure 2 (right).

(cid:88)

Coherence(d1, ..., dn) = max
Importance(w | di, di+1)1(w active in di, di+1)

i=1...n−1

activations

min

w

Finally, the coherence of a map is deﬁned as the minimal
coherence across its lines Π.
2.2 Coverage

In addition to coherence, we need to ensure that the map
has high coverage. The goal of coverage is twofold: we want
to both cover important aspects of the story, but also en-
courage diversity.

Our deﬁnition is inspired by [6]. Before we measure the
coverage of an entire map, we consider the coverage of a
single document. As in the previous section, documents
are feature vectors. Let function coverdi (w) : W → [0, 1]
quantify the amount that document di covers feature w. For
example, if W is a set of words, we can deﬁne cover·(·) as
tf-idf values.
Next, we extend cover·(·) to maps. Since in our model
coverage does not depend on map structure, it is enough to
extend cover·(·) to a function over sets of documents.

A natural candidate for coverM(w) is to view set-coverage

as an additive process:

coverM(w) =

(cid:88)

coverdi (w)

di∈docs(M)

Additive coverage is natural and easily computable. How-
ever, it suﬀers from one major drawback: since the coverage
each document provides is independent of the rest of the
map, additive coverage does not encourage diversity.
In
order to encourage diversity, we view set-coverage as a sam-
pling procedure: each document in the map tries to cover
feature w with probability coverdi (w). The coverage of w is
the probability at least one of the documents succeeded1:

coverM(w) = 1 − (cid:89)

(1 − coverdi (w))

di∈docs(M)

Thus, if the map already includes documents which cover w
well, coverM(w) is close to 1, and adding another document
which covers w well provides very little extra coverage of
w. This encourages us to pick articles which cover other
features, promoting diversity.

We now have a way to measure how much a map covers
a feature. Finally, we want to measure how much a map
covers the entire corpus. Remember, our goal is to ensure
that the map touches upon important aspects of the cor-
pus. Therefore, we ﬁrst assign weights λw to each feature
w, signifying the importance of the feature. For example, if
features are words (and stopwords have been removed), the
weights can correspond to word frequency in the dataset.

1If coverdi (w) are very small, we may want to sample more than
once from each document.

We model the amount M covers the corpus as the weighted

sum of the amount it covers each feature:

Cover(M) =

λwcoverM(w)

(cid:88)

w

The weights cause Cover to prefer maps which cover impor-
tant features of the corpus. In Section 6 we discuss learning
a personalized notion of coverage.
2.3 Connectivity

Our ﬁnal property is connectivity. There are many ways
to measure connectivity of a map: one can count the number
of connected components, or perhaps the number of vertices
that belong to more than one line.

We conducted preliminary experiments exploring diﬀer-
ent notions of connectivity. These results suggest that the
most glaring usability issue arises when maps do not show
connections that the user knows about. For example, in a
map about Israel, a line about legislative elections was not
connected to a line about the chosen government’s actions.
We came to the conclusion that the type of connection
(one article, multiple articles, position along the line) was
not as important as its mere existence. Therefore, we simply
deﬁne connectivity as the number of lines of Π that intersect:

(cid:88)

Conn(M ) =

1(pi ∩ pj (cid:54)= ∅)

i<j

2.4 Objective function: Tying it all together

Now that we have formally deﬁned our three properties,
we can combine them into one objective function. We need
to consider tradeoﬀs among these properties: for example,
maximizing coherence often results in repetitive, low-coverage
chains. Maximizing connectivity encourages choosing sim-
ilar chains, resulting in low coverage as well. Maximizing
coverage leads to low connectivity, as there is no reason to
re-use an article for more than one line.

Let us start with coherence. As mentioned in Section 2,
we are not interested in maximizing coherence. Instead, we
treat coherence as a constraint: only consider lines above a
certain coherence threshold τ , whether absolute or relative
(see Section 5 for parameter tuning). In the following, we
assume that τ is ﬁxed, and denote a chain coherent if its
coherence is above τ .

We are left with coverage and connectivity for our objec-
tive. Suppose we pick connectivity as our primary objective.
Our biggest obstacle is that coherent lines tend to come in
groups: a coherent line is often accompanied by multiple
similar lines. Those lines all intersect with each other, so
choosing them maximizes connectivity. However, the result-
ing map will be highly redundant.

For this reason, we choose coverage as our primary ob-
jective. Let κ be the maximal coverage across maps with
coherence ≥ τ . We can now formulate our problem:

Problem 2.2. Given a set of candidate documents D,
ﬁnd a map M = (G, Π) over D which maximizes Conn(M)
s.t. Coherence(M) ≥ τ and Cover(M) = κ.

In other words, we ﬁrst maximize coverage; then we maxi-
mize connectivity over maps that exhibit maximal coverage.
There is one problem left with our objective: consider
two metro lines that intersect at article d. Our coverage
function is a set function, therefore d is accounted for only
once.
In other words, replacing d in one of the lines can
only increase coverage. Since there usually exists a similar

WWW 2012 – Session: Web MiningApril 16–20, 2012, Lyon, France901article d(cid:48) which can replace d, the max-coverage map is often
disconnected. Worse yet, it is often unique.
In order to
mitigate this problem, we introduce slack into our objective:
Problem 2.3. Given a set of candidate documents D,
ﬁnd a map M = (G, Π) over D which maximizes Conn(M)
s.t. Coherence(M) ≥ τ and Cover(M) ≥ (1 − )κ.

for a given, small .
Finally, we need to restrict the size of M; we chose to
restrict M to K lines of length at most l. Alternatively,
since some stories are more complex than others, one may
prefer to add lines until coverage gains fall below a threshold.

3. ALGORITHM

In this section, we outline our approach for solving Prob-
lem 2.3. In Section 3.1 we represent all coherent chains as
a graph. In Section 3.2 we use this graph to ﬁnd a set of K
chains that maximize coverage; in Section 3.3, we increase
connectivity without sacriﬁcing coverage.
3.1 Representing all coherent chains

In order to pick good chains, we ﬁrst wish to list all pos-
sible candidates. However, representing all chains whose
coherence is at least τ is a non-trivial task. The number
of possible chains may be exponential, and therefore it is
infeasible to enumerate them all, let alone evaluate them.

Instead we propose a divide-and-conquer approach, con-
structing long chains from shorter ones. This approach al-
lows us to compactly encode many candidate chains as a
graph. See Figure 3 for an illustration: each vertex of the
graph corresponds to a short chain. Edges indicate chains
which can be concatenated and still maintain coherence. A
path in the graph corresponds to the concatenated chain.

It is tempting to concatenate any two chains that share an
endpoint. That is, concatenate (d1, ..., dk) and (dk, ..., d2k−1)
to form (d1, ..., d2k−1). However, caution is needed, as com-
bining two strong chains may result in a much weaker chain.
For example, Chain B ended with an article about protests
in Greece. If we concatenate it with a (coherent) chain about
protests across the globe, the concatenated chain will change
its focus mid-way, weakening coherence.

The problem appears to lie with the point of discontinuity:
when we concatenate (d1, ..., dk) with (dk, ..., d2k−1), there
is no evidence that both chains belong to the same storyline,
despite having a shared article dk. From the user’s point of
view, the ﬁrst k articles are coherent, but (d2, ..., dk+1) may
not be. This observation motivates our next deﬁnition:

Definition 3.1

(m-Coherence). A chain (d1, ..., dk)

has m-coherence τ if each sub-chain of length m (di, ..., di+m−1)
, i = 1, ..., k − m + 1 has coherence at least τ .

The idea behind m-coherence is to control the discontinu-
ity points. Choosing m is a tradeoﬀ: Increasing m results in

Figure 3: Encoding chains as a graph: each vertex of
the graph corresponds to a short chain. A path in the
graph corresponds to the concatenated chain.

Figure 4: A fragment of the coherence graph G for m =
3. Note overlap between vertices.

more-coherent chains, as the user’s ‘history window’ is wider.
However, it is also more computationally expensive. In par-
ticular, if m = l the user remembers the entire chain, thus
l-coherence is equivalent to the regular notion of coherence.
If m = 2, the user only remembers the last edge; therefore,
2-coherent chains optimize transitions without context, and
can result in associative chains like Chain A.

In practice, we chose the highest m we could aﬀord compu-
tationally (our website should handle queries in real time).
After choosing an appropriate m, we rephrase Problem 2.3:
Problem 3.2. Given a set of candidate documents D,
ﬁnd a map M = (G, Π) over D which maximizes Conn(M)
s.t. m-Coherence(M) ≥ τ and Cover(M) ≥ (1 − )κ.
Representing all chains whose m-coherence is at least τ is a
less daunting task. Observe that m-coherent chains can be
combined to form other m-coherent chains if their overlap
is large enough. Speciﬁcally, we require overlap of at least
(m − 1) articles:

Observation 3.3. If chains c = (d1, ..., dk) and c(cid:48) =

(dk−(m−2), ..., dk, ..., dr) are both m-coherent for k ≥ m > 1,
then the conjoined chain (d1, ..., dk, ..., dr) is also m-coherent.

The proof follows directly from Deﬁnition 3.1.
We can now construct a graph G encoding all m-coherent
chains. We call G a coherence graph. Vertices of G corre-
spond to coherent chains of length m. There is a directed
edge between each pair of vertices which can be conjoined
(m − 1 overlap).
It follows from observation 3.3 that all
paths of G correspond to m-coherent chains.
We are still left with the task of ﬁnding short coherent
chains to serve as vertices of G. These chains can be gen-
erated by a general best-ﬁrst search strategy. In a nutshell,
we keep a priority queue of sub-chains. At each iteration,
we expand the chain which features the highest coherence,
generating all of its extensions. When we reach a chain of
length m, we make it into a new vertex and remove it from
the queue. We continue until we reach our threshold. Since
the evaluation function used to sort the queue is admissi-
ble (as a subchain is always at least as coherent a chain
which extends it), optimality is guaranteed. In Section 4,
we present a faster method to ﬁnd good short chains.
Example Coherence Graphs Figure 4 shows a fragment
of a coherence graph for m = 3. The ﬁgure depicts multiple
ways to extend the story about the trapped Chilean miners:
one can either focus on the rescue, or skip directly to the
post-rescue celebrations.
3.2 Finding a high-coverage map

In the previous section, we constructed a coherence graph
G representing all coherent chains. Next, we seek to use

12345658912358933 Trapped Miners in Chile Say They’re Alive / 8.23.10Chileans Work to EnsureMiners Survive / 8.23.10Chileans Work to EnsureMiners Survive / 8.23.10Carnival Air Fills Chilean Campas Miners Rescue Nears / 10.11.10Facing Long Mine Rescue,Facing Long Mine Rescue,Chile Spares No Expense/ 8.27.10Facing Long Mine Rescue,Facing Long Mine Rescue,Chile Spares No Expense/ 8.27.10…Chile Mine Rescue to BeginWithin Hours / 10.12.10Chile Miners Honored by President in Capital / 10.26.10Carnival Air Fills Chilean Campas Miners Rescue Nears / 10.11.10Carnival Air Fills Chilean Campas Miners Rescue Nears / 10.11.10Facing Long Mine Rescue,Chile Spares No Expense/ 8.27.10Facing Long Mine Rescue,Chile Spares No Expense/ 8.27.10…WWW 2012 – Session: Web MiningApril 16–20, 2012, Lyon, France902Figure 5: An example of our results (condensed to ﬁt space). This map was computed for the query ‘Gree* debt’.
The main storylines discuss the austerity plans, the riots, and the role of Germany and the IMF in the crisis.

this graph to ﬁnd a set of chains which maximize coverage,
subject to map size constraints.

s.t. Cover(docs((cid:83)

Problem 3.4. Given G coherence graph, ﬁnd paths p1...pK

i pi)) is maximized, and |docs(pi)| ≤ l.

This problem is NP-hard, which necessitates resorting to ap-
proximation methods. First, let us pretend that we can enu-
merate all paths of G that contain up to l documents. Then,
we can take advantage of the submodularity of Cover(·):

Definition 3.5

(Submodularity). Function f is sub-
modular if for all A, B ⊂ V and v ∈ V we have f (A∪{v})−
f (A) ≥ f (B ∪ {v}) − f (B) whenever A ⊆ B.

In other words, f is submodular if it exhibits the property
of diminishing returns. Intuitively, Cover(·) is submodular
since reading some article v after already reading articles
A provides more coverage than reading v after reading a
superset of A [6].

Although maximizing submodular functions is still NP-
hard, we can exploit the classic result of [13], which shows
that the greedy algorithm achieves a (1− 1
e ) approximation.
In other words, we run K iterations of the greedy algorithm.
In each iteration, we evaluate the incremental coverage of
each candidate path p, given the paths which have been
chosen in previous iterations:

IncCover(p|M) = Cover(p ∪ M) − Cover(M)

That is, the additional cover gained from p if we already
have articles of M. We pick the best path and add it to M.
Let us revisit our assumption: unfortunately, enumerat-
ing all candidate paths is generally infeasible. Instead, we
propose a diﬀerent approach: suppose we knew the max-
coverage path for each pair of ﬁxed endpoints, documents
di and dj. Then, we could modify the greedy algorithm to
greedily pick a path amongst these paths only. Since there
are only O(|D|2) such pairs, greedy is feasible.

Computing the max-coverage path between two endpoints
is still a hard problem. In order to solve it, we formulate our
problem in terms of orienteering. Orienteering problems are
motivated by maximizing some function of the nodes visited
during a tour, subject to a budget on the tour length.

Problem 3.6

(Orienteering). Given an edge-weighted

directed graph G = (V, E, len) and a pair of nodes s, t, ﬁnd
an s-t walk of length at most B that maximizes a given func-
tion f : 2V → R+ of the set of nodes visited by the walk.

We set all edge lengths to 1. We want a path containing
at most l articles; since each vertex of G corresponds to m
articles, and the overlap is m − 1, we set the budget B to
be l − m. In addition, we want f to reﬂect the incremental
coverage of path p given the current map, so we deﬁne

f (p) = IncCover(p|M)

We adapt the submodular orienteering algorithms of [4]
to our problem. This is a quasipolynomial time recursive
greedy algorithm. Most importantly, it yields an α = O(log OP T )
approximation. We combine the greedy algorithm with sub-
modular orienteering. At each round, we compute approx-
imate best-paths between every two documents (given the
chains which have been selected in previous iterations) us-
ing submodular orienteering. We then greedily pick the best
one amongst them for the map. The algorithm achieves a
1 − 1

eα approximation.

The main bottleneck in our algorithm is the need to re-
evaluate a large number of candidates. However, many of
those re-evaluations are unnecessary, since the incremental
coverage of a chain can only decrease as our map grows
larger. Therefore, we use CELF [11], which provides the
same approximation guarantees, but uses lazy evaluations,
often leading to dramatic speedups.
3.3 Increasing connectivity
We now know how to ﬁnd a high-coverage, coherent map
M0. Our ﬁnal step is to increase connectivity without sac-
riﬁcing (more than an -fraction of) coverage.
In order to increase connectivity, we apply a local-search
technique. At iteration i, we consider each path p ∈ Πi−1.
We hold the rest of the map ﬁxed, and try to replace p by p(cid:48)
that increases connectivity and does not decrease coverage.
At the end of the iteration, we pick the best move and apply
it, resulting in Mi.
In order to ﬁnd good candidates to replace a path p, we
consider the map without p, Mi−1 \ p. We re-use the tech-

WWW 2012 – Session: Web MiningApril 16–20, 2012, Lyon, France903nique of submodular orienteering and compute approximate
max-coverage paths between every two documents. In order
to guide the process, we can bias the orienteering algorithm
into preferring vertices that already appear in Mi−1 \ p. We
consider all chains which do not decrease map coverage, and
pick the one which maximizes connectivity. We stop when
the solution value has not changed for T iterations.

Example 1

(Map). Figure 5 displays a sample map
generated by the methodology. This map was computed for
the query ‘Gree* debt’. The main storylines discuss the aus-
terity plans, the riots, and the role of Germany and the IMF
in the crisis. In order to facilitate navigation in the map, we
have added a legend feature. We assign a few characteristic
words to each line. The words chosen to describe each line
carry the highest incremental coverage for that line.

4.

IMPLEMENTATION

We have created a website that allows interactive visual-

ization of metro maps, which we hope to launch soon.

In this section, we discuss some of the practical imple-
mentation issues. In particular, the algorithm of Section 3
takes several minutes for most queries, which is not accept-
able for a website. Following an analysis of our algorithm,
we propose a method to speed up our bottlenecks.
4.1 Analysis

Let us analyze the complexity of the algorithm presented
in Section 3. Suppose our input consists of a set of docu-
ments D. The algorithm is composed of the following steps:
1. Constructing coherence graph vertices generated by
general best-ﬁrst heuristic (Section 3.1) may require solving
O(|D|m) linear programs in the worst case. The number of
edges may be O(|D|2m): however, using an implicit repre-
sentation (hashing preﬁxes and suﬃxes of each vertex) we
only need an expected time of O(|D|m) to store them.

2. The coverage step (Section 3.2) requires K iterations
of the greedy algorithm. Each iteration requires solving
O(|D|2) orienteering problems using the quasipolynomial al-
gorithm of [4].

3. The connectivity step (Section 3.3) performs local
search. Each iteration requires in the worst case solving
another O(K|D|2) orienteering problems.

4. The visualization step, which was not described earlier,
is based on a force-directed layout. The algorithm assigns
forces among edges as if they were springs, and simulates
the graph like a physical system. We have modiﬁed the algo-
rithm so that the graph respects chronological order among
vertices (by using x as a temporal axis); we have also tried
to minimize the number of kinks and turns in a line.

Steps 1-3 are computationally intensive. As noted earlier,
lazy evaluations lead to dramatic speedups in steps 2 and 3
without losing approximation bounds. Furthermore, steps 2

Figure 6: Behaviour of the top-9 features for Chain B
(left), and B’ (see in text, right). x axis represents docu-
ment position in the chain, y axis represents PCA feature
value. Circles correspond to the actual feature values in
the chain, and dashed lines are the lowest-degree poly-
nomial which ﬁts these points within a speciﬁed error.
The degree of each polynomial appears above it.

and 3 are very easy to parallelize. Step 1, on the other hand,
is both harder to parallelize and requires an external LP
solver. In the next section, we propose a practical method
to speed this step up.
4.2 Speeding up the coherence graph

We are interested in a fast way to build a coherence graph
G. The CTD coherence notion was created with long chains
in mind; we can take advantage of the fact that chains we
are interested in are short (length m). Shorter chains are
simpler: articles are often more related to each other, and
there is less topic drift.
In terms of Section 2.1, selected
words are active throughout the entire chain, which makes
them easier to identify; for example, we can look at the
words with the highest median (or average) value.

Next, we look at the behaviour of these words throughout
the chain. We observe that in many good chains, words
exhibit a smooth behaviour: in Chain B, the word ‘austerity’
becomes more important as the chain progresses, while ‘EU’
decreases and ‘debt’ is stable. In poor chains, words tend to
ﬂuctuate more.

In order to formalize this, we look at the series of coeﬃ-
cients of each word throughout the chain, and try to express
it as a low-degree polynomial. Figure 6 demonstrates this
approach. We tested two chains: Chain B and Chain B’,
which was obtained from Chain B by replacing an inter-
mediate article by an article about Hungary and the Greek
debt. Intuitively, this change made Chain B’ less coherent.
Figure 6 shows the behaviour of the top 9 features for
Chain B (left), and B’ (right). As words were too noisy, we
used PCA components instead. x axis represents document
position in the chain, y axis represents (PCA) feature value.
Circles correspond to the actual values, and dashed lines are
the lowest-degree polynomial which ﬁts them within a spec-
iﬁed error. The degree of each polynomial appears above it
(note that one can always can ﬁt a polynomial of degree 4
through 5 points). As expected, Chain B needs lower-degree
polynomials than Chain B’.

We have experimented with low-degree polynomials as a
measure of chain quality. Chains displaying low-degree be-
haviour were usually coherent, but some of our hand-picked
coherent chains required high-degree. That is, the process
seems biased towards false negatives. However, according to
our observations, this bias does not pose a problem when D
is large enough. If a coherent chain did not score well, there
was usually a similar chain which scored better.

WWW 2012 – Session: Web MiningApril 16–20, 2012, Lyon, France904Algorithm 1: FindShortChains(D, m)
input : D a set of documents, m desired length
output: A set of chains of length m.

1 for K iterations do
2

Randomly select (d, d(cid:48)) ∈ D2 ;
// Create a model of a chain between d and d(cid:48)
foreach (w) ∈ importantFeatures({d, d(cid:48)}) do
Modeld,d(cid:48) (w) = polyfit((1, d(w)), (m, d(cid:48)(w))) ;

// Evaluate other documents
Initialize array FitDocs to ∅;
foreach d(cid:48)(cid:48) ∈ D \ {d, d(cid:48)} do

// Find best position for d(cid:48)(cid:48) (null if far)
i(cid:48)(cid:48) = bestPos(d(cid:48)(cid:48), Modeld,d(cid:48) ) ;
if i(cid:48)(cid:48) (cid:54)= null then

FitDocs[i(cid:48)(cid:48)] = FitDocs[i(cid:48)(cid:48)] ∪ {d(cid:48)(cid:48)} ;

3
4

5
6

7
8
9

10
11

score = getScore(FitDocs) ;
Record best model as Model∗, and its FitDocs array as
FitDocs∗;

// Good model found. Reestimate from fitting docs

12 if Model∗ has a low score then return ;
13 foreach (w) ∈ importantFeatures(FitDocs∗) do
new(w) = polyfit(FitDocs∗(w), degree) ;
15 return extractChains(FitDocs, Model∗

Model∗

14

new) ;

We can now describe our algorithm for ﬁnding good short

chains (see Algorithm 1). Our approach is inspired by RANSAC
[8]. RANSAC is a random sampling method. In each itera-
tion, we randomly select a set of candidate pairs of articles,
{(d, d(cid:48))}, to be used as chain endpoints (Line 2). We then
hypothesize a model for the rest of the chain (Line 4). A
model is a sequence of predicted values for each feature. For
example, if d and d(cid:48) both display high levels of feature w,
we expect the rest of the documents in any coherent chain
to display similar high levels. If d displays higher levels of
w, we expect to observe this trend in the rest of the chain.
Since we only have two points to estimate the model from,
we simply ﬁt a line between them.
We then evaluate our model by ﬁnding other articles that
closely ﬁt the model’s prediction. For each article d(cid:48)(cid:48), we
i.e., the position that
ﬁnd the best position in the chain:
minimizes d(cid:48)(cid:48)’s distance from the model (Line 7, function
bestPos). If d(cid:48)(cid:48) is close enough to the model, this position
is recorded in the FitDocs array.

(cid:81)

After all documents were tested, Function getScore com-
putes the number of chains that can be generated from Fit-
Docs. If there are no chronological constraints, this is simply
i |FitDocs[i]|: there are |FitDocs[i]| options for the ith po-
sition. Otherwise, we construct a directed acyclic graph cor-
responding to constraints, and apply a linear-time algorithm
to count the number of possible paths.

We repeat this process for multiple candidate pairs, and
then pick the best model. Since the model was estimated
only from the initial two articles, we re-estimate it from all
of its close articles FitDocs (Line 14). Finally, we extract
short chains that are a close ﬁt to the re-estimated model.
Our algorithm is not guaranteed to succeed, since it may
not draw documents that capture a good model. However,
since many document pairs do encode a good model, the
algorithm works well in practice.
It is also fast and easy
to parallelize. In addition, the algorithm provides an inter-
esting interpretation of m-coherence: One can think of a
chain as a ride through feature-space. Each sub-chain has
a smooth trajectory, when projected on important-feature

axis. Because of the large overlap between sub-chains, we
are only allowed gentle adjustments to the steering wheel as
we progress throughout the chain.

As expected, the algorithm tends to recover topics that are
heavily represented in the dataset; topics that are poorly-
represented are less likely to be sampled. Nevertheless, the
chains recovered are of comparable quality to the chains re-
covered by methods of Section 3.1.

5. USER STUDY

map summarize the topic of the task?

information faster than other methods?

In our user study, we evaluate the eﬀectiveness of metro
maps in aiding users navigate, consume, and integrate dif-
ference aspects of a multi-faceted information need. Our ex-
periments were designed to answer the following questions:
• Accuracy: How well do the documents selected for the
• Micro-Knowledge: Can the maps help users retrieve
• Macro-Knowledge: Can the maps help users under-
• Structure: What is the eﬀect of the map structure?
We assembled a corpus of 18,641 articles from the Interna-
tional section of the New York Times, ranging from 2008 to
2010. This corpus was selected because of the material’s rel-
ative accessibility, as news articles are written with a broad
reader population in mind. Stopword removal and stemming
have been performed as a preprocessing step.

stand the big picture better than other methods?

We created three news exploration tasks, representing use
cases where the reader is interested in learning about the
trapped Chilean miners, the earthquake in Haiti, and the
debt crisis in Greece. We refer to these tasks as Chile, Haiti,
and Greece. Our tasks were chosen in order to cover diﬀerent
scenarios: The Chile task is very focused, concentrating on
a single geographic location and a short time period. The
Haiti task is broader, and the Greece task was the most
complicated, as it spans multiple countries for a long period
of time. In the following, we outline our evaluations.
5.1 Accuracy

In this study, we evaluate the map’s content. Before we
let users interact with our system and look for information,
we want to know whether the information is there at all.

For each task, three domain experts composed a list of the
top ten events related to the task. The experts composed
their lists separately, and the top ten events mentioned most
often were chosen. For example, Chile events included the
accident, miners’ discovery, miners’ video, drill beginning
and completion, ﬁrst and last miner outside, release from
the hospital, media coverage of the saga, and the presidential
ceremony held in the miners’ honor.

We then asked the experts to identify those events in
metro maps of diﬀerent sizes (3-6 lines of length at most
6). Below we measure subtopic recall (fraction of the impor-
tant events that are successfully retrieved) of our method.
In general, results are high: many of the important events
are captured.

Lines
Chile
Haiti
Greece

3

4

5

6

80% 100% 100% 100%
80%
50% 70%
30% 60%
70%

80%
60%

Note that high subtopic precision (fraction of retrieved doc-
uments which are relevant to the top ten events) is not a
desired property of metro maps: high precision means that

WWW 2012 – Session: Web MiningApril 16–20, 2012, Lyon, France905the maps is very focused on a small set of events, implying
repetitiveness.
If the top ten events are already covered,
submodular coverage will try to cover side stories as well.
5.2 Micro-Knowledge and Structure

In this study, our goal is to examine maps as retrieval
tools; we wish to see how maps help users answer speciﬁc
questions. We compare the level of knowledge (per time
step) attained by people using our prototype vs. two other
systems: Google News and TDT. Google News is a computer-
generated site that aggregates headlines from news sources
worldwide. News-viewing tools are dominated by portal and
search approaches, and Google News is a typical represen-
tative of those tools. TDT [12] is a successful system which
captures the rich structure of events and their dependencies
in a news topic.

We computed maps by methods of Section 4. We set m=3
for quick computation. After experimenting with several
other queries, we set the coherence threshold to top 15%.
Instead of ﬁxing the number of chains, we continued to add
chains until additional coverage was less than 20% of the
total coverage (since we use greedy coverage, there will be
at most 5 chains).
We implemented TDT based on [12] (cos+TD+Simple-
Thresholding). We used the same articles D for maps and
for TDT. We picked D using broad queries: ‘chile miners’,
‘haiti earthquake’ and ‘gree* debt’. We queried Google News
for ‘chile miners’, ‘haiti earthquake’ and ‘greece debt’ (plus
appropriate date ranges). We did not restrict Google News
to NYTimes articles, as not all of them are included. We
ensured that all systems display the same number of arti-
cles: for Google News, we picked the top articles. For TDT,
we picked a representative article from each cluster. The
purpose of the study was to test a single query. We defer
the evaluation of the interactive component to Section 6.

We note that comparing the diﬀerent systems is problem-
atic, as the output of Google News and TDT is diﬀerent
both in content and in presentation (and in particular, can-
not be double-blind), so it is hard to know what to attribute
observed diﬀerences to. In order to isolate the eﬀects of doc-
ument selection vs. map organization, we introduce a hybrid
system into the study: the system, Structureless metro
maps displays the same articles as metro maps but with
none of the structure. Instead, articles are sorted chrono-
logically and displayed in a fashion similar to Google News.
We recruited participants in the study via Amazon Me-
chanical Turk. Each user chose the number of tasks they
were interested in doing out of the three tasks available. For
each selected task, one of the four methods was assigned
randomly. To make the users more comfortable with the
system (and unfamiliar map interface), we asked them to do
a warm-up task: copy the ﬁrst sentence of the tenth arti-
cle. After the warm-up, users were asked to answer a short
questionnaire (ten questions), composed by domain experts.
Users were asked to answer as many questions as possible
in 10 minutes. In order to counter bias introduced by prior
knowledge, the users had to specify the article where the
answer was found. A honey pot question (an especially easy
question, that we expect all workers to be able to answer)
was used to identify spammers. After removing users who
got this question wrong, we were left with 338 unique users
performing 451 tasks.

A snapshot of the users’ progress (number of correct an-
swers) was taken every minute. Our interest is twofold: we
wish to measure the user’s total knowledge after ten min-

utes, and also the rate of capturing new knowledge. Figure
7 shows the results. x axis corresponds to time, and y axis
corresponds to the average number of correct answers.

The results indicate that metro maps are especially useful
for complex tasks, such as Greece. In this case, maps achieve
higher scores than Google and TDT at the end of the test,
as the advantage of structure outweighs the cost of ingesting
the additional structure and grappling with an unfamiliar
interface. Perhaps more importantly, the rate of capturing
new knowledge is higher for maps.

The structureless methods do better for the simple task
of Chile. Upon closer examination of the users’ browsing
patterns, it seems that many of the answers could be found
in the (chronologically) ﬁrst and last articles; the ﬁrst article
provided the basic facts, and the last summarized the story.
We believe that this is the reason for the map’s performance.
Let us examine Structurelss Maps. As discussed earlier,
the fact that Structurelss Maps outperforms Google News is
due to article selection. Metro maps and structureless maps
seem comparable, but Metro Map users acquire knowledge
more quickly, especially for complex stories; e.g., consider
the ﬁrst few minutes of the Greece task in Figure 7.

As a side note, the small number of correct answers is wor-
risome. We found that the main cause of mistakes was date-
related questions; many of the participants entered the arti-
cle’s date, rather than the event’s. Since about 30% of our
questions involved dates, this aﬀected the results severely.
In addition, the majority of Turk users are non U.S-based
(and non-native English speakers)2. When we conducted
a preliminary survey across CMU undergrads, the average
number of correct answers was signiﬁcantly higher.

Finally, we compare the ease of navigation. If a user has
a question in mind, we estimate the diﬃculty of ﬁnding an
article containing the answer by computing the number of
articles that users clicked per correct answer:

Maps

2.1

SL Maps Google TDT
4.91

3.74

5.28

Metro maps require the least amounts of clicks to reach an
answer. Most importantly, maps did better than structure-
less maps, demonstrating the utility of the structure.
5.3 Macro-Knowledge

The retrieval study in the previous section evaluated users’
ability to answer speciﬁc questions. We are also interested
in the use of metro maps as high-level overviews, allowing
users to understand the big picture.

We believe that the true test of one’s own understanding
of a topic is their ability to explain it to others. There-
fore, we recruited 15 undergraduate students and asked them
to write two paragraphs: one summarizing the Haiti earth-
quake, and one summarizing the Greek debt crisis. For each
of the stories, the students were randomly assigned either a
metro map or the Google News result page (stripped of logo
and typical formatting, to avoid bias).

We then used Mechanical Turk to evaluate the paragraphs.
At each round, workers were presented a two paragraphs
(map user vs. Google News user). The workers were asked
which paragraph provided a more complete and coherent
picture of the story; in addition, they justiﬁed their choice
in a few words (‘Paragraph A is more...’).

After removing spam, we had 294 evaluations for Greece,
and 290 for Haiti. 72% of the Greece comparisons preferred

2http://www.behind-the-enemy-lines.com/2010/03/new-
demographics-of-mechanical-turk.html

WWW 2012 – Session: Web MiningApril 16–20, 2012, Lyon, France906Figure 7: User study results: average number of correct answers amongst users vs. time. As the task gets more
complex, metro maps become more useful.

Figure 8: Tag clouds representing descriptions of Google
News (left) and Map (right) pararaphs. Note maps re-
ceive more positive adjectives.

map paragraphs, but only 59% of Haiti. After examining
the Haiti paragraphs, we found that the all paragraphs were
very similar; most followed the same pattern (earthquake,
damages, distributing aid). None of the paragraphs men-
tioned the Haitian child smugglers, and only one mentioned
the temporary laws for dislocated Haitians, despite the fact
that both stories appeared in the map. A possible explana-
tion was given by one of the participants: ”I chose to not use
the American politics. If this is a summary about the event
I wanted to remain as objective as possible”. In other words,
map users avoided some storylines intentionally. As in the
previous section, maps are more useful for stories without a
single dominant storyline (‘the event’), like Greece.

Finally, Figure 8 shows tag clouds of the words workers
chose to describe the Greece paragraphs. Sample map para-
graphs descriptions include ‘gives a clear picture’ and ‘gives
a better understanding of the debt crisis’. Google News
paragraph descriptions included ‘good but just explained
about the facts’ and ‘more like a list of what happened’.

6. PERSONALIZATION AND INTERACTION

Models of interaction can be naturally integrated with
In order to be useful, the model must be
metro maps.
capable of representing users’ interests. We rely on user
feedback in order to learn preferences and adjust the maps
accordingly. In the following, we illustrate the potential of
learning a personalized coverage function with an example.
Since our coverage objective is a set function, the most
natural notion of feedback from a machine learning perspec-
tive would be for users to provide a single label for the map,
indicating whether they like or dislike it. However, this ap-
proach is not practical. Since there are exponentially many
such maps, we are likely to need an extensive amount of user
feedback before we could learn the function.

Even more importantly, this approach makes it hard for
users to form expressive queries. Ideally, we would like to
support queries of the form ‘I want to know more about
the involvement of Germany in the debt crisis’, or ‘I do not
want to know about Wyclef Jean’s Haitian Presidential bid’.
However, labeling entire maps – or even single documents –
is just not rich enough to support this query model. Indeed,

the user could indicate that they dislike Wyclef Jean articles
shown to them, but there is no way for them to specify that
they would like to see something which is not on the map.
We propose to let the user provide feature-based feedback
instead. Feature-based feedback provides a very natural
way for supporting the queries mentioned above. For ex-
ample, the user could increase the importance of the word
‘Germany’ and decrease the importance of ‘Wyclef Jean’ to
achieve the desired eﬀect.

There has been growing recent interest in feature-based

feedback. [5] proposed a discriminative semi-supervised learn-
ing method that incorporates into training aﬃnities between
features and classes. For example, in a baseball vs. hockey
text classiﬁcation problem, the presence of the word “puck”
can be considered as a strong indicator of hockey. We refer
to this type of input as a labeled feature.

Unlike previous approaches that use labeled features to
create labeled pseudo-instances, [5] uses labeled features di-
rectly to constrain the model’s predictions on unlabeled in-
stances. They express these soft constraints using general-
ized expectation (GE) criteria – terms in a parameter esti-
mation objective function that express preferences on values
of a model expectation.

We apply the idea of labeled features to metro maps. We
aim at creating two classes of documents, roughly meant
to represent ‘interesting’ and ‘non-interesting’. Initially, we
have no labels; we compute a metro map (as discussed in
previous sections) and display it to the user. In addition,
we show the user a tag cloud. A tag cloud is a visual depic-
tion of words, where size represents frequency (see top-right
of the website ﬁgure in Section 4). The cloud describes the
documents of the map. We let users adjust word impor-
tance. For example, importance of 0.9 implies that 90% of
the documents in which the word appears are interesting
to the user. The relative transparency of the model allows
users to make sense of feature weights.
When the user is done adjusting word importance, we
train a MaxEnt classiﬁer on D using those constraints. The
classiﬁer then assigns a score µi to each document di ∈ D.
µi represents the probability that di is interesting to the
user. Given µi, we deﬁne a personalized notion of coverage:

per-coveri(j) = µi · coveri(j)

Weighting the coverage by µi causes non-interesting articles
to contribute very little coverage. Note that non-interesting
articles may still be picked for the map, e.g.
if they are a
coherent bridge between two areas of high interest.

In order to demonstrate the algorithm, we increase the
importance of ‘IMF’ in the Greek debt map. The new map
focused more on the IMF, including articles such as ‘I.M.F.

0123450246810# Correct answersMinutesSL MapsMapsGoogleNewsTDT0123450246810Minutes0123450246810MinutesChileHaitiGreeceWWW 2012 – Session: Web MiningApril 16–20, 2012, Lyon, France907May Require Greece to Cut Budget and Jobs’, ‘I.M.F. Is
Playing the Role of Deal Maker in Europe’, ‘E.U. Leaders
Turn to I.M.F. Amid Financial Crisis’. After decreasing the
importance of ‘Greece’, a new line appeared, focusing on the
Spanish economic struggle. Representative articles include
‘Spain Seen as Moving Too Slowly on Financial Reforms’
and ‘I.M.F. Gives Backing to Spain’s Austerity Measures’.

7. RELATED WORK

To the best of our knowledge, the problem of constructing
metro maps automatically is novel. There has been exten-
sive work done on related topics from topic detection and
tracking to summarization and temporal text mining.

Our work diﬀers from previous work in two important as-
pects. Our system has structured output: Not only does
our system pick nuggets of information, it explicitly shows
connections among them. Prior work, in contrast, has been
limited largely to list-output models. In the summarization
task [14, 2, 15], the goal is often to summarize a corpus of
texts by extracting a list of sentences. Other methods [10,
20, 19] discover new events, but do not attempt to string
them together.

Numerous prior eﬀorts have moved beyond list-output,
and proposed diﬀerent notions of storylines [1, 17, 18, 2].
Graph representations are common across a variety of re-
lated problems [9, 7, 12] , from topic evolution to news anal-
ysis. However, in all of those methods, there is no notion of
path-coherence. In other words, the edges in the graph
are selected because they pass some threshold, or belong to a
spanning tree. We believe that the notion of coherent paths
facilitates the process of knowledge acquisition for the users.
Finally, diﬀerent notions of coherence and coverage have
been proposed in the literature. For example, enhancing
coverage has been explored in the context of ranking and
summarization (see MMR [21]). We chose not to use MMR
as it does not provide approximation guarantees, and could
not be combined with our orienteering algorithm. Modeling
coherence via lexical relations was studied in [3]. However,
their notion is restricted to chains of related words (Machine,
Microprocessor, Device). In contrast, we generate coherent
chains of articles by taking multiple concepts into account.

8. CONCLUSIONS AND FUTURE WORK
We have presented a new task, creating structured sum-
maries of information, which we call metro maps. Given a
query, our algorithm generates a concise structured set of
documents which maximizes coverage of salient pieces of in-
formation. Most importantly, metro maps explicitly show
the relations between the retrieved pieces.

We formalized the characteristics of good metro maps
and provided eﬃcient methods with theoretical guarantees.
Our approach ﬁnds concise maps, making it well-suited for
complement existing visualization and user interaction ap-
proaches. In particular, we integrate user preferences into
our framework by providing an appropriate user-interaction
model based on feature-based feedback.

We conducted pilot user studies, testing our algorithm on
a real-world dataset. The study showed the promised of
the proposed approach as an eﬀective and fast method for
creating valuable metro maps.

In the future, we plan to pursue richer forms of input, out-
put and interaction, and the incorporation of higher-level se-
mantic relations into our framework. In addition, we would
like to apply our methods to other datasets, such as scien-

tiﬁc publications. We believe that metro maps will enable
users to better cope with information overload.

Acknowledgments: This work was partially supported by
ONR PECASE N000141010672, ARO MURI W911NF0810242,
and NSF Career IIS-0644225. Dafna Shahaf was supported in
part by Microsoft Research Graduate Fellowship.

9. REFERENCES
[1] A. Ahmed, Q. Ho, J. Eisenstein, E. Xing, A. J. Smola, and

C. H. Teo. Uniﬁed analysis of streaming news. In
WWW’11, 2011.

[2] J. Allan, R. Gupta, and V. Khandelwal. Temporal

summaries of new topics. In SIGIR ’01, 2001.

[3] R. Barzilay and M. Elhadad. Using lexical chains for text
summarization. In ACL Workshop on Intelligent Scalable
Text Summarization, 1997.

[4] C. Chekuri and M. Pal. A recursive greedy algorithm for

walks in directed graphs. In FOCS ’05, 2005.

[5] G. Druck, G. Mann, and A. McCallum. Learning from

labeled features using generalized expectation criteria. In
SIGIR ’08, pages 595–602. ACM, 2008.

[6] K. El-Arini, G. Veda, D. Shahaf, and C. Guestrin. Turning

down the noise in the blogosphere. In KDD ’09, 2009.

[7] C. Faloutsos, K. S. McCurley, and A. Tomkins. Fast

discovery of connection subgraphs. In KDD ’04, 2004.

[8] M. A. Fischler and R. C. Bolles. Random sample

consensus: a paradigm for model ﬁtting with applications
to image analysis and automated cartography. Commun.
ACM, 24:381–395, June 1981.

[9] Y. Jo, J. E. Hopcroft, and C. Lagoze. The web of topics:

discovering the topology of topic evolution in a corpus. In
WWW ’11, 2011.

[10] J. Kleinberg. Bursty and hierarchical structure in streams,

2002.

[11] J. Leskovec, A. Krause, C. Guestrin, C. Faloutsos,

J. VanBriesen, and N. Glance. Cost-eﬀective outbreak
detection in networks. In KDD, 2007.

[12] R. Nallapati, A. Feng, F. Peng, and J. Allan. Event

threading within news topics. In CIKM ’04, 2004.

[13] G. Nemhauser, L. Wolsey, and M. Fisher. An analysis of

the approximations for maximizing submodular set
functions. Mathematical Programming, 14, 1978.
[14] A. Nenkova and K. McKeown. A survey of text

summarization techniques. In C. C. Aggarwal and C. Zhai,
editors, Mining Text Data. 2012.

[15] D. Radev, J. Otterbacher, A. Winkel, and

S. Blair-Goldensohn. Newsinessence: summarizing online
news topics. Commun. ACM, 48:95–98, October 2005.

[16] D. Shahaf and C. Guestrin. Connecting the dots between

news articles. In KDD ’10, pages 623–632, New York, NY,
USA, 2010. ACM.

[17] R. Swan and D. Jensen. TimeMines: Constructing

Timelines with Statistical Models of Word Usage. In KDD’
00, 2000.

[18] R. Yan, X. Wan, J. Otterbacher, L. Kong, X. Li, and

Y. Zhang. Evolutionary timeline summarization: a
balanced optimization framework via iterative substitution.
In SIGIR’ 11, 2011.

[19] Y. Yang, T. Ault, T. Pierce, and C. Lattimer. Improving
text categorization methods for event tracking. In SIGIR
’00, 2000.

[20] Y. Yang, J. Carbonell, R. Brown, T. Pierce, B. Archibald,

and X. Liu. Learning approaches for detecting and tracking
news events. IEEE Intelligent Systems, 14(4), 1999.

[21] C. X. Zhai, W. W. Cohen, and J. Laﬀerty. Beyond

independent relevance: methods and evaluation metrics for
subtopic retrieval. In SIGIR ’03. ACM, 2003.

WWW 2012 – Session: Web MiningApril 16–20, 2012, Lyon, France908