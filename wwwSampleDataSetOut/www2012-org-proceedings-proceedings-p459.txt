Cross-lingual Knowledge Linking Across

Wiki Knowledge Bases

Zhichun Wang, Juanzi Li, Zhigang Wang, and Jie Tang

Department of Computer Science and Technology

{zcwang, ljz, wzhigang, tangjie}@keg.cs.tsinghua.edu.cn

Tsinghua University, Beijing, China

ABSTRACT
Wikipedia becomes one of the largest knowledge bases on
the Web. It has attracted 513 million page views per day in
January 2012. However, one critical issue for Wikipedia is
that articles in diÔ¨Äerent language are very unbalanced. For
example, the number of articles on Wikipedia in English has
reached 3.8 million, while the number of Chinese articles is
still less than half million and there are only 217 thousand
cross-lingual links between articles of the two languages. On
the other hand, there are more than 3.9 million Chinese Wi-
ki articles on Baidu Baike and Hudong.com, two popular
encyclopedias in Chinese. One important question is how to
link the knowledge entries distributed in diÔ¨Äerent knowledge
bases. This will immensely enrich the information in the on-
line knowledge bases and beneÔ¨Åt many applications. In this
paper, we study the problem of cross-lingual knowledge link-
ing and present a linkage factor graph model. Features are
deÔ¨Åned according to some interesting observations. Exper-
iments on the Wikipedia data set show that our approach
can achieve a high precision of 85.8% with a recall of 88.1%.
The approach found 202,141 new cross-lingual links between
English Wikipedia and Baidu Baike.

Categories and Subject Descriptors
E.2 [Data]: Data Storage Representations‚ÄîLinked repre-
sentations; H.3 [Information Systems]: Miscellaneous‚Äî
Information Storage and Retrieval

General Terms
Algorithms, Languages

Keywords
Knowledge Linking, Cross-lingual, Wiki knowledge base,
Knowledge sharing

1.

INTRODUCTION

Cross-lingual knowledge linking is the task of creating
links between articles in multiple diÔ¨Äerent languages that
reports on the same content. Cross-lingual knowledge link-
ing not only globalizes the knowledge sharing of diÔ¨Äerent
languages on the Web, but also beneÔ¨Åts many online appli-
cations such as information retrieval and machine transla-

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2012, April 16‚Äì20, 2012, Lyon, France.
ACM 978-1-4503-1229-5/12/04.

tion. For example, [20] explores cross-lingual links in ma-
chine translation and [30] studies how to improve informa-
tion retrieval by leveraging cross-lingual knowledge links.
The project of DBpedia [6][4] provides a semantic represen-
tation of Wikipedia in which multiple language labels are
attached to individual concepts. The idea of cross-lingual
linking has already become the nucleus for the linked da-
ta [6].

However, most traditional resources are monolingual, such
as Cyc [23] and WordNet [25] in English, HowNet [12] in Chi-
nese. Wikipedia tries to deal with this problem by providing
information in diÔ¨Äerent languages. Wikipedia contains 19
million articles in 281 languages. Articles in diÔ¨Äerent lan-
guages are interlinked. However, the number the articles in
diÔ¨Äerent languages is very unbalanced. Figure 1 shows the
number of articles in 12 diÔ¨Äerent languages on Wikipedia.
As it can be seen that there are 3.8 million English articles,
but only 382,000 Chinese articles on Wikipedia. This makes
it infeasible to create cross-lingual links between articles of
diÔ¨Äerent languages with a large coverage.

On the other hand, there are several separated Wik-
i knowledge bases on the Web. For example, Baidu Baike
and Hudong.com are two Chinese Wiki knowledge bases con-
taining more than 3.9 million articles. Ideally, automatical-
ly creating cross-lingual links between these Chinese Wiki
knowledge bases and the English Wikipedia would be very
useful. However, at present, the work is mainly taken by
manual, which is obviously tedious, time consuming, and er-
ror prone. In existing literature, a few approaches have been
proposed for Ô¨Ånding missing cross-lingual links in Wikipedi-
a [29, 31]. However, as we mentioned before, the number
of articles in diÔ¨Äerent languages is very unbalanced. For
most English articles, we will be not able to Ô¨Ånd the corre-
sponding Chinese version. To the best of our knowledge, no
previous work has extensively studied the problem of creat-
ing cross-lingual links across diÔ¨Äerent knowledge bases (e.g.,
the English Wikipedia and the Chinese encyclopedia Baidu
Baike).

In this paper, we try to systematically study the prob-
lem of cross-lingual knowledge linking across multiple Wiki
knowledge bases. The problem is non-trivial and poses a set
of challenges.
(cid:15) Linguistics. Existing methods for Ô¨Ånding cross-lingual
links heavily depend on translation tools.
Such a
method often results in high precisions, but low recall-
s. Can we Ô¨Ånd some language-independent features for
mining cross-lingual knowledge links?

(cid:15) Model. There are diÔ¨Äerent kinds of information that

WWW 2012 ‚Äì Session: Entity LinkingApril 16‚Äì20, 2012, Lyon, France459Figure 1: Number of articles in diÔ¨Äerent languages
in Wikipedia.

Figure 2: An example of knowledge linking.

could be used in the knowledge linking problem such
as article links, categories and authors, and they are
correlated with each other. How to deÔ¨Åne a model
to incorporate both the local features of articles and
relations of cross-lingual links together?

(cid:15) EÔ¨Éciency. Wiki knowledge bases contain thousands
and millions of articles. How to develop an eÔ¨Äective
and eÔ¨Écient algorithm that can deal with both com-
plex and large data sets?

In order to solve the above challenges, we Ô¨Årst empirically
investigate several important factors for cross-lingual knowl-
edge linking and then propose a factor graph model to solve
the knowledge-linking problem. Our contributions include:
(cid:15) We formally formulate the problem of knowledge link-
ing across Wiki knowledge bases in diÔ¨Äerent languages,
and analyze important factors for cross-lingual knowl-
edge linking.

(cid:15) We present a uniÔ¨Åed model for solving cross lingual
knowledge linking problem: Linkage Factor Graph
(LFG) model. EÔ¨Äective candidate selection method
and distributed learning algorithm enable LFG scale
to large data sets.

(cid:15) We evaluate our proposed approach on existing cross-
lingual links in Wikipedia; it achieves high precision
of 85.8% with a recall of 88.1%. Using our model,
we successfully identify 202,141 new cross-lingual links
between English Wikipedia and Baidu Baike, which
doubles the number of existing cross-lingual links on
Wikipedia.

The rest of this paper is organized as follows, Section 2
formally deÔ¨Ånes the problem of knowledge linking and some
related concepts; Section 3 presents some motivational anal-
ysis on collected data sets; Section 4 describes the proposed
knowledge linking approach; Section 5 presents the evalua-
tion results; Section 6 outlines some related work and Ô¨Ånally
Section 7 concludes this work.

2. PROBLEM FORMULATION

In this section, we formally deÔ¨Åne the knowledge linking
problem. Here, we Ô¨Årst deÔ¨Åne the Wiki knowledge base as
follows according to mechanism of the Wiki knowledge bases.

Definition 1. A Wiki knowledge base is a collection
of collaboratively written articles, each of which deÔ¨Ånes a
speciÔ¨Åc concept.
It can be formally represented as K =
faign
i=1, where ai is an article in K and n is the size of
K.

Articles are the key elements in a Wiki knowledge base.
Each article describes a speciÔ¨Åc concept. Articles are con-
nected with categories, authors, and other articles. Thus
an article ai 2 K can be represented as a Ô¨Åve-tuple
(T (ai); I(ai); O(ai); C(ai); U (ai)), where T (ai) denotes the
title of the article; O(ai) is the set of outlinks of ai, which
denotes the set of articles that are mentioned in the content
of ai; I(ai) is the set of inlinks of ai, which denotes the set
of articles that link to ai; C(ai) represents category tags of
ai, and U (ai) represents the article‚Äôs authors.

Definition 2. Knowledge linking. Given two Wiki
knowledge bases K1 and K2, knowledge linking is the process
of Ô¨Ånding, for each article ai 2 K1 from knowledge base K1,
an equivalent article aj 2 K2 in knowledge base K2. When
the two Wiki knowledge bases are in diÔ¨Äerent languages, we
call it the cross-lingual knowledge linking problem.

Here, we say two articles are equivalent if they semanti-
cally describe a same subject or topic. Figure 2 shows an
example for a possible knowledge linking result.

As shown in Figure 2, the article ‚ÄúAnaerobic exercise‚Äù is
from English Wikipedia and the other article ‚ÄúÊó†Ê∞ßËøêÂä®‚Äù is
from Baidu Baike. There is not a cross-lingual link from
‚ÄúAnaerobic exercise‚Äù to any article in Chinese on Wikipedi-
a. In the cross-lingual knowledge linking problem, our goal
is to Ô¨Ånd an equivalent article ‚ÄúÊó†Ê∞ßËøêÂä®‚Äù in Baidu Baike
for the English article ‚ÄúAnaerobic exercise‚Äù from Wikipedi-
a. In order to Ô¨Ånd the equivalent relations between articles,
diÔ¨Äerent features of articles can be considered. Figure 2
highlights some useful features in the two articles, including
title, outlinks, categories and authors. Please note that we
are more interested in the link-based features, instead of the
linguistic-based features. This is because the former is more
general and can be easily adapted to other languages while
the latter is heavily dependent on the speciÔ¨Åc language. Giv-
en this, the knowledge base can be represented as a graph
of linked articles, which is referred to as Citation Graph.

Definition 3. Citation Graph. A Wiki knowledge base
is represented as a citation graph CG(K) = (A; L), where

(cid:1)TitleAuthorsCategoriesOutlinksTitleOutlinksCategoriesAuthorsCross-linguallinksWWW 2012 ‚Äì Session: Entity LinkingApril 16‚Äì20, 2012, Lyon, France460Figure 3: Pair-wise connectivity graph.

the node set A represents all the articles in K, edge (ai; aj) 2
L denotes a link from ai to aj, satisfying aj 2 O(ai) and
ai 2 I(aj).

In this paper, we try to solve the knowledge linking prob-
lem as predicting the label (equivalent or not equivalent) of
article pairs between two Wiki knowledge bases. Given two
Wiki knowledge bases, we Ô¨Årst build their Citation Graphs,
and then construct a Pair-wise Connectivity Graph of two
Citation Graphs, which is deÔ¨Åned as follows.

Definition 4. Pair-wise Connectivity Graph. Given
two graphs G1 = (A1; L1) and G2 = (A2; L2), the Pair-wise
Connectivity Graph (PCG) of them is

Table 1: Statistics for our data sets

Knowledge base
English Wikipedia
Chinese Wikipedia

#Articles #Categories #Authors
3,592,495
3,786,000
382,000
91,226

531,771
97,045

Figure 4: Probability of being equivalent condi-
tioned on the number of common links.

P CG(G1; G2) = (V; E)

3.2 Observations

where each element in the node set V denotes a node-pair
between A1 and A2; the set of edges E in P CG(G1; G2) is
established as follows:

(a1; a2) 2 L1 ^ (b1; b2) 2 L2 () ((a1; b1); (a2; b2)) 2 E

Figure 3 shows an example of the constructed PCG of t-
wo graphs. There are two graphs, each of them having 3
nodes. The constructed PCG between them (Cf. the right
of Figure 3) contains 9 nodes representing all the possible
node-pairs of two graphs. PCG can represent the linking
relations of node-pairs between two graphs, we use PCG to
capture the interaction of cross-lingual links between two
Wiki knowledge bases. Our proposed approach takes PCG
of two Wiki knowledge bases as input and solves the knowl-
edge linking problem by predicting the label (equivalent or
inequivalent) of nodes in the PCG.

3. DATA OBSERVATIONS

3.1 Data Collection

What are the fundamental factors underlying the forma-
tion of cross-lingual knowledge links? We Ô¨Årst use existing
cross-lingual links in Wikipedia to investigate which factors
are of important for knowledge linking. Here, we down-
load English Wikipedia and Chinese Wikipedia dumps from
Wikipedia‚Äôs website and extract cross-lingual links between
them. The English Wikipedia dump was archived in April
2011, and the Chinese Wikipedia dump was archived in Oc-
tober 2011. Table 1 shows some statistics of the collected da-
ta sets. We have extracted 180,807 cross-lingual links from
English Wikipedia to Chinese Wikipedia, and 205,608 cross-
lingual links from Chinese to English. Finally, by merging
them together, we obtain 217,689 cross-lingual links. We
have also extracted 35,294 cross-lingual links between cate-
gory pages in Chinese and English.

Based on the above data sets, we Ô¨Årst investigate what
for predicting the cross-lingual
factors will be helpful
links between Wiki articles.
In particular, we study the
correlation of the following factors with the cross-lingual
links: (1) Link homophily: do articles linking to or linked
by equivalent articles tend to be equivalent?
(2) Cate-
gory homophily: do articles have semantically equivalent
category tags tend to be equivalent? (3) Author interest:
are authors‚Äô interests useful for Ô¨Ånding cross-lingual links?
We randomly selected 10,000 English-Chinese article pairs
connected by cross-lingual
links from Wikipedia. We
generate all possible 10; 000 (cid:2) 10; 000 article pairs from the
selected articles; 10,000 of them are equivalent pairs and the
others are inequivalent pairs. Considering all these article
pairs as a sample set, we make some analyses to Ô¨Ågure out
whether the above factors are helpful to knowledge linking.

Link homophily. If two articles cite two other articles
which have an equivalent relationship, we say the two
articles have a common outlink. Similarly, if two articles
are cited by two other equivalent articles, we say they have
a common inlink. We calculate the probabilities of being
equivalent conditioned on the number of common outlinks
and inlinks in the data set. As shown in Figure 4, the prob-
abilities of being equivalent grows as the number of common
outlinks and inlinks increase. It is obvious that the number
of common links is relevant to the equivalent relation of
articles, and the inlinks seems more important than outlinks.

Category homophily. If two articles belong to two cat-
egories which have an equivalent relationship, we say they
have a common category. Figure 5 shows the probability
of two articles be equivalent conditioned on the number
of common categories between them.
It clearly shows a
close correlation between common categories and equivalent
relationship. When the number of common categories is
more than 10, the probability of equivalent relation is close
to 1.0, 20 times higher than the probability when they only

a1a2a3b1b2b3a2 b3a3 b2a3 b3a2t2a1 b2a2 b1a3 b1a1 b3a1 b1








	

	WWW 2012 ‚Äì Session: Entity LinkingApril 16‚Äì20, 2012, Lyon, France461Figure 5: Probability of being equivalent condi-
tioned on the number of common categories.

Figure 7: Percentage of article pairs have common
inlinks, outlinks and categories.

Figure 6: Distribution of author‚Äôs interests.

have one common category.

Author interest. We presume that if two authors share
similar interests, their articles will have a higher probability
to be equivalent than that of two articles edited by random
authors. Here, we simply deÔ¨Åne authors‚Äô interests by those
categories to which the authors‚Äô edited articles belong.
We choose authors who have edited more than 50 articles
in both English and Chinese Wikipedia. Figure 6 shows
the distributions of authors over the number of categories
in two languages respectively.
It appears to be normal
distributions that most authors participated in a reasonable
number of categories, and only a few have extremely
small or large number categories. Therefore, most users
concentrate on a Ô¨Åxed number of categories, and we may
use author interests in the knowledge linking problem.

We also calculate the percentage of article pairs that have
at least one common inlink, outlink, or category respectively.
Figure 7 shows that a large portion of equivalent articles
have common links and categories, with a probability much
higher than that of two random articles. For outlinks, it has
a similar pattern: the probability of two equivalent articles
sharing a common outlink is 15 times higher than that of
two random articles.

According to the above analyses, we have the following

summaries:

(cid:15) Common links and common categories have obvious
correlations with cross-lingual links, but it seems that
if a factor has higher correlation, it will have low cov-
erage of cross-lingual links.

(cid:15) Author interest would be an important factor to iden-

tify the equivalent relationships.

(cid:15) Equivalent articles are very likely to share inlinks, out-

Figure 8: Graphical representation of the linkage
factor graph (LFG) model.

links, or categories. The probability is more than 15
times higher than that of two random articles.

4. THE PROPOSED APPROACH

In this section, we describe our proposed model and its

learning algorithm in detail.
4.1 Linkage Factor Graph Model

Factor graph [22] assumes observation data are cohesive
on both local features and relationships. It has been suc-
cessfully applied in many applications, such as social inÔ¨Çu-
ence analysis [34], social relationship mininig[19, 35, 37, 33],
and linked data disambiguation [10]. In this work, we for-
malize the knowledge linking problem into a linkage factor
graph model, which is shown in Figure 8. Given two Wik-
i knowledge bases K1 = fA1; L1g and K2 = fA2; L2g, let
EL = fei = (ai1 ; bi2 )gp
i=1 be p existing cross-lingual links
2 A2, jA1j = n and
between K1 and K2, ai1
jA2j = m. The input of the LFG model is P CG(K1; K2).
Each node (ai1 ; bi2 ) in P CG(K1; K2) is mapped to a ob-
served variable xi in LFG. There is also a set of hidden
variables Y = fyign(cid:1)m
i=1 , representing the labels (equivalent
or inequivalent) of the observed variables.

2 A1, bi2

We deÔ¨Åne three feature functions in LFG model:
(cid:15) Node feature function: f (yi; xi) is a feature function
which represents the posterior probability of label yi

	
	



	
		
			






		
	




	""&#"& !#&& &&&&& &"&&(cid:1)(cid:1)!
	
	a1a2a3b1b2b3x1x3x2x4x5y1y2y3y5y4y1=?y3=1y5=?y2=0y4=?K1K2f(x1,y1)f(x2,y2)f(x3,y3)f(x4,y4)f(x5,y5)g(y1,y3)g(y1,y4)g(y3,y5)g(y3,y4)h(y1,y2,y5)h(y2,y4,y5)(a2,b3)(a2,b2)(a1,b1)(a3,b3)(a3,b2)WWW 2012 ‚Äì Session: Entity LinkingApril 16‚Äì20, 2012, Lyon, France462given xi;
LFG;

it describes local information on nodes in

(cid:15) Edge feature function: g(yi; G(yi)) denotes the corre-
lation between nodes via the edge on the graph model;
G(yi) is the set of nodes having relations to yi;

(cid:15) Constraint feature function: h(yi; H(yi)) deÔ¨Ånes con-
straints on all relationships, where H(yi) is the set of
relationships constrained on yi.

Based on the LFG model, we can deÔ¨Åne joint distribution

over Y as

‚àè

p(Y ) =

f (yi; xi)g(yi; G(yi))h(yi; H(yi))

(1)

i

In the following part, we introduce the deÔ¨Ånition of three

feature functions in detail.

(1) Node feature function

f (yi; xi) =

expf(cid:11)T f (yi; xi)g

1
Z(cid:11)

(2)

where f =< fout; fin; fcate; fauth > is a vector of feature
functions; (cid:11) deÔ¨Ånes the corresponding weights; and variable
xi corresponds to article pair (ai1 ; bi2 ). Functions fout, fin,
fcate and fauth are similarity functions based on outlinks,
inlinks, categories and authors. The similarity functions are
deÔ¨Åned as follows:

(a) Outlink similarity function:

it computes similarities
between articles based on the equivalent articles in their out-
links.

2 (cid:1) jf(a

‚Ä≤

‚Ä≤

‚Ä≤

)j(a

; b

fout =

; b

‚Ä≤

) 2 EL; a
jO(ai1 )j + jO(bi2 )j

‚Ä≤ 2 O(ai1 ); b

‚Ä≤ 2 O(bi2 )gj

(3)
(b) Inlink similarity function: it computes similarities be-
tween articles based on the equivalent articles in their in-
links.

2 (cid:1) jf(a

‚Ä≤

‚Ä≤
; b

‚Ä≤

)j(a

fin =

‚Ä≤
; b

) 2 EL; a
jI(ai1 )j + jI(bi2 )j

‚Ä≤ 2 I(ai1 ); b

‚Ä≤ 2 I(bi2 )gj

(4)
(c) Category similarity function: it computes similarities
between articles based on the equivalent categories between
them.

2 (cid:1) jf(c; c

‚Ä≤

)j(c; c

fcate =

‚Ä≤

) 2 EC; c 2 C(ai1 ); c
jC(ai1 )j + jC(bi2 )j

‚Ä≤ 2 C(bi2 )gj

(5)
Here EC is a set of equivalent categories from two Wiki

knowledge bases.

(d) Author interest similarity function: it computes sim-
ilarities between articles based on their authors‚Äô mutual in-
terests. In order to compute interest similarity between two
authors, we Ô¨Årst represent each author as a vector of cate-
gories they have participated, then compute the angle of two
authors‚Äô feature vectors, as shown in Figure 9. Let s(u1; u2)
be the interest similarity of two authors, the author interest
similarity of two articles is deÔ¨Åned as

‚àë

‚àë

fauth =

1

jU (ai1 )j (cid:1) jU (bi2 )j

u12U (ai1

)

u22U (bi2

)

s(u1; u2) (6)

Figure 9: An illustration of computing interest simi-
larity between authors (categories connected by red
dash lines are equivalent)

‚àë

yj2G(xi)

(2) Edge feature function

g(yi; G(yi)) =

1
Z(cid:12)

expf

(cid:12)T g(yi; yj)g

(7)

where g(yi; yj) is a function to specify whether there is a link
from node i to node j in the P CG(K1; K2); g(yi; yj) = 1
if there is a edge from node i to node j, otherwise 0. Edge
feature function is used to consider the relations between
nodes in the model, which is based on the assumption that
articles links to other two equivalent articles tend to be e-
quivalent, too. We should notice that similarity functions
fout and fin capture the relations between candidate cross-
lingual links and existing ones, but g(yi; G(yi)) is used to
model the relations within candidate cross-lingual links.

(3) Constraint feature function
Here, we set a rule that one article from K1 can only
have cross-lingual link with one article from K2, which is
consistent with real circumstances. Therefore, we deÔ¨Åne the
constrain feature function as

h(yi; H(yi)) =

1
Z(cid:13)

expf

(cid:13)T h(yi; yj)g

(8)

‚àë

yj2H(yi)

where H(yi) denotes the set of labels conÔ¨Çicting with yi
according to the 1-to-1 linking constraint. h is the contraint
function, h(yi; yj) = 0 if yi = 1 and yj = 1, otherwise 1.
4.2 Model Learning and Inference

Given a set of labeled nodes in the LFG, learning the
model is to estimate a optimum parameter conÔ¨Åguration (cid:18) =
((cid:11); (cid:12); (cid:13)) to maximize the log-likelihood function of p(Y ).
Based on Equtions 1-8, the joint distribution p(Y ) can be
written as

‚àè

expf(cid:18)T (f (yi; yj);

p(Y ) =

=

1
Z

1
Z

i

expf(cid:18)T

‚àë

i

s(yi)g =
‚àë
‚àë

yj

‚àë

yj
1
Z

‚àë

g(yi; yj);

yj

expf(cid:18)T Sg
‚àë

h(yi; yj))g

(9)

where all feature functions for a node yi is brieÔ¨Çy writ-
ten as s(yi) = (f (yi; yj)T ;
h(yi; yj)T )T ;
Z = Z(cid:11)Z(cid:12)Z(cid:13), and S =
i s(yi). Thus, the log-likelihood
objective function is deÔ¨Åned as

g(yi; yj)T ;

yj

u1c2a1c1...a2ak... ...ctu2c'2b1c'1...b2bl... ...c'tVector ComparisonUser Interest similarityAuthorsArticlesCategoriesWWW 2012 ‚Äì Session: Entity LinkingApril 16‚Äì20, 2012, Lyon, France463O((cid:18)) = log p(Y L) = log

‚àë

1
Z
expf(cid:18)T Sg (cid:0) log

Y jY L

expf(cid:18)T Sg
‚àë

expf(cid:18)T Sg

= log

Y jY L

(10)

‚àë

Y

where Y L denotes the known labels and Y jY L is a labeling
conÔ¨Åguration of Y inferred from Y L. In order to maximize
the object function, we adopt a gradient decent method. We
calculate the gradient for each parameter (cid:18)

@(log

‚àë

‚àë
Y jY L expf(cid:18)T Sg (cid:0) log
‚àë
‚àë
Y exp (cid:18)T S (cid:1) S
Y exp (cid:18)T S

‚àë
‚àë
@(cid:18)
Y jY L exp (cid:18)T S (cid:1) S
(cid:0)
Y jY L exp (cid:18)T S
p(cid:18)(Y jY L)S (cid:0) Ep(cid:18)(Y )S

= E

@O((cid:18))

@(cid:18)

=

=

Y expf(cid:18)T Sg)

(11)
p(cid:18)(Y jY L)S and Ep(cid:18)(Y )S are two expectations of S,
where E
which cannot be directly calculated. Here, we use an extend-
ed version of the Loopy Belief Propagation algorithm [35] to
approximate marginal probabilities p(yij(cid:18)) and p(yi; yjj(cid:18)).
The general idea is to use two steps, one step for calculat-
ing E
p(cid:18)(Y jY L)S and the other step for calculating Ep(cid:18)(Y )S,
to estimate the gradient of a parameter (cid:18) wrt the objective
function (Eq. 10). Interested readers please refer to [35] for
details of the algorithm.
After learning the optimal parameters f(cid:18)g, we can infer
the unknown labels by Ô¨Ånding a label conÔ¨Åguration which
maximizes the joint probability p(Y )

(cid:3)

Y

= argmaxY jY L p(Y )

(12)

To do the inference in the above equation, we again per-
form the two-step LBP to compute marginal probabilities.
Finally, each node in LFG is assigned with label that maxi-
mizes the marginal probability.
4.3 Candidate Selection and Distributed

Learning

Finding cross-lingual links between two large Wiki knowl-
edge bases is a challenging problem, because the number
of nodes in LFG model will increase sharply.
In order to
handle large scale knowledge linking problems, we Ô¨Årst use
a candidate selection strategy to reduce the number of n-
odes in LFG, that is only article pairs that have at least one
common outlink are mapped to nodes in the LFG model.
According to the observation on existing cross-lingual links
in Wikipedia, this candidate selection criterion can eliminate
a large number of unnecessary nodes. Thus complexity of
the resultant LFG can be eÔ¨Äectively reduced within a small
loss of recall.

We also implement the learning algorithm of LFG based
on MPI to enable distributed learning.
In the process of
distributed learning, the LFG is Ô¨Årst divided into sever-
al subgraphs that are assigned to slave computing nodes.
Then LBP is performed on each slave nodes to compute the
marginal probabilities and the parameter gradient. There
is a master node collects and sums up all gradients from
subgraphs, and updates parameters by gradient descent
method. For details, please refer to [35].

5. EXPERIMENTS

In this paper, the proposed approach for cross-lingual
knowledge linking is a general model.
It can be used to
Ô¨Ånd cross-lingual links between any Wiki knowledge bases in
diÔ¨Äerent languages. In this section, we Ô¨Årst evaluate our ap-
proach on existing Chinese-English cross-lingual links within
Wikipedia. And then we use our approach to Ô¨Ånd English-
Chinese links between Wikipedia and Baidu Baike.
5.1 Experiment Settings
5.1.1 Dataset
In order to evaluate our approach, we construct a dataset
that contains article pairs from English Wikipedia and Chi-
nese Wikipedia. We randomly select 2,000 English articles
with cross-lingual links to Chinese articles from Wikipedia,
and then pick out the corresponding 2,000 Chinese articles.
2; 000 (cid:2) 2; 000 article pairs are generated from the selected
Chinese and English articles. Among all these article pairs,
those 2,000 pairs linked by cross-lingual links are labeled as
positive examples, and the rest of article pairs are labeled
as negative examples.
5.1.2 Comparison Methods
We deÔ¨Åne four state-of-the-art cross-lingual linking meth-
ods as the comparison methods. They are translation based
method Title Matching (TM), Similarity Aggregation (SA)
based method, classiÔ¨Åcation based method Support Vector
Machine (SVM) and another classiÔ¨Åcation based method
(SVM-SC) based on the work of Sorg and Cimiano [31].

(cid:15) Title Matching (TM). This method Ô¨Årst translates
the titles of Chinese articles into English by Google
Translation API [1], then matches the translated ti-
tles with English articles. For each article pair, if two
articles have strictly the same English titles, they are
considered as equivalent articles.

(cid:15) Similarity Aggregation (SA). This method aggre-
gates diÔ¨Äerent similarities of each article pair into a
combined one. Then for each Chinese article, select
the English article having the largest similarity with
it to establish its cross-lingual link. Here, we compute
outlink similarity, inlink similarity, category similari-
ty and author interest similarity for each article pair,
which are the same as deÔ¨Åned in Section 4. For each
article pair (ai1 ; bi2 ), its diÔ¨Äerent similarities are ag-
gregated by computing their average value:

1
4

Sim(ai1 ; bi2 ) =

(fout + fin + fcate + fauth)

(13)
(cid:15) Support Vector Machine (SVM). This method
Ô¨Årst computes the four similarities deÔ¨Åned in Section 4
for each article pair, and then train a SVM [9] classiÔ¨Å-
cation model on the known cross-lingual links, and pre-
dict the relationships of new article pairs. Compared
to our approach, SVM only consider the similarity of
articles‚Äô local features, it does not take the relations
of predictions and any constraints into account. Here,
we use SVM-Light package [3] in our experiment.

(cid:15) SVM-SC. Sorg and Cimiano [31] deÔ¨Åned several
graph-based and text-based features between Wiki ar-

WWW 2012 ‚Äì Session: Entity LinkingApril 16‚Äì20, 2012, Lyon, France464Figure 10: Performance of knowledge linking with
diÔ¨Äerent methods (%).

Table 2: Contribution analysis of diÔ¨Äerent factors
(%).

Figure 11: TOP-k precision of LFG.

ticles, and also trained a classiÔ¨Åer to Ô¨Ånd missing cross-
lingual links between German Wikipedia and English
Wikipedia. Here we train a SVM with their features
on evaluation dataset, and compare the results with
our approach.

We use precision, recall and F1-score to evaluate diÔ¨Äerent
knowledge linking methods. For SVM, SVM-SC and LFG,
we conduct 3-fold cross validation on the evaluation dataset.
LFG uses 0.001 learning rate and runs 3000 iterations in
all the experiments, SVM runs with the default settings in
SVM-Light package. All experiments are carried out on a
Windows 2008 server with 1.87GHz CPU (4 cores) and 6
GB memory.
5.2 Results Analysis

5.2.1 Performance Comparison
Figure 10 shows the performance of 5 diÔ¨Äerent methods.
According to the result, the TM method gets a really high
precision of 99.5%, but its recall is only 32.1%. SA does
not achieve good results by using simple averaging strategy.
SVM and SVM-SC both use the same classiÔ¨Åcation model
but with diÔ¨Äerent features.
SVM gets better precision
while SVM-SC gets better recall, SVM-SC outperforms
SVM by 0.8% in terms of F1-score. LFG achieves the best
recall and F1-score among all these methods. Compared
to the SA method, LFG outperforms it by 8.0% in terms
of F1-score. LFG and SVM both using the same training
data, they have similar precisions, but LFG outperforms
SVM by 12.3% in terms of recall, and has a 6.5% increase
of F1-score. Therefore, our LFG model can discover more
cross-lingual
links by considering the relations between
article pairs. LFG also performs better than SVM-SC in
terms of both precision and recall.

We also evaluate LFG by TOP-k evaluation. For each
Chinese article, we Ô¨Ånd all the nodes in LFG model related
to it. Then TOP-k nodes are selected according to a ranking
determined by the marginal probabilities p(yi = 1). We
deÔ¨Åne TOP-k-Precision as the percentage of Chinese articles
that have correct equivalent articles in its Top-k candidate

Ignored Factor
Outlinks
Inlinks
Categories
Authors‚Äô Interests
Relations
LFG

Pre. Rec. F1-score
82.2
82.6
84.2
82.0
83.3
85.8

83.0 (-3.9)
82.3 (-4.6)
84.6 (-2.3)
85.1 (-1.8)
83.8 (-3.1)
86.9

83.8
82.0
84.9
88.5
84.2
88.1

set. We set k = 1; 2; :::; 5 respectively and calculate the
TOP-k precisions of LFG.

Figure 11 shows the result of TOP-k evaluation. The
precision grows as k increases, LFG achieves 92.3% precision
when k = 5. Therefore, if we do not want to Ô¨Ånd the exact
cross-lingual links, LFG can also provide candidates of cross-
lingual links of high precision.

5.2.2 Factor contribution analysis
How much does each factor contribute to the LFG model?
In order to get some insight to this question, we perform
an analysis to evaluate the contribution of diÔ¨Äerent factors.
Here, we run LFG 5 times on the evaluation data, and each
time remove one factor from LFG. Table 2 lists the results
of ignoring diÔ¨Äerent factors.

According to the decrement of F1-scores, all these factors
are useful in predicting new cross-lingual links. It is reason-
able to evaluate the importance of each factor by the de-
crease of F1-score without that factor. So we can rank these
factors in a descending order of importance as inlinks, out-
links, relations, categories and authors‚Äô interests. Although
the factor of authors‚Äô interests is less important than oth-
er factors, we Ô¨Ånd it is indeed helpful to improve the per-
formance of LFG. Also, LFG achieves a 3.1% increase of
F1-score by considering the relations among article pairs.
5.3 Discover new links between Wikipedia

and Baidu

The motivation of our work is to Ô¨Ånd cross-lingual links
across Wiki knowledge bases to get more equivalent arti-
cle pairs in diÔ¨Äerent languages. Therefore, we use LFG to
discover cross-lingual links between English Wikipedia and
Baidu Baike (a large scale Chinese Wiki knowledge base).

	##"
!"#!"#!"#
"!""

"#""""""" # !"#							
					
	
WWW 2012 ‚Äì Session: Entity LinkingApril 16‚Äì20, 2012, Lyon, France465Table 3: Examples of discovered cross-lingual links.

Chinese articles
ËÇØÂ∞ºÊñØ¬∑ÂÖãÊãâÂÖã
Êµ∑Áëü¬∑Ê†ºÊãâÊ±âÂßÜ
Êù∞Â§´‰∏π¬∑Â∞ºÂ∞îÊñØ
‰∏πÂ∞ºÂ∞î¬∑ÂÖãÈõ∑Ê†º
Êù∞ÂÖã¬∑È∫¶Âç°Èî°
ÊãâÂ∞î¬∑Â§´Ë¥ùÊãâÁ±≥
ÂÆâËíÇÁßëÊñØËíÇÂ≤õ
Èü¶Èü¶ÁâπÂçóÊàà
Âú£ËÉ°ÂÆâÁæ§Â≤õ
‰∫öÊãâÂ∑¥È©¨Ê≤≥
ÊõºÂæ∑ÂãíÂ±±
Â••ÊñØÈôÜÂ∏ÇÊîøÂéÖ

Types English articles
Kenneth Clark
Per.
Heather Graham
JeÔ¨Ä Daniels
Daniel Craig
Mick McCarthy
Ralph Bellamy
Anticosti Island
Huehuetenango
San Juan Islands
Alabama River
Mandalay Hill
Oslo City Hall
Yale University Library ËÄ∂È≤ÅÂ§ßÂ≠¶Âõæ‰π¶È¶Ü
University of Troms
American MaÔ¨Åa
America West Airlines ÁæéÂõΩË•øÈÉ®Ëà™Á©∫ÂÖ¨Âè∏
Superconductivity
Wave propagation
Basal cell carcinoma
Pleural eÔ¨Äusion
Mildew

Ë∂ÖÂØºÁîµÊÄß
ÁîµÊ≥¢‰º†Êí≠
Âü∫Â∫ïÁªÜËÉûÁôå
ËÉ∏ËÖîÁßØÊ∂≤
ÈúâËèå

ÁâπÁΩóÂßÜÁëüÂ§ßÂ≠¶
ÁæéÂõΩÈªëÊâãÂÖö

Loc.

Org.

Ter.

cross-lingual links between English and Japanese. Their
method works in two steps, it Ô¨Årst selects candidates of miss-
ing links based on cross-lingual similarities between English
and Japanese Wikipedia articles, and then trains a classiÔ¨Å-
er to predict whether a given candidate of missing links is
correct or not. These two methods try to Ô¨Ånd the missing
links within Wikipedia, while our proposed approach aims
to Ô¨Ånd cross-lingual links across diÔ¨Äerent Wiki knowledge
bases. Furthermore, some features used by these two meth-
ods are not available in the task of Ô¨Ånding cross-lingual links
between Wikipedia and other Wiki knowledge bases. For ex-
ample, Sorg and Cimiano used the orthographical similarity
between English and German, which cannot be calculated
between other language pairs, such as English and Chinese.
The method proposed by Oh et al. used common images as a
feature, which cannot be used across Wiki knowledge bases.
Although both of the two methods train SVM to predict
new cross-lingual links, they did not consider the relations
between predictions.

Recently, many projects based on the cross-lingual links
of Wikipeida have been proposed. DBpedia [6][4] is a knowl-
edge based built by extracting structured information from
Wikipedia. Currently DBpedia has described more than 3.5
million things, and 1.67 million of these things are classi-
Ô¨Åed in a consistent ontology. The DBpedia ontology or-
ganized things into persons, places, music albums, Ô¨Ålms,
video games, organizations, species and diseases. Erdmann
et al. [15] extracted a dictionary from Wikipedia by ana-
lyzing the link structure of Wikipdia.
In addition to the
cross-lingual links, they also explore the redirect page, link
text to extend the coverage of the built dictionary. They
Ô¨Årst constructed a baseline dictionary by exploring the cross-
lingual links in Wikipedia; and then extracted more trans-
lation candidates from redirect page and link text informa-
tion. MENTA [11] is a multilingual entity taxonomy built
from Wikipedia and WordNet. By aggregating unreliable
taxonomic links between entities from diÔ¨Äerent language ver-

Figure 12:
inter-language links between articles.

Illustration of existing and discovered

We crawled 3,941,659 articles from Baidu Baike, which are
edited by 1,454,204 authors and organized in 599,463 cate-
gories.

There have already been 217,689 cross-lingual links of ar-
ticles and 35,294 cross-lingual links of categories between
English Wikipedia and Chinese Wikipedia. Among those
linked Chinese articles and categories, we have found 96,970
equivalent articles and 10,350 equivalent categories in Baidu
Baike. Therefore, we establish 96,970 initial cross-lingual
links of articles between English Wikipedia and Baidu Baike,
and 10,350 initial cross-lingual links of categories between
English Wikipedia and Baidu Baike. Based on our candidate
selection method, we choose 3,082,751 English articles and
963,788 Chinese articles for LFG to discover cross-lingual
links between them. We run LFG on a server with 1.87GHz
CPU (4 cores) and 6 GB memory. It costs 17 hour 32 min-
utes to Ô¨Ånally get 202,141 cross-lingual links between English
Wikipedia and Baidu Baike. Figure 12 shows the relation
of existing and discovered inter-language links between ar-
ticles. Table 3 lists some of these cross-lingual links. There
are generally four types these links, including persons (Per.),
locations (Loc.), organizations (Org.) and scientiÔ¨Åc terms
(Ter.). The discovered links are not available in Wikipedia.
By using our approach, the number of cross-lingual links has
been doubled.

6. RELATED WORK

In this section, we review some related work.

6.1 Discovering Missing Cross-lingual Links

in Wikipedia

A group of highly related work is to discover missing cross-
lingual links within Wikipedia. As more and more researches
use the cross-lingual links of Wikipedia to build multilin-
gual lexical resources, the problem of missing cross-lingual
links has attracted increasingly attention. The missing of
cross-lingual links means that there are corresponding arti-
cles within two languages, but there is no direct cross-lingual
link between them. In order to solve this problem and en-
rich the cross-lingual links in Wikipedia, several approach-
es have been proposed. Sorg and Cimiano [31] proposed a
method to Ô¨Ånd missing cross-lingual links between English
and German. Their method makes use of the link struc-
ture of articles to Ô¨Ånd candidates of missing links. And
then a classiÔ¨Åer is trained based on several graph-based fea-
tures and text-based features to predict the missing links.
Oh et al. [29] proposed a method for discovering missing

English WikipediaWzhChinese WikipediaWzhBaidu BaikeBzhExisting inter-language links217,689Matched Chinese articles96,970Discovered new inter-language links202,141WWW 2012 ‚Äì Session: Entity LinkingApril 16‚Äì20, 2012, Lyon, France466sions of Wikipedia, a single more reliable and coherent tax-
onomy is build. HeiNER [39] is a multilingual Heidelberg
Named Entity Resource, which translates Named Entities
into the various target languages by exploiting cross-lingual
information contained in the online encyclopedia Wikipedia.
BabelNet [26] is a large multilingual semantic network built
from Wikipedia and WordNet, which provides concepts and
named entities lexicalized in many languages and connected
with large amounts of semantic relations. Hassan et al. [18]
address the task of cross-lingual semantic relatedness by ex-
ploiting the cross-lingual links available between Wikipedia
versions in multiple languages. Ye et al. [40] proposed a
graph-based approach to constructing a multilingual associ-
ation dictionary from Wikipedia. The extracted association
dictionary is applied in cross language information retrieval.
6.2 Ontology and Instance Matching

Ontology and instance matching is another related prob-
lem. As the development of the Linked Data project [2], on-
tology and instance matching is attracting more and more
interests. The goal of otology and instance matching is to
Ô¨Ånd equivalent elements between two heterogeneous seman-
tic data sources. Currently, most work focus on monolingual
matching tasks, such as Silk [36],idMesh [10],KnoFuss [28].
Some approaches such as SOCOM [17], RiMOM [24][32][41]
and [16] deal with the cross-lingual ontology matching, they
mainly use the machine translation tools to bridge the gap
between languages. Our approach uses only language inde-
pendent features of Wiki articles, which does not need any
translation tools.
6.3 Record linkage

Record linkage is to identify records in the same or diÔ¨Äer-
ent databases that refer to the same real-world entity [14].
An record linkage approach typically compares various Ô¨Åelds
of database records, and either matches records based on do-
main knowledge and generic distance metrics, or applies su-
pervised machine learning techniques to learn how to match
the records [21]. Approaches based on supervised learning
techniques include [7][5][8] . Unsupervised record linkage ap-
proaches include [27][38]. Some tools such as TAILOR [13]
have been proposed for record linkage applications. Re-
searches on record linkage try to Ô¨Ånd equivalent objects in
databases, while our approach aims to Ô¨Ånd equivalent arti-
cles across Wiki knowledge bases in two diÔ¨Äerent languages.
Although the problem of record linkage has been studied for
decades, few works on cross-lingual record matching have
been proposed.

7. CONCLUSION AND FUTURE WORK

In this paper, we propose a cross-lingual knowledge linking
approach for building cross-lingual links across Wiki knowl-
edge bases. Our approach uses only language-independent
features of article, and employs a graph model to predict
new cross-lingual links. Evaluations on existing cross-lingual
links in Wikipedia shows that our approach can achieve high
precision 85.8% with a recall of 88.1%. Using our approach,
we are able to Ô¨Ånd 202,141 new cross-lingual links between
English Wikipedia and Baidu Baike.

Because the number of initial links is relative small based
on the existing cross-lingual links in Wikipedia, we can Ô¨Ånd
only part of new links between Wikipedia and Baidu Baike.
If we add discovered new links to existing links, and use

the merged links as seed, our approach can iteratively dis-
cover new linked articles. Therefore, our future work is to
extending our approach to an iterative one, to Ô¨Ånd more
cross-lingual links.

8. ACKNOWLEDGEMENTS

We thank Honglei Zhuang and Wenbin Tang for providing
their software and useful suggestions. We also thank anony-
mous reviewers for valuable suggestions and comments.
This work is supported by the National Natural Science
Foundation of China (No. 61073073, 61035004, 661035004,
60973102), 863 high technology program (2011AA01A207)
and European Union 7th framework project FP7-288342.
It is also partially supported by THU-NUS Next research
center.

9. REFERENCES
[1] http://code.google.com/intl/zh-

cn/apis/language/translate/overview.html.

[2] http://linkeddata.org/.
[3] http://svmlight.joachims.org/.
[4] S. Auer, C. Bizer, G. Kobilarov, J. Lehmann,

R. Cyganiak, and Z. G. Ives. Dbpedia: A nucleus for a
web of open data. In ISWC/ASWC‚Äô07, pages 722‚Äì735,
2007.

[5] M. Bilenko, R. Mooney, W. Cohen, P. Ravikumar, and

S. Fienberg. Adaptive name matching in information
integration. Intelligent Systems, IEEE, 18(5):16 ‚Äì 23,
2003.

[6] C. Bizer, J. Lehmann, G. Kobilarov, S. Auer,

C. Becker, R. Cyganiak, and S. Hellmann. Dbpedia - a
crystallization point for the web of data. Web
Semantics: Science, Services and Agents on the World
Wide Web, 7(3):154 ‚Äì 165, 2009.

[7] M. Cochinwala, V. Kurien, G. Lalk, and D. Shasha.

EÔ¨Écient data reconciliation. Information Sciences,
137(1-4):1 ‚Äì 15, 2001.

[8] W. W. Cohen and J. Richman. Learning to match and

cluster large high-dimensional data sets for data
integration. In Proceedings of SIGKDD‚Äô02, pages
475‚Äì480, 2002.

[9] C. Cortes and V. Vapnik. Support-Vector Networks.

Machine Learning, 20(3):273‚Äì297, Sept. 1995.

[10] P. Cudre-Mauroux, P. Haghani, M. Jost, K. Aberer,

and H. De Meer. idmesh: graph-based disambiguation
of linked data. In Proceedings of WWW ‚Äô09, pages
591‚Äì600, 2009.

[11] G. de Melo and G. Weikum. Menta: inducing

multilingual taxonomies from wikipedia. In
Proceedings of CIKM‚Äô10, pages 1099‚Äì1108, 2010.

[12] Z. Dong and Q. Dong. Hownet And the Computation

of Meaning. World ScientiÔ¨Åc Publishing Co., Inc.,
River Edge, NJ, USA, 2006.

[13] M. Elfeky, V. Verykios, and A. Elmagarmid. Tailor: a

record linkage toolbox. In Proceedings of ICDE‚Äô02,
pages 17 ‚Äì28, 2002.

[14] A. K. Elmagarmid, P. G. Ipeirotis, and V. S. Verykios.

Duplicate record detection: A survey. IEEE
Transactions on Knowledge and Data Engineering,
19:1‚Äì16, 2007.

WWW 2012 ‚Äì Session: Entity LinkingApril 16‚Äì20, 2012, Lyon, France467[15] M. Erdmann, K. Nakayama, T. Hara, and S. Nishio.

[28] A. Nikolov, V. S. Uren, E. Motta, and A. N. D. Roeck.

Improving the extraction of bilingual terminology
from wikipedia. ACM Transactions on Multimedia
Computing, Communications, and Applications,
5:31:1‚Äì31:17, November 2009.

[16] B. Fu and R. Brennan. Cross-lingual ontology

mapping and its use on the multilingual semantic web.
In Proceedings of WWW Workshop on Multilingual
Semantic Web, 2010.

[17] B. Fu, R. Brennan, and D. O‚ÄôSullivan. Cross-lingual
ontology mapping ‚Äì an investigation of the impact of
machine translation. In A. G¬¥omez-P¬¥erez, Y. Yu, and
Y. Ding, editors, Proceedings of ASWC ‚Äô09, volume
5926, pages 1‚Äì15, 2009.

[18] S. Hassan and R. Mihalcea. Cross-lingual semantic

relatedness using encyclopedic knowledge. In
Proceedings of EMNLP ‚Äô09, volume 3, pages
1192‚Äì1201, 2009.

[19] J. Hopcroft, T. Lou, and J. Tang. Who will follow you

back? reciprocal relationship prediction. In
Proceedings of CIKM‚Äô11, 2011.

[20] G. J. Jones, F. Fantino, E. Newman, and Y. Zhang.

Domain-speciÔ¨Åc query translation for multilingual
information access using machine translation
augmented with dictionaries mined from wikipedia. In
Proceedings of CLIA ‚Äô08, 2008.

[21] N. Koudas, S. Sarawagi, and D. Srivastava. Record

linkage: similarity measures and algorithms. In
Proceedings of SIGMOD ‚Äô06, pages 802‚Äì803, 2006.

[22] F. Kschischang, B. Frey, and H.-A. Loeliger. Factor

graphs and the sum-product algorithm. IEEE
Transactions on Information Theory, 47(2):498‚Äì519,
2001.

[23] D. B. Lenat. Cyc: a large-scale investment in

knowledge infrastructure. Communications of the
ACM, 38:33‚Äì38, November 1995.

[24] J. Li, J. Tang, Y. Li, and Q. Luo. Rimom: A dynamic

multistrategy ontology alignment framework. IEEE
Transactions on Knowledge and Data Engineering,
21(8):1218‚Äì1232, 2009.

[25] G. A. Miller. Wordnet: a lexical database for english.

Communications of the ACM, 38:39‚Äì41, November
1995.

Handling instance coreferencing in the knofuss
architecture. In In Proceedings of IRSW‚Äô08, volume
422, 2008.

[29] J.-H. Oh, D. Kawahara, K. Uchimoto, J. Kazama, and

K. Torisawa. Enriching multilingual language
resources by discovering missing cross-language links
in wikipedia. In Proceedings of WI-IAT ‚Äô08, volume 1,
pages 322 ‚Äì328, 2008.

[30] M. Potthast, B. Stein, and M. Anderka. A

wikipedia-based multilingual retrieval model. In
Proceedings of ECIR‚Äô08, pages 522‚Äì530, 2008.

[31] L. Sorg and P. Cimiano. Enriching the crosslingual
link structure of Wikipedia - A classiÔ¨Åcation-based
approach. In AAAI 2008 Workshop on Wikipedia and
ArtiÔ¨Åcal Intelligence, 2008.

[32] J. Tang, J. Li, B. Liang, X. Huang, Y. Li, and
K. Wang. Using bayesian decision for ontology
mapping. Web Semantics: Science, Services and
Agents on the World Wide Web, 4(4):243‚Äì262, 2006.

[33] J. Tang, T. Lou, and J. Kleinberg. Inferring social ties

across heterogenous networks. In Proceedings of
WSDM‚Äô12, pages 743‚Äì752, 2012.

[34] J. Tang, J. Sun, C. Wang, and Z. Yang. Social

inÔ¨Çuence analysis in large-scale networks. In
Proceedings of SIGKDD‚Äô09, pages 807‚Äì816, 2009.

[35] W. Tang, H. Zhuang, and J. Tang. Learning to infer

social ties in large networks. In Proceedings of
ECML/PKDD‚Äô11, pages 381‚Äì397, 2011.

[36] J. Volz, C. Bizer, M. Gaedke, and G. Kobilarov.

Discovering and maintaining links on the web of data.
In Proceedings of ISWC ‚Äô09, pages 650‚Äì665, 2009.

[37] C. Wang, J. Han, Y. Jia, J. Tang, D. Zhang, Y. Yu,

and J. Guo. Mining advisor-advisee relationships from
research publication networks. In Proceedings of
KDD‚Äô10, pages 203‚Äì212, 2010.

[38] W. E. Winkler. Methods for record linkage and

bayesian networks. Technical report, Series
RRS2002/05, U.S. Bureau of the Census, 2002.
[39] C. S. Wolodja Wentland, Johannes Knopp and

M. Hartung. Building a multilingual lexical resource
for named entity disambiguation, translation and
transliteration. In Proceedings of LREC‚Äô08, 2008.

[26] R. Navigli and S. P. Ponzetto. Babelnet: building a

[40] Z. Ye, X. Huang, and H. Lin. A graph-based approach

very large multilingual semantic network. In
Proceedings of ACL ‚Äô10, pages 216‚Äì225, 2010.

[27] H. B. Newcombe, J. M. Kennedy, S. J. Axford, and

A. P. James. Automatic linkage of vital records.
Science, 130(3381):954‚Äì959, 1959.

to mining multilingual word associations from
wikipedia. In Proceedings of SIGIR‚Äô09, pages 690‚Äì691,
2009.

[41] X. Zhang, Q. Zhong, F. Shi, J. Li, and J. Tang.

Rimom results for oaei 2009. In Proceedings of ISWC
Workshop on Ontology Matching, 2009.

WWW 2012 ‚Äì Session: Entity LinkingApril 16‚Äì20, 2012, Lyon, France468