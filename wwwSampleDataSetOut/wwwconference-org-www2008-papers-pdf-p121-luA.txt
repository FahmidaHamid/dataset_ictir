Opinion Integration Through Semi-supervised Topic

Modeling

Yue Lu

Department of Computer Science

University of Illinois at Urbana-Champaign

Urbana, IL 61801

yuelu2@uiuc.edu

Chengxiang Zhai

Department of Computer Science

University of Illinois at Urbana-Champaign

Urbana, IL 61801
czhai@uiuc.edu

ABSTRACT
Web 2.0 technology has enabled more and more people to
freely express their opinions on the Web, making the Web an
extremely valuable source for mining user opinions about all
kinds of topics. In this paper we study how to automatically
integrate opinions expressed in a well-written expert review
with lots of opinions scattering in various sources such as
blogspaces and forums. We formally deÔ¨Åne this new integra-
tion problem and propose to use semi-supervised topic mod-
els to solve the problem in a principled way. Experiments on
integrating opinions about two quite diÔ¨Äerent topics (a prod-
uct and a political Ô¨Ågure) show that the proposed method is
eÔ¨Äective for both topics and can generate useful aligned in-
tegrated opinion summaries. The proposed method is quite
general. It can be used to integrate a well written review
with opinions in an arbitrary text collection about any topic
to potentially support many interesting applications in mul-
tiple domains.

Categories and Subject Descriptors: B.3.3 [Informa-
tion Search and Retrieval]: Text Mining

General Terms: Algorithms

Keywords: opinion integration, semi-supervised, proba-
bilistic topic modeling, expert review

1.

INTRODUCTION

As Web 2.0 applications become increasingly popular, more
and more people express their opinions on the Web in various
ways such as customer reviews, forums, discussion groups,
and Weblogs. The wide coverage of topics and abundance
of opinions make the Web an extremely valuable source for
mining user opinions about all kinds of topics (e.g., products,
political Ô¨Ågures, etc.). However, with such a large scale of
information source, it is quite challenging for a user to inte-
grate and digest all the opinions from diÔ¨Äerent sources.

In general, for any given topic (e.g., a product), there are
often two kinds of opinions: the Ô¨Årst is opinions expressed
in some well-structured relatively complete review typically
written by some expert about the topic and the second is
fragmental opinions scattering around in all kinds of sources
such as blog articles and forums. For convenience of discus-
sion, we will refer to the Ô¨Årst kind as expert opinions and
the second ordinary opinions. The expert opinions are rela-
tively easy for a user to access through some opinion search
Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2008, April 21‚Äì25, 2008, Beijing, China.
ACM 978-1-60558-085-2/08/04.

website such as CNET. Because a comprehensive product
review is often written carefully, it is also easy for a user
to digest expert opinions. However, Ô¨Ånding, integrating,
and digesting ordinary opinions pose signiÔ¨Åcant challenges
as they are scattering in many diÔ¨Äerent sources, and are
generally fragmental and not well structured. While expert
opinions are clearly very useful, they may be biased and of-
ten out of date after a while. In contrast, ordinary opinions
tend to represent the general opinions of a large number
of people and get refreshed quickly as people dynamically
generate new content. For example, a query ‚ÄúiPhone‚Äù re-
turns 330,431 matches in Google‚Äôs blogsearch (as of Nov. 1,
2007), suggesting that there are many opinions expressed
about iPhone in blog articles within a short period of time
since it hit the market. To enable a user to beneÔ¨Åt from both
kinds of opinions, it is thus necessary to automatically inte-
grate these two kinds of opinions and present an integrated
opinion summary to a user.

To the best of our knowledge, such an integration prob-
lem has not been studied in the existing work. In this pa-
per, we study how to integrate a well-written expert review
about an arbitrary topic with many ordinary opinions ex-
pressed in a text collection such as blog articles. We pro-
pose a general method to solve this integration problem in
three steps: (1) extract ordinary opinions from text using in-
formation retrieval; (2) summarize and align the extracted
opinions to the expert review to integrate the opinions; (3)
further separate ordinary opinions that are similar to ex-
pert opinions from those that are not. Our main idea is
to take advantage of the high readability of the expert re-
view to structure the unorganized ordinary opinions while at
the same time summarizing the ordinary opinions to extract
representative opinions using the expert review as guidance.
From the viewpoint of text data mining, we are essentially
to use the expert review as a ‚Äútemplate‚Äù to mine text data
for ordinary opinions. The Ô¨Årst step in our approach can
be implemented with a direct application of information re-
trieval techniques. Implementing the second and third steps
involves special challenges. In particular, without any train-
ing data, it is unclear how we should align ordinary opinions
to an expert review and separate similar and supplementary
opinions. We propose a semi-supervised topic modeling ap-
proach to solve these challenges. SpeciÔ¨Åcally, we cast the
expert review as a prior in a probabilistic topic model (i.e.,
PLSA[6]) and Ô¨Åt the model to the text collection with the
ordinary opinions with Maximum A Posterior (MAP) es-
timation. With the estimated probabilistic model, we can
then naturally obtain alignments of opinions as well as ad-

121WWW 2008 / Refereed Track: Data Mining - ModelingApril 21-25, 2008 ¬∑ Beijing, Chinaditional ordinary opinions that cannot be well-aligned with
the expert review. The separation of similar and supple-
mentary opinions can also be achieved with a similar model.
We evaluate our method on integrating opinions about two
quite diÔ¨Äerent topics. One is a popular product ‚ÄúiPhone‚Äù,
and the other is a popular political Ô¨Ågure Barack Obama.
Experiment results show that our method can eÔ¨Äectively in-
tegrate the expert review (a produce review from CNET for
iPhone and a short biography from Wikipedia for Barack
Obama) with ordinary opinions from blog articles.

This paper makes the following contributions:

1. We deÔ¨Åne a new problem of opinion integration. To
the best of our knowledge, there is no existing work
that solves this problem.

2. We propose a new semi-supervised topic modeling ap-
proach for integrating opinions scattered around in
text articles with those in a well-written expert review
for an arbitrary topic.

3. We evaluate the proposed method both qualitatively
and quantitatively. The results show that our method
is eÔ¨Äective for integrating opinions about quite diÔ¨Äer-
ent topics.

Collecting and digesting opinions about a topic is critical
for many tasks such as shopping, medical decision making,
and social interactions. Our proposed method is quite gen-
eral and can be applied to integrate opinions about any topic
in any domain, thus potentially has many interesting appli-
cations.

The rest of the paper is organized as follows. In Section 2,
we formally deÔ¨Åne the novel problem of opinion integration.
After that, we present our Semi-supervised Topic Model in
Section 4. We discuss our experiments and results in Sec-
tion 5. Finally, we conclude in Section 7.

2. PROBLEM DEFINITION

In this section, we deÔ¨Åne the novel problem of opinion

integration.

Given an expert review about a topic T (e.g., ‚ÄúiPhone‚Äù or
‚ÄúBarack Obama‚Äù) and a collection of text articles (e.g., blog
articles), our goal is to extract opinions from text articles
and integrate them with those in the expert review to form
an integrated opinion summary.

The expert review is generally well-written and coher-
ent, thus we can view it as a sequence of semantically co-
herent segments, where a segment could be a sentence, a
paragraph, or other meaningful segments (e.g., paragraphs
corresponding to product features) available in some semi-
structured review. Formally, we denote the expert review
by R = {r1, ..., rk} where ri is a segment. Since we can al-
ways treat a sentence as a segment, this deÔ¨Ånition is quite
general.

The text collection is a set of text documents where or-
dinary opinions are expressed and can be represented as
C = {d1, ..., d|C|} where di = (si1, ..., si|di|) is a document
and sij is a sentence. To support opinion integration in a
general and robust manner, we do not rely on extra knowl-
edge to segment documents to obtain opinion regions; in-
stead, we treat each sentence as an opinion unit. Since a
sentence has a well-deÔ¨Åned meaning, this assumption is rea-
sonable. To help a user interpret any opinion sentence, in

Figure 1: Problem Setup

real applications, we would link each extracted opinion sen-
tence back to the original document to facilitate navigating
into the original document and obtaining context of an opin-
ion.

We would like our integrated opinion summary to include
both opinions in the expert review and those most repre-
sentative opinions in the text collection. Since the expert
review is well written, we keep their original form and lever-
age its structure to organize the ordinary opinions extracted
from text. To quantify the representativeness of an ordinary
opinion sentence, we will compute a ‚Äúsupport value‚Äù for each
extracted ordinary opinion sentence. SpeciÔ¨Åcally, we would
like to partition the extracted ordinary opinion sentences
into groups that can be potentially aligned with all the re-
view segments r1, ..., rk. Naturally, there may also be some
groups with extra ordinary opinions that are not alignable
with any expert opinion segment, and these opinions can be
very useful to augment the expert review with additional
opinions.

Furthermore, for opinions aligned to a review segment ri,
we would like to further separate those that are similar to
ri from those that are supplementary for ri; such separation
can allow a user to digest the integrated opinions more easily.
Finally, if ri has multiple sentences, we can further align
each ordinary opinion sentence (both ‚Äúsimilar‚Äù and ‚Äúsupple-
mentary‚Äù) with a sentence in ri to increase the readability.
This problem setup is illustrated in Figure 1. We now

deÔ¨Åne the problem more formally.

DeÔ¨Ånition (Representative Opinion (RO)) A repre-
sentative opinion(RO) is an ordinary opinion sentence ex-
tracted from the text collection with a support value. For-
mally, we denote it by oij = (Œ≤, sij) where Œ≤ ‚àà [1, +‚àû) is
a support value indicating how many sentences this opinion
sentence can represent, and sij is a sentence in document di.

Since ordinary opinions tend to be redundant and we are
primarily interested in extracting representative opinions,

Aspect r1...Aspect rkAspect r2Blog Collection...Aspect r1Aspect rkAspect r2...Aspect rk+1Aspect rk+m...Similar OpinionsSupplementaryOpinionsScattered OpinionsBlog CollectionPartition ofReview Article 122WWW 2008 / Refereed Track: Data Mining - ModelingApril 21-25, 2008 ¬∑ Beijing, Chinathe support can be very useful to assess the representative-
ness of an extracted opinion.
Let RO(C) be all the possible representative opinion sen-
tences in C. We can now deÔ¨Åne the integrated opinion sum-
mary that we would like to generate as follows.

DeÔ¨Ånition (Integrated Opinion Summary) An inte-
grated opinion summary of R and C is a tuple
(R, Ssim, Ssupp, Sextra) where (1) R is the given expert re-
}
view; (2) Ssim = {Ssim
, ..., Ssupp
k
are similar and supplementary representative opinion sen-
‚äÇ
, Ssupp
tences, respectively, that can be aligned to R, and Ssim
RO(C) are sets of representative opinion sentences; (3) Sextra ‚äÇ
RO(C) is a set of extra representative opinion sentences that
cannot be aligned with R.

} and Ssupp = {Ssupp

1

, ..., Ssim

k

1

i

j

Note that we deÔ¨Åne ‚Äúopinion‚Äù broadly as covering all the
discussion about a topic in opinionate sources such as blog
spaces and forums. The notion of ‚Äúopinion‚Äù is quite vague;
we adopt this broad deÔ¨Ånition to ensure generality of the
problem set up and its solutions.
In addition, any exist-
ing sentiment analysis technique could be applied as a post-
processing step. But since we only focus on the integration
problem in this paper, we will not cover sentiment analysis.

3. OVERVIEW OF PROPOSED APPROACH
The opinion integration problem as deÔ¨Åned in the previous
section is quite diÔ¨Äerent from any existing problem setup for
opinion extraction and summarization, and it presents some
special challenges: (1) How can we extract representative
opinion sentences with support information? (2) How can
we distinguish alignable opinions from non-alignable opin-
ions? (3) For any given expert review segment, how can we
distinguish similar opinions from those that are supplemen-
tary? (4) In the case when a review segment ri has mul-
tiple sentences, how can we align a representative opinion
to a sentence in ri? In this section, we present our overall
approach to solving all these challenges, leaving a detailed
presentation to the next section.

At a high level, our approach primarily consists of two
stages and an optional third stage: In the Ô¨Årst stage, we
retrieve only the relevant opinion sentences from C using
the topic description T as a query. Let CO be the set of
all the retrieved relevant opinion sentences. In the second
stage, we use probabilistic topic models to cluster sentences
in CO and obtain Ssim, Ssupp and Sextra. When ri has
multiple sentences, we have a third stage, in which we again
use information retrieval techniques to align any extracted
representative opinion to a sentence of ri. We now describe
each of the three stages in detail.

The purpose of the Ô¨Årst stage is to Ô¨Ålter out irrelevant
sentences and opinions in our collection. This can be done
by using the topic description as a keyword query to retrieve
relevant opinion sentences. In general, we may use any re-
trieval method. In this paper, we used a standard language
modeling approach (i.e., the KL-divergence retrieval model
[20]). To ensure coverage of opinions, we perform pseudo
feedback using some top-ranked sentences; the idea is to
expand the original topic description query with additional
words related to the topic so that we can further retrieve
opinion sentences that do not necessarily match the original
topic description T . After this retrieval stage, we obtain a
set of relevant opinion sentences CO.

In the second stage, our main idea is to exploit a proba-
bilistic topic model, i.e., Probabilistic Latent Semantic Anal-
ysis (PLSA) with conjugate prior [6, 11] to cluster opinion
sentences in a special way so that there will be precisely one
cluster corresponding to each segment ri in the expert re-
view. These clusters are to collect opinion sentences that can
be aligned with a review segment. There will also be some
clusters that are not aligned with any review segments, and
they are designed to collect extra opinions. Thus the model
provides an elegant way to simultaneously partition opin-
ions and align them to the expert review. Interestingly, the
same model can also be adapted to further partition opinions
aligned to a review segment into similar and supplementary
opinions. Finally, a simpliÔ¨Åed version of the model (i.e.,
no prior, basic PLSA) can be used to cluster any group of
sentences to extract representative opinion sentences. The
support of a representative opinion is deÔ¨Åned as the size of
the cluster represented by the opinion sentences.

Note that what we need in this second stage is semi-
supervised clustering in the sense that we would like to con-
strain many of the clusters so that they would correspond to
the segments ris in the expert review. Thus a direct applica-
tion of any regular clustering algorithm would not be able to
solve our problem. Instead of doing clustering, we can also
imagine using each expert review segment ri as a query to
retrieve similar sentences. However, it would be unclear how
to choose a good cutoÔ¨Ä point on the ranked list of retrieved
results. Compared with these alternative approaches, PLSA
with conjugate prior provides a more principled and uniÔ¨Åed
way to tackle all the challenges.

In the optional third stage, we have a review segment
ri with multiple sentences and we would like to align all
extracted representative opinions to the sentences in ri. This
can be achieved by using each representative opinion as a
query and retrieve sentences in ri. Once again, in general,
any retrieval method can be used. In this paper, we again
used the KL-divergence retrieval method.

From the discussion above, it is clear that we leverage both
information retrieval techniques and text mining techniques
(i.e., PLSA), and our main technical contributions lie in the
second stage where we repeatedly exploit semi-supervised
topic modeling to extract and integrate opinions. We de-
scribe this step in more detail in the next section.

4. SEMI-SUPERVISED PLSA FOR OPINION

INTEGRATION

Probabilistic latent semantic analysis (PLSA) [6] and its
extensions [21, 13, 11] have recently been applied to many
text mining problems with promising results. Our work adds
to this line yet another novel use of such models for opinion
integration.

As in most topic models, our general idea is to use a uni-
gram language model (i.e., a multinomial word distribution)
to model a topic. For example, a distribution that assigns
high probabilities to words such as ‚ÄúiPhone‚Äù, ‚Äúbattery‚Äù, ‚Äúlife‚Äù,
‚Äúhour‚Äù, would suggest a topic such as ‚Äúbattery life of iPhone.‚Äù
In order to identify multiple topics in text, we would Ô¨Åt a
mixture model involving multiple multinomial distributions
to our text data and try to Ô¨Ågure out how to set the pa-
rameters of the multiple word distributions so that we can
maximize the likelihood of the text data. Intuitively, if two
words tend to co-occur with each other and one word is

123WWW 2008 / Refereed Track: Data Mining - ModelingApril 21-25, 2008 ¬∑ Beijing, Chinaassigned a high probability, then the other word generally
should also be assigned a high probability to maximize the
data likelihood. Thus this kind of model generally captures
the co-occurrences of words and can help cluster the words
based on co-occurrences.

In order to apply this kind of model to our integration
problem, we assume that each review segment corresponds
to a unigram language model which would capture all opin-
ions that can be aligned with a review segment. Further-
more, we introduce a certain number of unigram language
models to capture the extra opinions. We then Ô¨Åt the mix-
ture model to CO, i.e., the set of all the relevant opinion
sentences generated using information retrieval as described
in the previous section. Once the parameters are estimated,
they can be used to group sentences into diÔ¨Äerent aspects
corresponding to the diÔ¨Äerent review segments and extra as-
pects corresponding to extra opinions. We now present our
mixture model in detail.

Figure 2: Generation Process of a Word

4.1 Basic PLSA
We Ô¨Årst present the basic PLSA model as described in [21].
Intuitively, the words in our text collection CO can be clas-
siÔ¨Åed into two categories (1) background words that are of
relatively high frequency in the whole collection. For exam-
ple, in the collection of topic ‚ÄúiPhone‚Äù, words like ‚ÄúiPhone‚Äù,
‚ÄúApple‚Äù are considered as background words. (2) words re-
lated to diÔ¨Äerent aspects which we are interested in. So we
deÔ¨Åne k +1 unigram language models: Œ∏B as the background
model to capture the background words, Œò = {Œ∏1, Œ∏2, ..., Œ∏k}
as k theme models, each capturing one aspect of the topic
and corresponding to the k review segments r1, ..., rk. A
document d in CO (in our problem it is actually a sentence)
can then be regarded as a sample of the following mixture
model.

k(cid:88)

j=1

pd(w) = ŒªBp(w|Œ∏B) + (1 ‚àí ŒªB)

[œÄd,jp(w|Œ∏j)]

(1)

(cid:80)k

(cid:80)

where w is a word, œÄd,j is a document-speciÔ¨Åc mixing
j=1 œÄd,j = 1), and ŒªB is the

weight for the j-th aspect (
mixing weight of the background model Œ∏B. The log-likelihood
of the collection CO is

logp(CO|Œõ) =

d‚ààCO
log(ŒªBp(w|Œ∏B) + (1 ‚àí ŒªB)

(cid:80)
(cid:80)k
w‚ààV {c(w, d)√ó
j=1[œÄd,jp(w|Œ∏j)]}

(2)

where V is the set of all the words (i.e., vocabulary),
c(w, d) is the count of word w in document d, and Œõ is the
set of all model parameters. The purpose of using a back-
ground model is to ‚Äúforce‚Äù clustering to be done based on
more discriminative words, leading to more informative and
more discriminative theme models.

The model can be estimated using any estimator. For
example, the Expectation-Maximization (EM) algorithm [3]
can be used to compute a maximum likelihood estimate with
the following updating formulas:

p(zd,w,j) =

p(zd,w,B) =

œÄ(n+1)
d,j

=

p(n+1)(w|Œ∏j) =

(cid:80)
(cid:80)
(cid:80)

j(cid:48)

(cid:80)
(cid:80)

(1 ‚àí ŒªB)œÄ(n)
ŒªBp(w|Œ∏B) + (1 ‚àí ŒªB)

d,j p(n)(w|Œ∏j)
j(cid:48)=1 œÄ(n)

d,j(cid:48) p(n)(w|Œ∏(cid:48)
j)

ŒªBp(w|Œ∏B) + (1 ‚àí ŒªB)

j(cid:48)=1 œÄ(n)

d,j(cid:48) p(n)(w|Œ∏(cid:48)
j)

(cid:80)k
(cid:80)k

ŒªBp(w|Œ∏B)

w‚ààV c(w, d)p(zd,w,j)

w‚ààV c(w, d)p(zd,w,j(cid:48) )
c(w, d)p(zd,w,j)
d‚ààCO

(cid:80)

w(cid:48)‚ààV

d‚ààCO

c(w, d)p(zd,w(cid:48),j)

4.2 Semi-supervised PLSA
We could have directly applied the basic PLSA to extract
topics from CO. However, the extracted topics in this way
would generally not be well-aligned to the expert review. In
order to ensure alignment, we would like to ‚Äúforce‚Äù some of
the multinomial distribution component models (i.e., lan-
guage models) to be ‚Äúaligned‚Äù with all the segments in the
expert review. In probabilistic models, this can be achieved
by extending the basic PLSA to incorporate a conjugate
prior deÔ¨Åned based on the expert review segments and us-
ing the Maximum A Posterior (MAP) esatimator instead
of the Maximum Likelihood estimator as we did in the ba-
sic PLSA. Intuitively, a prior deÔ¨Åned based on an expert
review segment would tend to make the corresponding lan-
guage model similar to the empirical word distribution in
the review segment, thus the language model would tend to
attract opinion sentences in CO that are similar to the expert
review segment. This ensures the alignment of the extracted
opinions with the original review segment.
for each review segment rj (j ‚àà {1, ..., k}) and deÔ¨Åne a conju-
gate prior (i.e., a Dirichlet prior) on each multinomial distri-
bution topic model, parameterized as Dir({œÉjp(w|rj)}w ‚àà
V ), where œÉj is a conÔ¨Ådence parameter for the prior. Since
we use a conjugate prior, œÉj can be interpreted as the ‚Äúequiv-
alent sample size‚Äù which means that the eÔ¨Äect of adding the
prior would be equivalent to adding œÉjp(w|rj)pseudo counts
for word w when we estimate the topic model p(w|Œ∏j). Fig-
ure 2 illustrates the generation process of a word W in such a
semi-supervised PLSA where the prior serves as some ‚Äútrain-
ing data‚Äù to bias the clustering results.

SpeciÔ¨Åcally, we build a unigram language model {p(w|rj)}w‚ààV

    ... 									
 !"#$!"#%!"#&!"#&'()* +,-./010230 124WWW 2008 / Refereed Track: Data Mining - ModelingApril 21-25, 2008 ¬∑ Beijing, ChinaThe prior for all the parameters is given by

p(Œõ) ‚àù k+m(cid:89)

(cid:89)

j=1

w‚ààV

p(w|Œ∏j)œÉj p(w|rj )

(3)

(cid:80)
(cid:80)

(cid:80)

Generally we have m > 0, because we may want to Ô¨Ånd
extra opinion topics other than the corresponding segments
in the expert review. So we set œÉj = 0 for k < j ‚â§ k + m.
With the prior deÔ¨Åned above, we can then use the Max-
imum A Posterior (MAP) estimator to estimate all the pa-
rameters as follows

ÀÜŒõ = arg max

p(CO|Œõ)p(Œõ)

Œõ

(4)

The MAP estimate can be computed using essentially the
same EM algorithm as presented above with only slightly
diÔ¨Äerent updating formula for the component language mod-
els. The new updating formula is:

p(w|Œ∏j)(n+1) =

d‚ààCO

w(cid:48)‚ààV

c(w, d)p(zd,w,j) + œÉjp(w|rj)
c(w(cid:48), d(cid:48))p(zd(cid:48),w(cid:48),j) + œÉj
d(cid:48)‚ààCO

(5)
We can see that the main diÔ¨Äerence between this equation
and the previous one for basic PLSA is that we now pool
the counts of terms in the expert review segment with those
from the opinion sentences in CO, which is essentially to
allow the expert review to serve as some training data for
the corresponding opinion topic. This is why we call this
model semi-supervised PLSA.

If we are highly conÔ¨Ådent of the aspects captured in the
prior, we could empirically set a large œÉj. Otherwise, if we
need to ensure the impact of the prior without being over re-
stricted by the prior, some regularized estimation techniques
are necessary. Following the similar idea of regularized es-
timation [19], we deÔ¨Åne a decay parameter Œ∑ and a prior
weight ¬µj as

¬µj =

w(cid:48)‚ààV

œÉj

c(w(cid:48), d(cid:48))p(zd(cid:48),w(cid:48),j) + œÉj

d(cid:48)‚ààCO

(6)

So we could start from a large œÉj (say 5000) (i.e., starting
with perfectly alignable opinion models) and gradually de-
cay it in each EM iteration by equation 7, and we stop the
decaying of œÉj until the weight of the prior ¬µj is below some
threshold Œ¥ (say 0.5). Decaying allows the model to grad-
ually pick up words from CO. The new updating formulas
are

(cid:80)

(cid:80)

j

Ô£±Ô£≤Ô£≥ Œ∑œÉ(n)
(cid:80)

œÉ(n)
j

œÉ(n+1)
j

=

(cid:80)
(cid:80)

d‚ààCO

w(cid:48)‚ààV

if ¬µj > Œ¥
if ¬µj ‚â§ Œ¥

(7)

c(w, d)p(zd,w,j) + œÉ(n+1)

p(w|rj)
c(w(cid:48), d(cid:48))p(zd(cid:48),w(cid:48),j) + œÉ(n+1)
(8)

j

j

d(cid:48)‚ààCO

p(w|Œ∏j)(n+1) =

4.3 Overall Process

In this section, we describe how we use the semi-supervised
topic model to achieve three tasks in the second stage as de-
Ô¨Åned in Section 3. We also summarize the computational
complexity of the whole process.

4.3.1 Theme Extraction from Text Collection
We start from a topic T , a review R = {r1, ..., rk} of k
segments, a collection CO = {d1, d2, ..., dN} of opinion sen-
tences closely relevant to T . We assume that CO covers a
number of themes each about one aspect of the topic T . We
further assume that there are k + m major themes in the
collection, {Œ∏1, Œ∏2, ..., Œ∏k+m}, each being characterized by a
multinomial distribution over all the words in our vocabu-
lary V (also known as a unigram language model or a topic
model).
We propose to use review aspects as priors in the partition
of CO into aspects. We could have used the whole expert re-
view segment to construct the priors. But if so, we could
only get the opinions that are most similar to the review
opinions. However, we would like extract not only opinions
supporting the review opinions but also supplementary opin-
ions on the review aspect. So we use only the ‚Äúaspect words‚Äù
to estimate the prior. We use a simple heuristic: opinions
are usually expressed in the form of adjectives, adverbs and
verbs while aspect words are usually nouns. And we ap-
ply a Part-of-Speech tagger1 on each review segment ri and
further Ô¨Ålter out the opinion words to get a r(cid:48)
i. The prior
{p(w|r(cid:48)

i)}w‚ààV is estimated by Maximum Likelihood:

(cid:80)

p(w|r

(cid:48)
i) =

c(w, r(cid:48)
i)
w(cid:48)‚ààV c(w(cid:48), r(cid:48)
i)

(9)

Given these priors constructed from the expert review
i)}w‚ààV , i ‚àà {1, ..., k}, we could estimate the param-
{p(w|r(cid:48)
eters for the semi-supervised topic model according to Sec-
tion 4.2. After that, we have a set of theme models extracted
from the text collection {Œ∏i|i = 1, ..., k + m}, and we could
group each sentence di in CO into one of the k + m themes
by choosing the theme model with the largest probability of
generating di:

arg max

j

p(di|Œ∏j) = arg max

j

c(w, di)p(w|Œ∏j)

(10)

If we deÔ¨Åne g(di) = j if di is grouped into {p(w|Œ∏j)}w‚ààV ,

then we have a partition of CO:

CO = {Si|i = 1, ..., k + m}

(11)
where each Si is a set of sentences Si = {dj|g(dj) = i, dj ‚àà
CO} with the following two properties:

(cid:88)

w‚ààV

k+m(cid:91)

CO =

Si

i=1

Si ‚à© Sj = ‚àÖ

‚àÄi, j ‚àà {1, ..., k + m} , i (cid:54)= j

(12)

(13)

Thus each Si, i = 1, ..., k, corresponds to the review aspect
ri and each Sj, j = k + 1, ..., k + m, is the set of sentences
that supplements the expert review with additional aspects.
Parameter m, the number of additional aspects, is set em-
pirically.
4.3.2 Further Separation of Opinions
In this subsection, we show that how we further partition

each Si, i = 1, ..., k into two parts:

Si = {Ssim

i

, Ssupp

i

}

(14)

1http://l2r.cs.uiuc.edu/Àúcogcomp/asoftware.php?skey=LBPPOS

125WWW 2008 / Refereed Track: Data Mining - ModelingApril 21-25, 2008 ¬∑ Beijing, Chinasuch that Ssim

opinions in the review while Ssupp
supplement the review opinions on the review aspect ri.

contains sentences that is similar to the
is a set of sentences that

i

i

i

i

)}w‚ààV and {p(w|Œ∏supp

We assume that each subset of sentences Si, i = 1, ..., k,
covers two themes captured by two subtopic models
)}w‚ààV . We Ô¨Årst construct
{p(w|Œ∏sim
a unigram language model {p(w|ri)}w‚ààV from review seg-
ment ri using both the feature words and opinion words.
)}w‚ààV .
This model is used as a prior for extracting {p(w|Œ∏sim
After that, we estimate the model parameters as described
in Section 4.2. And then, we could classify each sentence
dj ‚àà Si into either Ssim
in the way similar to equa-
tion 10.

or Ssupp

i

i

i

4.3.3 Generation of Summaries
So far, we have a meaningful partition over CO:

1

1

k

, ..., Ssim

, ..., Ssupp

}‚à™{Ssupp

CO = {Ssim

}‚à™{Sk+1, ..., Sk+m}
(15)
Now we need to further summarize each block P in the
} ‚à™ {Sk+1,
partition P ‚àà {Ssim
..., Sk+m} by extracting representative opinions RO(P ). We
take a two-step approach.

} ‚à™ {Ssupp

, ..., Ssupp

, ..., Ssim

k

k

k

1

1

In the Ô¨Årst step, we try to remove the redundancy of sen-
tences in P and group the similar opinions together by un-
supervised topic modeling. In detail, we use PLSA (without
any prior) to do the clustering and set the number of clusters
proportional to the size of P . After the clustering, we get a
further partition of P = {P1, ..., Pl} where l = |P|/c and c
is a constant parameter that deÔ¨Ånes the average number of
sentences in each cluster. One representative sentence in Pi
is selected by the similarity between the sentence and the
cluster centroid (i.e. a word distribution) of Pi. If we deÔ¨Åne
rsi as the representative sentenced of Pi, and Œ≤i = |Pi| as
the support, we have a representative opinion of Pi which is
oi = (Œ≤i, rsi). Thus RO(P ) = {o1, o2, ..., ol}.

In the second step, we aim at providing some context in-
formation for each representative opinion oi of P to help
the user to better understand the opinion expressed. What
we propose is to compare the similarity between opinion sen-
tence rsi and each review sentence in segment corresponding
to P and assign rsi to the review sentence with the high-
est similarity. For both steps, we use KL-Divergence as the
similarity measure.

4.3.4 Computational Complexity
PLSA and semi-supervised PLSA have the same complex-
ity: O(I ¬∑ K(|V | + |W| + |C|)), where I is the number of EM
iterations, K is the number of themes, |V | is the vocabu-
lary size, |W| is the total number of words in the collection,
|C| is the number of documents. Our whole process makes
multiple invocations of PLSA/semi-supervised PLSA, and
we suppose we use the same I across diÔ¨Äerent invocations.
‚ÄúTheme Extraction from Text Collection‚Äù makes one invo-
cation of semi-supervised PLSA on the whole collection CO,
where the number of cluster is k + m. So the complexity is
O(I ¬∑ (k + m) ¬∑ (|V | + |W| + |CO|) = O(I ¬∑ (k + m) ¬∑ |W|).

There are k invocations of semi-supervised PLSA in ‚ÄúFur-
ther Separation of Opinions‚Äù, each on a subset of the collec-
tion Si(i = 1, ..., k) with only two clusters. And we know
i=1 Si = CO. Suppose
from equation 11 that
WSi is the total number of words in Si. So the total com-
I ¬∑2¬∑(|V |+|WSi|+|Si|)) which in the worst
plexity is O(

(cid:83)k
i=1 Si ‚äÜ (cid:83)k+m

(cid:80)

Si

1

1

k

k

, ..., Ssim

, ..., Ssupp

}‚à™{Ssupp

case is O(I ¬∑ 2 ¬∑ (k|V | + |W| + |CO|)) = O(I ¬∑ (k|V | + |W|)).
Finally, ‚ÄúGeneration of Summaries‚Äù makes 2k + m invo-
cations of PLSA, each on a subset of the collection P ‚àà
}‚à™{Sk+1, ..., Sk+m} = CO.
{Ssim
(cid:80)
|P|
c , and WP
In each invocation, the number of clusters is
is the total number of words in P . So the total complexity
P I ¬∑ |P|
c (|V | + |WP| + |P|)), which in
in this stage is O(
c ¬∑ (|CO| ¬∑ |V | + |CO| ¬∑ |W| + |CO|2)) =
the worst case is O( I
O( I
Thus, our whole process is bounded by the computational
complexity O(I ¬∑ ((k + m + 1)|W| + k|V | +
)). Since
k, m, and c are usually much smaller than |CO|, the running
time is basically bounded by O(I ¬∑ |CO| ¬∑ |W|).

c ¬∑ |CO| ¬∑ |W|).

|CO|¬∑|W|

c

5. EXPERIMENTAL RESULTS

In this section, we Ô¨Årst introduce the data sets used in the
experiment. Then we demonstrate the eÔ¨Äectiveness of our
semi-supervised topic modeling approach by showing two
examples in two diÔ¨Äerent scenarios. Finally, we also provide
some quantitative evaluation.
5.1 Data Sets

Topic Desc.

iPhone

Source # of words # of aspects
CNET

Barack Obama wikipedia

4434
312

19
14

Table 1: Basic Statistics of the REVIEW data set

Topic Desc. Query Terms # of articles N

iPhone

iPhone

Barack Obama Barack+Obama

552
639

3000
1000

Table 2: Basic Statistics of the BLOG data set

We need two types of data sets for evaluation. One type
is expert reviews. We construct this data set by leverag-
ing the existing services provided by CNET and wikipedia,
i.e., we submit queries to their web sites and download the
expert reviews on ‚ÄúiPhone‚Äù written by CNET editors2 and
the introduction part of articles about ‚ÄúBarack Obama‚Äù in
wikipedia3. The composition and basic statistics of this data
set (denoted as ‚ÄúREVIEW‚Äù) is shown in Table 1.

The other type of data is a set of opinion sentences re-
lated to certain topic. In this paper, we only use Weblog
data, but our method can be applied on any kind of data
that contain opinions in free text. SpeciÔ¨Åcally, we Ô¨Årstly
submit topic description queries to Google Blog Search4 and
collect the blog entries returned. The search domain are re-
stricted to spaces.live.com, since schema matching is not our
focus. We further build a collection of N opinion sentences
CO = {d1, d2, ..., dN} which are highly relevant to the given
topic T using information retrieval techniques as described
as the Ô¨Årst stage in Section 3. The basic information of these
collections (denoted as ‚ÄúBLOG‚Äù is shown in Table 2. For all
the data collections, Porter stemmer [18] is used to stem the
text and stop words in general English are removed.
2http://reviews.cnet.com/smart-phones/apple-iPhone-8gb-
at/4505-6452 7-32309245.html?tag=pdtl-list
3http://en.wikipedia.org/wiki/Barack Obama
4http://blogsearch.google.com

126WWW 2008 / Refereed Track: Data Mining - ModelingApril 21-25, 2008 ¬∑ Beijing, China5.2 Scenario I: Product

Gathering opinions on products is the main focus of the
research on opinion mining, so our Ô¨Årst example of opinion
integration is a hot product, iPhone. There are 19 deÔ¨Åned
segments in the ‚ÄúiPhone‚Äù review of the REVIEW data set.
We use these 19 segments as aspects from the review and
deÔ¨Åne 11 extra aspects in the semi-supervised topic model.
Due to the limitation of the spaces, only part of the inte-
gration with review aspects are show in Table 3. We can see
that there is indeed some interesting information discovered.
‚Ä¢ In the ‚Äúbackground‚Äù aspect (which corresponds to the
background introduction part of the expert review), we
see that lots of people care about the price of iPhone,
and the sentences extracted from blog articles show
diÔ¨Äerent pricing information which conÔ¨Årms the fact
that the price of iPhone has been adjusted. In fact,
the Ô¨Årst two sentences only mention the original price
while the third sentence talks about the cut down of
the price but the actual numbers are incorrect.

‚Ä¢ The displayed sentence in the ‚Äúactivation‚Äù aspect de-
scribes the results if you do not activate the iPhone.
A piece of very interesting information related to this
aspect, ‚Äúunlocking the iPhone‚Äù is never mentioned in
the expert review but is extracted from blog articles
by using our semi-supervised topic modeling approach.
Indeed, we know that ‚Äúunlock‚Äù or ‚Äúhack‚Äù is a hot topic
since the iPhone hit the market. This is a good demon-
stration that our approach is able to discover informa-
tion which is highly related and supplementary to the
review.

‚Ä¢ The last aspect shown is about battery life. There is
a high support (support = 19 in the column of sim-
ilar opinions) of the life of battery described in the
review, and there is another supplementary set of sen-
tences (support = 7) which gives a concrete number of
battery in hours under real usage of iPhone.

Figure 3: Support Statistics for iPhone Aspects

Furthermore, we may also want to know which aspects
of iPhone people are most interested in.
If we deÔ¨Åne the
support of an aspect as the sum of the support of represen-
tative opinions in this aspect, we could easily get the sup-
port statistics for each review aspects in our topic modeling
approach. As can be seen in Figure 3, the ‚Äúbackground‚Äù
aspect attracts the most discussion. This is mainly caused

by the mention of the price of iPhone in the background as-
pect. The next two aspects with highest support are ‚ÄúBlue-
tooth and Wireless‚Äù and ‚ÄúActivation‚Äù both with support 101.
As stated in the iPhone review ‚ÄúThe Wi-Fi compatibility
is especially welcome, and a feature that‚Äôs absent on far
too many smart phones.‚Äù, and our support statistics suggest
that people do comment a lot about this unique feature of
iPhone. ‚ÄúActivation‚Äù is another hot aspect as discovered by
our method. As many people know, the activation of iPhone
requires a two-year contract with AT&T, which brings much
controversy among customers.

In addition, we show three of the most supported repre-
sentative opinions in the extra aspects in Table 4. The Ô¨Årst
sentence points out another way of activating iPhone, while
the second sentence brings up the information that Cisco
was the original owner of the trademark ‚ÄúiPhone‚Äù. The third
sentence expresses a opinion in favor of another smartphone,
Nokia N95, which could be useful information for a poten-
tial smartphone buyer who did not know about Nokia N95
before.
5.3 Scenario II: Political Figure

If we want to know more about a political Ô¨Ågure, we could
treat a short biography of the person as an expert review
and apply our semi-supervised topic model. In this subsec-
tion, we demonstrate what we can achieve by an example of
‚ÄúBarack Obama‚Äù. There is no deÔ¨Ånition of segments in the
short introduction part in wikipedia, so we just treat each
sentence as a segment.

In Table 5, we display part of the opinion integration with
the 14 aspects in the review. Since there is no short descrip-
tion of each aspect in this example, we use ID in the Ô¨Årst
column of the table to distinguish one aspect from another.
‚Ä¢ Aspect 0 is a brief introduction of the person and his
position, which attracts many sentences in the blog
articles some directly conÔ¨Årming the information pro-
vided in the review, some also suggest his position
while stating other facts.

‚Ä¢ Aspect 1 and 3 talk about his heritage and early life,
and we further discover from the blog articles sup-
plementary information such as his birthplace is Hon-
olulu, his parents‚Äô names are Barack Hussein Obama
Sr. and Ann Dunham, and even why his father came
to the US.

‚Ä¢ For aspect 10 about his presidential candidacy, our
summaries not only conÔ¨Årm the fact but also point
out another democratic presidential candidate Hillary
Clinton.

‚Ä¢ A brief description of his family is in review aspect 12,
and the mention of his daughters has attracted a piece
of news related to young daughters of White House
aspirants.

After further summing up the support for each aspect,
we display two of the most supported aspects and one least
supported aspect in Table 6. The most supported aspect
is aspect 0 with Support = 68, which as mentioned above
is a brief introduction of the person and his position. As-
pect 2 talking about his heritage ranks as the second with
Support = 36, which agrees with the fact that he is special
among the presidential candidates because of his Kenyan

13474507970737010163879553746857557310160020406080100120140160backgroundDesignDisplayMenusTouch screenExterior featuresFeaturesBluetooth and wirelessMessaging and e-mailiPhone's iPodSafari browserYouTubeWidgetsVisual voice mailCameraCall qualityBrowser speedActivationBattery127WWW 2008 / Refereed Track: Data Mining - ModelingApril 21-25, 2008 ¬∑ Beijing, ChinaAspect

Review

Similar Opinions

Supplementary Opinions

Background

Even with the new $399 price for
the 8GB model (down from an
original price of $599), it‚Äôs still
a lot to ask for a phone that lacks
so many features and locks you
into an iPhone-speciÔ¨Åc two-year
contract with AT&T.

Activation

You can make emergency calls, but
you can‚Äôt use any other functions,
including the iPod music player.

Battery

Battery life The Apple iPhone
has a rated battery life of 8 hours
talk time, 24 hours of music
playback, 7 hours of video playback,
and 6 hours on Internet use.

[support=19] iPhone will Feature Up
to 8 Hours of Talk Time, 6 Hours of
Internet Use, 7 Hours of Video
Playback or 24 Hours of Audio
Playback

[support=19]The iPhone will come in two versions, a
4GB 499 model, and an 8GB 599 model with a two
year contract.
[support=16]The Price: 499 (4GB) or 599(8GB) with
a two year contract , by the time the contract is over
your iPhone will probably be scratched all over like the
Nano or be made obsolete by better phone on the
market.
[support=12]Recently, Apple decided to cut down
price of iPhone from 399 to 200 , giving rise to much
rage from consumers bought the phone before.
[support=10]Several other methods for unlocking the
iPhone have emerged on the Internet in the past few
weeks, although they involve tinkering with the iPhone
hardware or more complicated ways of bypassing the
protections for AT T‚Äôs exclusivity.
[support=7]Playing relatively high bitrate VGA H.264
videos, our iPhone lasted almost exactly 9 freaking
hours of continuous playback with cell and WiFi on
(but Bluetooth oÔ¨Ä).

Table 3: iPhone Example: Opinion Integration with Review Aspects

Supplementry Opinions on Extra Aspects

[support=15]You may have heard of iASign (http: iphone.Ô¨Åveforty.net wiki index.php IASign), an iPhone Dev Wiki tool that
allows you to activate your phone without going through the iTunes rigamarole.
[support=13]Cisco has owned the trademark on the name ‚ÄùiPhone‚Äù since 2000, when it acquired InfoGear Technology Corp.,
which originally registered the name.
[support=13]With the imminent availability of Apple‚Äôs uber cool iPhone, a look at 10 things current smartphones like the
Nokia N95 have been able to do for a while and that the iPhone can‚Äôt currently match...

Table 4: iPhone Example: Opinion Integration on Extra Aspects

ID Review

Support

Barack Hussein Obama (born August 4, 1961)
is the junior United States Senator from
Illinois and a member of the Democratic Party.
Born to a Kenyan father and an American
mother, Obama grew up in culturally diverse
surroundings.
He married in 1992 and has two daughters.

68

36

3

0

1

12

Table 6: Obama Example: Support of Aspects

origin and indicates that people are interested in it. The
least covered aspect is aspect 12 about his family, since the
total support is only 3.
5.4 Quantitative Evaluation

In order to quantitatively evaluate the eÔ¨Äectiveness of our
semi-supervised topic modeling approach, we designed a test
which consists of three tasks, each asks a user to perform a
part of our processing. The main goal is to see to what ex-
tent can our approach reproduce the human choice. The test
is designed based on the above-mentioned ‚ÄúBarack Obama‚Äù
example. In order to reduce the bias, we collect the evalu-
ation results from three users, who are all PhD students in
our department, two males and one female.

In the Ô¨Årst designed task, we aims at evaluating the ef-
fectiveness of our approach in identifying the extra aspects
in addition to review aspects. Towards this goal, we gener-
ate a big set of sentences Sall by mixing all the sentences
in {Ssim
} with seven most sup-
ported sentences in {Sk+1, ..., Sk+m}. There are |Sall| = 34
sentences in Sall in total. The users are asked to select seven
sentences from randomly permutated Sall that do not Ô¨Åt into
the k review aspects. In this way, we could see how is the

}‚à™{Ssupp

, ..., Ssupp

, ..., Ssim

k

k

1

1

human consensus on this task and how our approach could
recover the choice of human.

User
Our Approach
User 1
User 2
User 3

Sentence ID of the 7 sentences
2, 6, 9, 21, 22, 25, 30
1, 6, 9, 13, 16, 25, 30
9, 11, 16, 20, 21, 30, 31
2, 6, 8, 9, 24, 25, 31

Table 7: Selection of 7 Sentences on Extra Aspects

Table 7 displays the selection of the seven sentences on
extra aspects by our method and the three users. The only
sentence out of seven that all three users agree on is sentence
number 9, which suggests that grouping sentences into extra
aspects is quite a subjective task so it is diÔ¨Écult to produce
results satisfactory to each individual user. However our
method is able to recover 52.4% of the user‚Äôs choices on
average.

1

, ..., Ssupp

In the second task, we try to evaluate the performance of
our approach in grouping sentences into k review aspects. we
randomly permutate all the sentences in {Ssim
} ‚à™
} to construct a Sreview and remove the as-
{Ssupp
pect assigned to each sentence. For each of the 27 sentences,
the users are asked to assign one of the 14 review aspects
to it. In essence, this is a multi-class classiÔ¨Åcation problem
where the number of classes is 14.

, ..., Ssim

k

k

1

The results turn out to be
‚Ä¢ Three users agree on 13 sentences about the class label,
which means that more than half of the sentences are
controversial even among human users.

‚Ä¢ On average, our method could recover the user‚Äôs choices
by 10.67 sentences out of 27. Note that if we randomly

128WWW 2008 / Refereed Track: Data Mining - ModelingApril 21-25, 2008 ¬∑ Beijing, ChinaID Review

Similar Opinions

0

1

3

Barack Hussein Obama (born August
4, 1961) is the junior United States
Senator from Illinois and a member of
the Democratic Party.

[support=9]Senator Barack Hussein
Obama is the junior United States
Senator from Illinois and a member of
the Democratic Party .

The U.S. Senate Historical OÔ¨Éce lists
him as the Ô¨Åfth African American
Senator in U.S. history and the only
African American currently serving in
the U.S. Senate.
He lived for most of his childhood in
the majority-minority U.S. state of
Hawaii and spent four of his pre-teen
years in the multi-ethnic Indonesian
capital city of Jakarta.

10 He is among the Democratic Party‚Äôs

[support=2]Mr Obama will contest the

leading candidates for nomination in the Democrat presidential nomination
2008 U.S. presidential election.

12 He married in 1992 and has two

daughters.

Supplementary Opinions
[support=21]Barack Obama, another leading
Democratic presidential hopeful, campaigns
for more dollars with ‚ÄùDinner With Barack.‚Äù
[support=11]A Chicago, Illinois, radio station
recently conducted a live survey on a man
called Barack Obama.
[support=10]In fact, there is not a single
metropolitan area in the country where a
family earning minimum wage can aÔ¨Äord
decent housing, said Senator Barack Obama.

[support=16]Barack Obama is an African
American whose father was born in Kenya
and got a sholarship to study in American.

[support=12]Obama was born in Honolulu,
Hawaii, to Barack Hussein Obama Sr., a
Kenyan, and Kansas born Ann Dunham.

[support=14](AP) Democratic presidential
candidate Barack Obama said Sunday that
the front runner for his party‚Äôs nomination,
Hillary Rodham Clinton, does not oÔ¨Äer the
break from politics as usual that voters need.
[support=3]MARCH 4 Senator Barack Obama
is threatening legal action against a self describ-
-ed pedophile who has posted photos of the
Democratic politician‚Äôs young daughters on a
web site that purports to handicap the 2008
presidential campaign by evaluating the
‚Äùcuteness‚Äù of underage daughters and
granddaughters of White House aspirants

Table 5: Obama Example: Opinion Integration with Review Aspects

assign one aspect out of 14, (1) the probability of re-
covering k sentences out of 27 is

√ó prk √ó (1 ‚àí pr)27‚àík

(cid:195)

(cid:33)

27
k

27(cid:88)

k=0

(cid:195)

(cid:33)

27
k

where pr = 1
14 . When k = 10, the probability is only
around 0.00037; (2) the expected number of sentences
recovered would be

√ó prk √ó (1 ‚àí pr)27‚àík = 1

‚Ä¢ Our method and all three users assigned the same label

to 8 sentences.

‚Ä¢ Among the many mistakes our method made, three
users only agree on 5 sentences. In other words, they
assigned the same label to the 5 sentences which is
diÔ¨Äerent the label assigned by our method.

Again, this task is subjective, and there is still much con-
troversy among human users. But our approach performs
reasonably : in the 13 sentences with human consensus, our
method achieves the accuracy of 61.5%.

In the third task, our goal is to see how well we can sep-
arate similar opinions from supplementary opinions in the
semi-supervised topic modeling approach. We Ô¨Årst select 5
review aspects out of 14 which our method has identiÔ¨Åed
both similar and supplementary opinions; then for each of
the 5 aspects, we mix one similar opinion with several sup-
plementary opinions; the users are supposed to select one
sentence which share the most similar opinion with the re-
view aspect. On average, our method could recover 60% of

the choices of human users. Among the diÔ¨Äerent choices
between our method and the users, only one aspect has
achieved consensus of three users. That is to say, this is
a ‚Äútrue‚Äù mistake of our method, while other mistakes do not
have agreement in the users.

6. RELATED WORK

To the best of our knowledge, no previous study has ad-
dressed the problem of integrating a well-written expert re-
view with opinions scattering in text documents. But there
are some related studies which we will brieÔ¨Çy review in this
section.

Recently there has been a lot of work in opinion mining
and summarization especially on customer reviews. In [2],
sentiment classiÔ¨Åers are built from some training corpus.
Some papers [8, 7, 10, 17] further mine product features from
reviews on which the reviewers have expressed their opin-
ions. Zhuang and others focused on movie review mining
and summarization [22].
[4] presented a prototype system,
named Pulse, for mining topics and sentiment orientation
jointly from customer feedback. However, these techniques
are limited to the domain of products/movies, and many are
highly dependent on the training data set, so are not gen-
erally applicable to summarize opinions about an arbitrary
topic. Our problem setup aims at shallower but more robust
integration.

Weblogs mining has attracted many new research work.
Some focus on sentiment analysis. Mishne and others used
the temporal pattern of sentiments to predict the book sales [14,
15]. Opinmind[16] summarizes the weblog search results
with positive and negative categories. On the other hand,
researchers also extract the subtopics in weblog collections,

129WWW 2008 / Refereed Track: Data Mining - ModelingApril 21-25, 2008 ¬∑ Beijing, Chinaand track their distribution over time and locations [12].
Last year, Mei and others proposed a mixture model to
model both facets and opinions at the same time [11]. These
previous work aims at generating sentiment summary for a
topic purely based on the blog articles. We aim at aligning
blog opinions to an expert review. We also take a broader
deÔ¨Ånition of opinions to accommodate the integration of
opinions for an arbitrary topic.

Topic model has been widely and successfully applied to
blog articles and other text collections to mine topic patterns
[5, 1, 21, 9]. Our work adds to this line yet another novel
use of such models for opinion integration. Furthermore, we
explore a novel way of deÔ¨Åning prior.

7. CONCLUSIONS

In this paper, we formally deÔ¨Åned a novel problem of opin-
ion integration which aims at integrating opinions expressed
in a well-written expert review with those in various Web 2.0
sources such as Weblogs to generated an aligned integrated
opinion summary. We proposed a new opinion integration
method based on semi-supervised probabilistic topic model-
ing. With this model, we could automatically generate an
integrated opinion summary that consists of (1) support-
ing opinions with respect to diÔ¨Äerent aspects in the expert
review; (2) opinions supplementary to those in the expert
review but on the same aspect; and (3) opinions on extra
aspects which are not even mentioned in the expert review.
We evaluate our model on integrating opinions about two
quite diÔ¨Äerent topics (a product and a political Ô¨Ågure) and
the results show that our method works well for both top-
ics. We are also planning to evaluate our method more rig-
orously. Since integrating and digesting opinions from mul-
tiple sources are critical in many tasks, our method can be
applied to develop many interesting applications in multiple
domains. A natural future research direction would be to
address the more general setup of the problem ‚Äì integrating
opinions in arbitrary text collections with a set of expert
reviews instead of a single expert review.

8. ACKNOWLEDGMENTS

This work was in part supported by the National Science
Foundation under award numbers 0425852, 0428472, and
0713571. We thank the anonymous reviewers for their useful
comments.

9. REFERENCES
[1] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent

dirichlet allocation. J. Mach. Learn. Res., 3:993‚Äì1022,
2003.

[2] D. Dave and S. Lawrence. Mining the peanut gallery:

opinion extraction and semantic classiÔ¨Åcation of
product reviews.

[3] A. P. Dempster, N. M. Laird, and D. B. Rubin.

Maximum likelihood from incomplete data via the EM
algorithm. Journal of Royal Statist. Soc. B, 39:1‚Äì38,
1977.

[4] M. Gamon, A. Aue, S. Corston-Oliver, and E. K.

Ringger. Pulse: Mining customer opinions from free
text. In IDA, volume 3646 of Lecture Notes in
Computer Science, pages 121‚Äì132, 2005.

[5] T. Hofmann. Probabilistic latent semantic analysis. In
Proc. of Uncertainty in ArtiÔ¨Åcial Intelligence, UAI‚Äô99,
Stockholm.

[6] T. Hofmann. Probabilistic latent semantic indexing. In

Proceedings of SIGIR ‚Äô99, pages 50‚Äì57.

[7] M. Hu and B. Liu. Mining and summarizing customer

reviews. In KDD, pages 168‚Äì177.

[8] M. Hu and B. Liu. Mining opinion features in

customer reviews. In AAAI, pages 755‚Äì760.

[9] W. Li and A. McCallum. Pachinko allocation:

Dag-structured mixture models of topic correlations.
In ICML ‚Äô06: Proceedings of the 23rd international
conference on Machine learning, pages 577‚Äì584.
[10] B. Liu, M. Hu, and J. Cheng. Opinion observer:

analyzing and comparing opinions on the web. In
WWW ‚Äô05: Proceedings of the 14th international
conference on World Wide Web, pages 342‚Äì351.

[11] Q. Mei, X. Ling, M. Wondra, H. Su, and C. Zhai.

Topic sentiment mixture: Modeling facets and
opinions in weblogs. In Proceedings of the World Wide
Conference 2007, pages 171‚Äì180.

[12] Q. Mei, C. Liu, H. Su, and C. Zhai. A probabilistic

approach to spatiotemporal theme pattern mining on
weblogs. In WWW ‚Äô06: Proceedings of the 15th
international conference on World Wide Web, pages
533‚Äì542.

[13] Q. Mei and C. Zhai. A mixture model for contextual

text mining. In KDD ‚Äô06: Proceedings of the 12th
ACM SIGKDD international conference on Knowledge
discovery and data mining, pages 649‚Äì655.

[14] G. Mishne and M. de Rijke. MoodViews: Tools for

blog mood analysis. In AAAI 2006 Spring Symposium
on Computational Approaches to Analysing Weblogs
(AAAI-CAAW 2006), pages 153‚Äì154.

[15] G. Mishne and N. Glance. Predicting movie sales from

blogger sentiment. In AAAI 2006 Spring Symposium
on Computational Approaches to Analysing Weblogs
(AAAI-CAAW 2006).

[16] Opinmind. http://www.opinmind.com.
[17] A.-M. Popescu and O. Etzioni. Extracting product

features and opinions from reviews. In HLT ‚Äô05:
Proceedings of the conference on Human Language
Technology and Empirical Methods in Natural
Language Processing, pages 339‚Äì346.

[18] M. F. Porter. An algorithm for suÔ¨Éx stripping. pages

313‚Äì316, 1997.

[19] T. Tao and C. Zhai. Regularized estimation of mixture

models for robust pseudo-relevance feedback. In
SIGIR ‚Äô06: Proceedings of the 29th annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 162‚Äì169.
[20] C. Zhai and J. LaÔ¨Äerty. Model-based feedback in the
language modeling approach to information retrieval.
In Proceedings of CIKM 2001, pages 403‚Äì410.

[21] C. Zhai, A. Velivelli, and B. Yu. A cross-collection

mixture model for comparative text mining. In
Proceedings of KDD ‚Äô04, pages 743‚Äì748.

[22] L. Zhuang, F. Jing, and X.-Y. Zhu. Movie review

mining and summarization. In CIKM ‚Äô06: Proceedings
of the 15th ACM international conference on
Information and knowledge management, pages 43‚Äì50.

130WWW 2008 / Refereed Track: Data Mining - ModelingApril 21-25, 2008 ¬∑ Beijing, China