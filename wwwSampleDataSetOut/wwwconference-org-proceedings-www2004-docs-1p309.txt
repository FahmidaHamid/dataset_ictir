Ranking the Web Frontier

Nadav Eiron, Kevin S. McCurley, John A. Tomlin

IBM Almaden Research Center

ABSTRACT
The celebrated PageRank algorithm has proved to be a very effec-
tive paradigm for ranking results of web search algorithms. In this
paper we reÔ¨Åne this basic paradigm to take into account several
evolving prominent features of the web, and propose several algo-
rithmic innovations. First, we analyze features of the rapidly grow-
ing ‚Äúfrontier‚Äù of the web, namely the part of the web that crawlers
are unable to cover for one reason or another. We analyze the ef-
fect of these pages and Ô¨Ånd it to be signiÔ¨Åcant. We suggest ways to
improve the quality of ranking by modeling the growing presence
of ‚Äúlink rot‚Äù on the web as more sites and pages fall out of main-
tenance. Finally we suggest new methods of ranking that are mo-
tivated by the hierarchical structure of the web, are more efÔ¨Åcient
than PageRank, and may be more resistant to direct manipulation.

Categories and Subject Descriptors
H.3.3 [Information Systems]: Information Search and Retrieval

General Terms
Algorithms, Theory

Keywords
Ranking, PageRank, Hypertext

1.

INTRODUCTION

The PageRank algorithm [28] of Page et. al. can be used to dra-
matically improve the quality of results from web search engines.
The underlying idea of PageRank is to use the stationary distri-
bution of a random walk on the web graph in order to assign rel-
ative ranks to the pages. While this basic paradigm has proven
to be remarkably effective in practice, it leaves considerable room
for enhancement to reÔ¨Çect emerging features of the web, and the
global nature of the calculation presents a computational challenge
as the web continues to grow. In this paper we examine several
issues surrounding the basic paradigm of PageRank, and suggest
several improvements over previously published work. Our main
contributions are in two areas; an analysis and algorithms to handle
the ‚Äúdangling node‚Äù problem, and in devising ways to reduce the
difÔ¨Åculty of computing pagerank while simultaneously addressing
problems of rank manipulation.

The remainder of the paper is structured as follows.

In Sec-
tion 2 we deÔ¨Åne a basic problem addressed here, namely the prob-
lem of dangling nodes. In Section 3 we describe the data set that
Copyright is held by the author/owner(s).
WWW2004, May 17‚Äì22, 2004, New York, New York, USA.
ACM 1-58113-844-X/04/0005.

we worked on for our experimental results. Section 4 describes
previous treatments of the dangling node problem, and provides a
framework that addressed some ambiguities in the previous work.
In Section 5 we provide motivation for why dangling pages are im-
portant to the overall ranking process. In Section 6 we describe
three new algorithms that incorporate information from different
types of dangling links, with the goal of producing better ranking
functions. In Sections 7 and 8 we consider two simpler forms of
PageRank that exploit the hierarchical organization of information
on the web and may have advantages for resisting manipulation. In
Section 9 we describe some experimental results from some large-
scale computations of PageRank and in particular why PageRank
appears to be subject to direct manipulation today.
In principle the PageRank algorithm is simple. The Web is mod-
eled by a directed graph G = (V,E), and the rank or ‚Äúimportance‚Äù
xi of each for the n pages i ‚àà V is deÔ¨Åned recursively in terms of
the pages which point to it:

xi = :(j,i)‚ààE

aijxj,

(1)

or in matrix terms, x = Ax. It is the form of the coefÔ¨Åcients aij
which gives rise to so many questions, including those considered
here. For the system (1) to have a useful solution, A must be (col-
umn) stochastic, that is we must have eT A = eT , where eT is the
vector of all ones. Then x is the principal eigenvector correspond-
ing to the principal eigenvalue unity[18].
The standard ‚Äúideal‚Äù assumption is that G is strongly connected,
that is that every page can be reached by clicking through from
every other page. In this case it is assumed that the aij can be given
by 1/dj, where dj is the out-degree of page j, in other words, it
is assumed that a web surfer will follow the outlinks from the page
with equal probability. However, in practice, the web is not strongly
connected, and adjustments to the ideal case must be made.

Among the most common devices is the addition of links from
pages with no outlinks to some or all of the other pages, and the
use of ‚Äúrandom jumps‚Äù not associated with actual links (sometimes
referred to as teleportation). This device is usually represented by
modifying (1) to be of the form:

x =D(1 ‚àí Œ±)f eT + Œ±AE x

(2)

where Œ± is the probability of following an actual outlink from a
page, (1 ‚àí Œ±) is the probability of taking a ‚Äúrandom jump‚Äù rather
than following a link, and f is a stochastic vector (i.e. eT f = 1).
In other words the stochastic matrix in (2) is a convex combination
of our A and a rank one matrix.

At this point it is convenient to observe [32] that solving the
system (2) is equivalent to deÔ¨Åning an additional, ‚Äúvirtual‚Äù node

309n + 1, and deÔ¨Åning an augmented system:

 x
xn+1  =

Œ±A

(1 ‚àí Œ±)eT

f

0  x
xn+1 

(3)

It is easy to verify that the solutions of this system are in one-to-
one correspondence with those of the modiÔ¨Åed problem (2). This
augmentation technique will be pursued further below.

Rather than being a single well-speciÔ¨Åed algorithm, the Page-
Rank algorithm has multiple variations that are collectively referred
to as ‚ÄúPageRank‚Äù. Included among these variations are:
‚Ä¢ Different choices for the teleportation parameter Œ± can pro-
duce different convergence rates and different rankings, but
it has become standard to use the value Œ± = 0.85, which
appears to strike a balance between achieving rapid conver-
gence with minimal perturbation to the rankings.
‚Ä¢ When a random jump is taken, it can be taken to one of an
arbitrary set of pages, as deÔ¨Åned by f. Previous suggestions
include the choice of a uniform distribution among all pages
(i.e. f = e/n), among a set of trusted ‚Äúseed sites‚Äù, uni-
formly among the set of all ‚Äútop-level‚Äù pages of sites [28], or
a personalized set of preferred pages.
‚Ä¢ Various options for the handling of so-called ‚Äúdangling nodes‚Äù,
namely nodes that either have no outlinks or for which no
outlinks are known.

The last item has received relatively little attention in the past, but
is the focus of the Ô¨Årst part of this paper. Previous treatments have
tended to treat the dangling nodes only in passing, with rather little
detail on the effect of these dangling nodes, or simply omitting this
issue entirely. In passing we note that some alternatives to Page-
Rank are not sensitive to dangling nodes, but have received much
less attention than PageRank (e.g., [2]). This attitude was perhaps
justiÔ¨Åed in the days of a largely static web, but we believe that the
relative effect of these nodes has increased over time, for a number
of reasons. In particular, we believe that it is almost impossible to
avoid the situation of having a crawl with more dangling pages than
non-dangling pages. Moreover, we believe that different classes
of dangling nodes provide useful information for ranking of non-
dangling pages, and we describe improved methods that take these
factors into account.

At last count our crawler has discovered links to over six billion
URLs, and the introduction of sites with dynamic content leads us
to believe that there is no reasonable upper bound on the number of
URLs that one can Ô¨Ånd. This huge number of URLs means that the
computational task of computing PageRank is formidable. There
have been multiple papers [1, 20, 24, 22, 23, 9] that address the
problem of efÔ¨Åcient implementation. We address this point also, in
section 7, by modifying the problem.

2. THE WEB FRONTIER

We use the term ‚Äúfrontier‚Äù to refer to the set of dangling pages.
Pages may be dangling for a variety of reasons, the most obvious of
which is that the crawler might not yet have crawled them. At the
time that the original PageRank algorithm was conceived, most of
the web was served as a static set of human-edited documents re-
siding in Ô¨Ålesystems. Under such an assumption, it made sense
to reason about the percentage of the web covered by a crawl,
and to attempt to index ‚Äúall of the web‚Äù.
In the last few years
the web has evolved to have a large number of sites that produce
dynamic content, and are driven by databases.
It has been esti-
mated that dynamic pages are 100 times more numerous than static
pages [19], but in fact the situation is even worse than this. There
are an essentially inÔ¨Ånite number of URLs (limited only by the
size of the namespace, which is at least 642000). For example,

www.amazon.com currently uses dynamically generated URLs
with a session ID embedded in them. Unless prior knowledge of
this fact is given to a crawler, it can Ô¨Ånd an essentially unbounded
number of URLs to crawl at this one site alone. 1

The problem of indexing and ranking ‚Äúthe indexable web‚Äù there-
fore starts with the problem of identifying what constitutes ‚Äúthe us-
able web‚Äù. One possible approach to avoiding all database-driven
dynamic pages is to avoid crawling URLs that contain a ‚Äô?‚Äô char-
acter in them. Unfortunately this heuristic is unreliable, and in par-
ticular it has become commonplace to use such query fragments in
URLs that produce static content.

Moreover, the database-driven pages can be (and often are) pre-
sented with URLs that do not contain a ‚Äô?‚Äô character in them, with
arguments for a database query encoded differently in the URL.
Because they appear to a crawler exactly as a static Ô¨Åle-based web
page, it is difÔ¨Åcult to prune the number of pages to be crawled. As
a result, search engines are now faced with a vast excess of poten-
tially crawlable and indexable content. The fact that an essentially
unbounded number of URLs can be found suggests that the steady
state for any crawler and search engine is for there to always be a
frontier of uncrawled pages, and that this frontier will always be
relatively large.

There are many other reasons why a page might be considered a
dangling page. It might be protected by a robots.txt, and therefore
off-limits under the standard practice of crawling. Such pages may
however contain very high-quality information, and therefore be of
great interest to readers and worthy of indexing. It is important to
note that even if a page cannot be crawled, it may still be indexable
using its anchor text. While anchor text is not a substitute for full
text indexing, it has proved to be remarkably effective in satisfying
most web search queries [11, 14, 17]. Thus it is important to be
able to evaluate the relative ranking of dangling nodes, and in fact
when we calculated PageRank we found several pages in the top
100 that were protected by robots.txt. Paradoxically, there may
even be good reasons to calculate a rank of a page that no longer
exists (e.g., a signiÔ¨Åcant document that was removed for political
or legal reasons).

Another reason for dangling nodes is pages that genuinely have
no outlink. For example, most PostScript and PDF Ô¨Åles on the web
contain no embedded outlinks, and yet the content tends to be of
relatively high quality. A URL might also be a dangling page if it
has a meta tag indicating that links should not be followed from the
page, or if it requires authentication (e.g., most of the Wall Street
Journal site). Other reasons for dangling nodes include those that
return a 500 class response at crawl time due to a conÔ¨Åguration
problem, or servers that are not resolvable in DNS, or routing prob-
lems, etc.

Our recent experience with crawling has shown that even after
crawling well over a billion pages, the number of uncrawled pages
still far exceeds the number of crawled pages. At the time that we
took a snapshot for this paper, we had crawled approximately 1.1
billion pages but the crawler knew about 4.75 billion pages. This
is partly a function of crawl ordering, but our experience shows
that the fraction of dangling pages is likely to always remain large
for any reasonable expenditure of resources, and that there will be
many pages with dangling links from them. In Figure 1 we show
the snapshot of the distribution of the number of dangling outlinks
from a given page after crawling approximately 100 million pages.
Nearly 20% of all pages turn out to have none of their outlinks

1Andrei Broder has mused that the size of the web depends strongly
on whether his laptop is on the web, since it can be conÔ¨Ågured to
produce links to an essentially inÔ¨Ånite number of URLs that are all
crawlable.

310crawled yet, and only 25% have had their outlinks fully explored,
with the rest in the middle. Thus if a crawl was halted at this point,
75% of the pages would still have dangling outlinks. By adjusting
the crawl strategy we can change the shape of the graph, but the
size of the uncrawled queue will always be a large fraction of the
total URLs known.

Figure 1: After truncating our billion page crawl at 100 million
pages, we have crawled some fraction of the outlinks for each
of the pages discovered so far. Here we show the cumulative
distribution of the number of pages vs.
the fraction of their
outlinks that have been crawled. We have only crawled all the
outlinks of 25% of the pages, which means that most pages have
links into the frontier.

All of this suggests that the problem of dealing with large num-
bers of dangling nodes is pervasive and important. If we omit the
ranking of dangling nodes, we are ignoring a majority of the po-
tential pages, and we are neglecting their effect on the rankings of
pages we have already crawled. Moreover, given that a crawler is
faced with a limitless supply of URLs to crawl, it becomes impor-
tant to assign ranks to dangling pages in order to efÔ¨Åciently manage
crawling resources [1, 10].

3. EXPERIMENTAL METHODOLOGY

The current work is based upon observations from a large crawl
performed at IBM Almaden. We began by extracting the links from
more than a billion pages, with more than 37 billion links, extend-
ing to more than 4.75 billion URLs. The process of extracting
the links and constructing the graph took a considerable amount
of computation on a cluster of machines. Since we had only 32-bit
machines available for most of this work, we spent a considerable
amount of effort to reduce the data set down to something manage-
able. We began by replacing URLs by 9-byte hash values, which
produced a data set of approximately 650 gigabytes for the links
alone (saving 4.75 billion URLs alone consumes over 300 giga-
bytes before compression!). We then separated the dangling links
from the non-dangling links, leaving us with approximately 19.2
billion non-dangling links. Following this step we assigned 4-byte
IDs to the URLs, and rewrote the graph in a much more compact
format. Aside from the treatment of dangling pages, this is simi-
lar to the procedure used by previous authors [20, 8], though on a
much larger scale.

A large amount of effort went into this data gathering step, and
the amount of time dedicated to the actual PageRank calculations
was much shorter.
It is therefore imperative that any evaluation

of algorithms for ranking of pages should take the data extraction
and preprocessing into account. On the other hand, the link gather-
ing and preprocessing of links can theoretically be done in parallel
with the crawl, whereas the actual PageRank calculation is gener-
ally done ofÔ¨Çine (but see [9]). For this reason, methods that acceler-
ate the convergence of the iterative PageRank algorithm [24, 3, 22,
23] can still be very effective. Moreover, algorithms that can work
with only local information in an online fashion [1] are all the more
appealing given that PageRank requires so much data. For smaller
data sets (e.g., intranets) this would not be much of a problem.

4. HANDLING DANGLING PAGES

We begin by reviewing some of the suggestions that have been
made previously for handling the dangling pages. In the original
Pagerank paper [28] the authors suggested simply removing the
links to dangling pages from the graph, and calculating the Page-
Rank on the remaining pages. After doing so it was suggested that
they can be ‚Äúadded back in‚Äù without signiÔ¨Åcantly affecting the re-
sults, but the details of how to do this are lacking (as is the analysis
of the effect from dangling nodes). In [23] the authors suggested
removing the dangling nodes and then re-inserting them ‚Äúfor the
last few iterations‚Äù. Note that removing the dangling links entirely
will skew the results on the non-dangling nodes somewhat, since
the outdegrees from the pages are adjusted to reÔ¨Çect the lack of
links to dangling nodes. This approach was also suggested in [6],
where they also note that this approach seems preferable to leaving
them in the calculation. This is also the approach used in [20].

Note that the process of removing dangling nodes may itself
produce new dangling nodes, and the process could therefore be
repeated iteratively until no dangling nodes remain. In theory this
may remove everything, but in practice the process terminates quickly
on the web. We chose to omit this iterative step in our process, as
we preferred to save as much information as possible about the
graph and compute ranks for dangling pages anyway.

One alternative method for handling dangling pages is to jump
to a randomly selected page with probability 1 from every dangling
node. This approach is mentioned in [23] and treated more formally
as follows. Suppose the nodes V of the graph (n = |V|) can be
partitioned into two subsets:
1. C corresponds to a completely (strongly) connected subgraph
2. The remaining nodes in subset D have links from C but no

(|C| = m).

outlinks.

In addition, following the approach in (3) assume a virtual (n+1)th
node to and from which random jumps may be made. The new node
set is denoted V(cid:48) = V ‚à™ {n + 1}. In addition we add new edges
(i, n + 1) for i ‚àà D and (n + 1, j) for j ‚àà C to deÔ¨Åne an expanded
edge set E(cid:48).
Suitably partitioning the matrix and vector in (3) the PageRank
of the nodes in V(cid:48) may be computed via the principal eigenvector
computation:

Œ±C
Œ±D

(1 ‚àí Œ±)eT

O e/m
O
eT

0

0 )(

x
y

z )

(4)

‚àí1
j

if (i, j) ‚àà E and i, j ‚àà C
otherwise

if (i, j) ‚àà E and i ‚àà C, j ‚àà D
otherwise

where, if dj is the out-degree of node j

(

x
y

z ) =(
cij = d
dij = d

‚àí1
j

0

0

00.20.40.60.81020406080100Fraction of uncrawled outlinks per pagePercentage of pages311and x, y, z are of the row dimension of C, D and 1, and e is the
vector of 1‚Äôs of conforming dimension. Note that the individual
equations are:

x = Œ±Cx + (z/m)e

y = Œ±Dx
z = (1 ‚àí Œ±)x + eT y

= {(1 ‚àí Œ±)eT + Œ±eT D}x

(5)
(6)
(7)
(8)

We may exploit this structure to compute x (and z) from a reduced
eigen-system:

 ÀÜx
ÀÜz  =

Œ±C

(1 ‚àí Œ±)eT + Œ±eT D

e/m

0  ÀÜx

ÀÜz  (9)

since the reduced matrix is column stochastic and

ÀÜx = Œ±C ÀÜx + (ÀÜz/m)e
ÀÜz = {(1 ‚àí Œ±)eT + Œ±eT D}ÀÜx

(10)
(11)

We can solve for ÀÜx, ÀÜz in equation 9 by a standard iterative method

(e.g., Power Iteration), which then allows us to compute

ÀÜy = Œ±DÀÜx.

(12)

Comparing the two systems we see that ÀÜx, ÀÜy, ÀÜz are a solution of (4)
and may be identiÔ¨Åed with x, y, z. We may thus solve the reduced
eigensystem problem (9) to obtain x = ÀÜx, and then compute the
ranks of the nodes in D from (12), in a single step. This will lead
to signiÔ¨Åcant savings unless |D| is small.
4.1 Simple examples

In extreme cases the existence of dangling pages could have sig-
niÔ¨Åcant effects on the ranking of non-dangling pages. Consider
the simplest example possible in Ô¨Ågure 2(a).
In this example it
makes a crucial difference whether we allow teleportation to dan-
gling nodes. If we were to allow uniform jumps from the dangling
node to all nodes (including the dangling node), then the transition
matrix would be

and the corresponding normalized PageRank scores are (x1, x2, z)
= (0.31746, 0.31746, 0.365079). Hence the vitual page gets a
higher score than the other pages, which is intuitively clear since it
has two inlinks while the others have only one each. When we back
out the rank of the dangling page 3, via (12) it has value 0.269841,
lower than 1 or 2, and also less than the rank of the virtual node,
since the latter also receives random (teleportation) jumps from 1
and 2.

A slightly more complicated example in Figure 2(b) illustrates
that dangling pages can have a higher rank than non-dangling pages,

and there is no need for teleportation because the graph is now
strongly connected. In any event we get a higher rank for page 3
than for pages 1 and 2.
Now consider the case where teleportation to dangling nodes
is avoided, using the notation of the previous section. Here C =
{1, 2} and D = {3}. In this case the reduced transition matrix (9)
matrix (with Œ± = .85) would be:

0
1
2
1
2

1
2
0
1
2

1
3
1
3
1
3

 $"

!%#

0

0.425

0.425

0

0.575 0.575

 $"

1
2
1
2

0 !%#

(a) Three page example

(b) Six page example

Figure 2: Simple examples with dangling links. The six page
example shows that dangling pages can have higher rank than
non-dangling pages

even when teleportation to dangling nodes is forbidden. Solving
the reduced system, and backing out the y values, we obtain nor-
malized ranks (including that of the virtual node) of (x, y, z) =
(0.122883, 0.111862, 0.108739, 0.107855, 0.143159, 0.0973207,
0.308181) Note that the virtual page has the highest rank, as we
might expect, but that page 5 has a higher rank than any of the
connected pages 1, ..., 4.

5. LINK ROT AND RANKING

Some dangling pages have already been noted to be of extremely
high quality, and a page that points to them may be considered to
have good hub characteristics as a result of the link relationship.
By contrast, pages that are dangling because they produced a 404
or 403 response code should be considered to reÔ¨Çect poorly upon
pages that link to them. There are two primary reasons for such a
links to exist:
‚Ä¢ the page existed at the time that the link was created, but was
subsequently removed from the web, causing a broken link.
In this case we may consider that the page containing the
broken link is no longer being maintained, and is out of date.
‚Ä¢ the page never existed, and the original link was created in
error. In this case, the document containing the link should
be considered to have been poorly authored, as the links were
never checked for validity.

In both cases, pages that contain links to pages that with 403 or 404
HTTP return codes should be considered deÔ¨Åcient in some way. We
refer to pages that return a 403 or 404 HTTP code as penalty pages.
This reÔ¨Çects a principle that has not previously been captured by
PageRank, since PageRank computes a score for a page based on
the pages that link to it, rather than based on features of pages that
are linked to by the page.

There is anecdotal evidence that as the novelty wears off and the
web matures, there is a growing trend toward ‚Äúlink rot‚Äù, where links
that worked at one time are broken by the removal of content on the
web or change in the URL. Several studies [26, 31] have forecast
the half-life of a URL at between four and Ô¨Åve years, and in [26] it
was found that more than half the pages being tracked in the .com
domain disappeared in 24 months. In our crawl of over a billion
pages we discovered that approximately 6% of all web pages that
were linked to turned out to return a 404 code. These presumably
reÔ¨Çect a fraction of pages that are no longer maintained or were
poorly authored in the Ô¨Årst place. As time passes we expect that
this problem will only worsen as an increasing fraction of pages on
the web fall into disrepair.

In our experiments we observed that as a crawl progresses, the
percentage of penalty pages tends to increase. This is related to
the observation of Najork and Wiener [27] that a breadth-Ô¨Årst or-
der of crawling tends to Ô¨Ånd highly ranked pages early on in the

312561234312crawl. In Figure 3 we show the rate at which penalty pages were
discovered during our crawl of 1.1 billion pages. The rate actu-
ally starts out high and then drops quickly, only to rise again as the
crawl progresses. The reason for this is that this crawl was seeded
with a large set of URLs from a previous crawl, many of which
were now 404s. Once these 404s were attempted, the rate dropped
and then started to rise again. Since the quality of pages encoun-
tered in the crawl tends to decrease, and the probability of having
links to penalty pages also increases, there already appears to be a
correlation between the two events.

Figure 4: An extreme case for PageRank in which there are
three pages that are strongly connected, and one dangling page.

with links to penalty pages. There are four basic methods de-
scribed here, which we refer to as ‚Äúpush-back‚Äù, ‚Äúself-loop‚Äù, ‚Äújump-
weighting‚Äù, and ‚ÄúBHITS‚Äù.
6.1 The push-back algorithm

The principle behind this algorithm is that if a page has a link to
a penalty page, then it should have its rank reduced by a fraction,
and the excess rank from this page should be returned to the pages
that pushed rank to it in the previous iteration. The end effect is
that of limiting the ‚ÄúinÔ¨Çow‚Äù of rank to such pages.

Figure 3: Rate at which penalty pages are encountered during
a billion page crawl. The initially high rate was caused by the
use of a large seed set from a previous crawl. After an initial
dip, the rate climbs during the rest of the crawl.

In section 6 we describe some algorithms that can be used to
explicitly discriminate against pages on the basis of whether they
point to penalty pages. It is our hope that by incorporating more
Ô¨Åne-grained information such as this into ranking, we can improve
the quality of individual search results and better manage resources
for crawling.
5.1 Another simple example

Let us now consider a case in which penalty nodes are signiÔ¨Å-
cant. In the example of Ô¨Ågure 4, there are four pages, with one of
them being a dangling page with a link from page 3. If we compute
pagerank with Œ± = 0.85, the reduced problem gives us scores of

s = [0.198684, 0.283124, 0.283124, 0.235068]

for the three strongly connected pages and the virtual page. Now
suppose that node 3 has not one but four dangling links. The new
ranks are:

s = [0.195954, 0.229266, 0.279234, 0.29554]

Note that the virtual node‚Äôs rank has increased (as we might ex-
pect), but the rank of node 2 has signiÔ¨Åcantly decreased. Thus
dangling links, which may or may not be signiÔ¨Åcant, can have a
signiÔ¨Åcant effect on the ranks of nearby pages. We might then con-
sider how this inÔ¨Çuence is in turn affected by the presence of dead
dangling links, such as 404s.

6. LINKS TO PENALTY PAGES

In this section we describe several modiÔ¨Åcations to the basic
PageRank algorithm that can be used to adjust the ranks of pages

= :(j,i)‚ààE

To describe the push-back algorithm, let i be a page that contains
a link to a penalty page p. Now looking at the equation for xi above
we have

x(k+1)
i

aijx(k)

j

(13)

and we wish to return a portion (say Œ≤i, where 0 < Œ≤i < 1) of that
rank to the pages which point to it (i.e. the j such that (j, i) ‚àà E).
We may do this by modifying the PageRank calculation so that

x(k+1) = BAx(k)

where B, like A, is a column stochastic matrix. That is eT B =
eT A = eT and e is a vector of all ones. This clearly preserves the
column stochastic property, since eT (BA) = eT A = eT .

We suggest that the penalized rank should be returned to its con-
tributors in the same proportion as it was bestowed in (13). That is
the penalized page i should retain a proportion (1‚àí Œ≤i) of its undi-
luted rank, and the remaining rank should be distributed in pro-
portion Œ≤iaij to the pages which point to it (i.e.
the j such that
(j, i) ‚àà E). Naturally, these proportions must be normalized so
that the total is 1.

In matrix terms this corresponds, in the case of a single penalized
page‚Äîwhich we may, without loss of generality assume to be the
Ô¨Årst page ‚Äî a B of the following form:

B = (1 ‚àí Œ≤1)/œÉ 0
I 

Œ≤1¬Øa1/œÉ

(14)

where ¬ØaT

1 is the 1st row of A (except for a11), and

œÉ = (1 ‚àí Œ≤1) + Œ≤1eT ¬Øa1

is a normalizing factor, so that B is column stochastic.

In the event that several pages are penalized we may extend this
procedure in the obvious way to construct a B such that each such
page ‚Äúgives back‚Äù the fraction (1 ‚àí Œ≤i) of its standard rank.

In practice this modiÔ¨Åcation involves an extra step at each itera-
tion of the standard PageRank power iteration ‚Äì the (sparse) post-
multiplication of the rank vector by B.

A particular example of the above procedure might apply to pages
which point to 404 (or other bogus) pages. Let gi be the number of
‚Äúgood‚Äù links out of page i and bi be the number of bad (penalty)
links. Then we might penalize page i by setting

Œ≤i =

bi

gi + bi

0.0250.030.0350.040.0450.050.0550.060.06502e+084e+086e+088e+081e+091.2e+09Fraction of penalty pagesNumber of pages crawled4123313If we modify the example in Figure 4 to have eight dangling
links from page 3, where four are bad (404s) and four are good,
and apply this technique, we obtain the new set of ranks (0.292287,
0.312162, 0.1666, 0.228948) for pages 1‚Äì3 and the virtual node.
We now see that page 3 has a signiÔ¨Åcantly reduced rank compared
to pages 1 and 2.

6.2 The self-loop algorithm
Ordinarily at each step we would follow an outlink with proba-
bility Œ± or jump to a random page with probability 1 ‚àí Œ±. In the
self-loop algorithm we augment each page with a self-loop link to
itself, and with some probability Œ≥i follow this link (we assume that
all self-loops have been removed from the link graph prior to aug-
mentation). The probability Œ≥i should be smaller if the page has a
large number of outlinks to penalty pages. In this way, a page that
has no bad outlinks will retain some of its own rank by following a
link to itself, whereas a page with only bad links will not retain any
of its rank this way. One potential choice for Œ≥i is to choose a prob-
ability Œ≥ and use Œ≥i = Œ≥ ¬∑
, where again bi is the number of
outlinks from i to penalty pages, and gi is the number of outlinks to
non-penalty pages. In order to create a stochastic matrix, we adjust
the teleportation probability from 1 ‚àí Œ± to 1 ‚àí Œ± ‚àí Œ≥bi

bi+gi

gi

.

There are several variations on this theme, including a simpliÔ¨Åed
version where we simply add self loops to the page for every good
outlink, and select a random outlink from the page (including the
added self-loops) with equal probability each time. Alternatively,
we could choose a parameter Œ≥i for each page, and with probability
Œ≥i we follow the self loop, and with probability 1‚àíŒ≥i we follow the
standard PageRank process. This would result in transition proba-
bilities of Œ≥i for the self-loop, probability (1 ‚àí Œ≥i)(1 ‚àí Œ±) for the
teleportation step, and probability Œ±(1‚àíŒ≥i)/g for following a non-
penalty outlink from the page. This results in having no rank for
penalty pages, but it can be modiÔ¨Åed in an obvious way to compute
such ranks.

bi+gi

6.3 The jump-weighting approach

We previously treated penalty pages (such as 404s) as dangling
nodes in the web graph, and collapsed them into the virtual node
along with the legitimate dangling nodes, in equation (4). The stan-
dard procedure then redistributes the rank of the virtual node evenly
(or to a chosen seed set). We propose an alternative procedure of
biasing the redistribution so that penalized pages receive less of this
rank. Using the above notation for good and bad pages, a straight-
forward choice is to weight the link from the virtual node to an
unpenalized node in C (or the seed set) by œÅ and to a penalized
node by œÅgi/(gi + bi), where œÅ is chosen so that the sum of all
these edge weights is unity.

6.4 The BHITS algorithm

In this section we describe an algorithm that resembles the HITS
algorithm [7], in the sense that it is derived from a random walk in
both the forward and backward directions. In contrast to the HITS
algorithm, the BHITS algorithm does not depend on a query, and
ranks all pages together. In this way it is more similar to the hub
walk of SALSA [25], or to the method describe in [30]. As in the
other algorithms, the intent is that pages pointing to penalty pages
should be degraded somewhat.

The algorithm is most easily described as a random walk that
uses a forward step as in ordinary PageRank, followed by a ‚Äúback-
ward‚Äù step for dangling nodes. For all non-dangling nodes, the
backward step consists only of a self-loop. We distinguish two
cases for the backward step from a dangling node. In the case of
a penalty page, we forward all of its score to the virtual node. In

the case of a non-penalty page, the backward step would divide the
current score of the page by the number of inlinks, and propagate
its score equally among all of the backward links. We assume that
all pages have an inward link, which is certainly true of pages that
we discover by crawling, but we must also assume that the seed
pages have known inward links in order to run the algorithm for
them. Without loss of generality we could treat any page with no
inlinks as a penalty page for this process. For a penalty page, in-
stead of traversing an inlink in the reverse direction, we take a step
to a randomly selected seed node as in PageRank. The effect of this
is to ‚Äúreturn‚Äù the rank of pages that point to non-penalty pages, but
to redistribute the rank that is given to penalty pages.

If P denotes the matrix representing a PageRank Markov process
(as described in section 4, then the matrix describing the BHITS al-
gorithm is simply BP , where B is the matrix that encodes the back-
wards step. More speciÔ¨Åcally, order the pages so that the penalty
pages are at the end, and for a non-penalty page j, let Œ¥(j) denote
the indegree of a node j. The let bij = 1
Œ¥(j) be the probability of
going from j to i. In this case we get the matrix

B = bij

1

m 

to describe the backwards step. Here 1 denotes a matrix of 1‚Äôs, but
this could easily be replaced with a personalized distribution that
favors some pages over others, as can also be done in PageRank.
One might for example exclude the penalty pages from the redistri-
bution of weight, which if there are p penalty pages, would produce
a matrix as:

B = B11

B21

1

m‚àíp

0



The matrix BP is stochastic, because it is the product of two
stochastic matrices. This Markov chain will produce a unique sta-
tionary probability distribution on the pages, but it is evident that
for pages that link to penalty nodes, their probability will generally
be less than it is in standard PageRank. Note that this algorithm can
be cast in a uniÔ¨Åed framework as was done in [13].
Implementation Considerations
6.5

Considering the size of the web, the feasibility of efÔ¨Åcient im-
plementations for any ranking algorithm are essential. For the Ô¨Årst
three methods, that are based on the PageRank algorithm, we com-
pare the methods we propose for penalizing page scores for pages
with broken links to the standard ranking method as described in
Section 4. In all of these methods, the modiÔ¨Åcation to the standard
calculation is expressed as a modiÔ¨Åcation of the matrix A. Looking
at the particulars of the three methods, one can immediately notice
that those modiÔ¨Åcations are local in nature.
i.e., apart for the re-
quirement to keep the matrix normalized, the changes to an entry
in the ith row of the matrix are only dependent on the number of
penalty and ‚Äúgood‚Äù outlinks from the ith node, bi and gi. This al-
lows either the modiÔ¨Åed matrix to be pre-computed in linear time
(given the vectors b and g), or the required modiÔ¨Åcations to be ex-
ecuted on the Ô¨Çy along with the power iteration computation. We
therefore conclude that these three methods require minimal com-
putational overhead.

The BHITS algorithm is slightly more involved to implement.
The main complication, however, is independent from the modiÔ¨Å-
cation required to penalize pages with broken links. Rather than
allowing the collapse of all dangling nodes into a virtual node, as
described in Section 4, the forward/backward approach requires an
eigen-system that actually includes all of the dangling nodes. In our
sample crawl we had roughly one billion crawled pages (some of
them dangling), but nearly Ô¨Åve billion total URLs discovered. This

314means that a system that includes dangling nodes is more than Ô¨Åve
times bigger than that required for a ‚Äúforward-only‚Äù process based
on PageRank. Therefore the BHITS algorithm is not as suitable for
large scale implementations as the others.

7. THE HOSTRANK ALGORITHM

Up until this point we have concentrated on the issue of dangling
pages, but there are numerous other structural features of the web
that can be incorporated into PageRank. The original paradigm
was to model the web as a set of pages that readers can navigate
through, occasionally jumping to a random page as they lost inter-
est or decided to investigate a different topic. This basic paradigm
makes many simplifying assumptions, and in particular one might
object that it is impossible for a user to choose a page uniformly
at random (or even to know what URLs there are to choose from).
There have been several recent studies suggesting that a large per-
centage of web browsing sessions start by a visit to a search engine,
expressing a query for their need, and following links suggested by
the search engine. The duration of the session will usually then
consist of following links from the search results for a while, pos-
sibly returning to the list of results or reformulating the query, until
the information need is satisÔ¨Åed or the user gives up. When the
next session starts, users don‚Äôt simply jump to a random page, but
instead return to the search engine and formulate a new query to
describe their need. This behavior is quite different from the model
of the PageRank paradigm, and in particular when a user takes a
random jump, they are presented with a set of possible choices that
are profoundly nonuniform in their distribution.

Another variation on the model for a web surfer that was sug-
gested in PageRank involved having the user periodically jump to
a random page selected from some smaller set of ‚Äútrusted pages‚Äù.
This has further been reÔ¨Åned as a means of producing a personal-
ized or topic-sensitive page rank [21, 28]. This would be a good
model for a user who jumps to a URL selected from a set of book-
marks, or perhaps to a portal. On the other hand, portals now tend
to change their content rapidly in order to encourage return visits
from users, so once again the random jumps follow a nonuniform
distribution. Moreover, each time the user visits the portal the links
are likely to be to content that is new, and did not even exist during
their prior visit. Hence the jump distribution changes over time,
and is even directed to the frontier.

In [29] it was observed that the probability values produced by
the PageRank algorithm decay according to a power law, and they
incorporate this into a model of how the web evolves. They as-
sumed that teleportation takes place to a page chosen uniformly
at random. In Figure 5 we show the distribution of pagerank val-
ues computed using teleportation to a randomly selected page vs.
teleportation with probability 0.5 to one of two highly ranked sites
(www.microsoft.com and www.yahoo.com). The distribu-
tion of the pagerank values is quite different, and while our obser-
vations conÔ¨Årm the results of [29], it points out that the teleporta-
tion strategy has a profound effect on the hypothesized probability
distribution for a user to end up at a page.
In particular, if the
link structure of the web was hierarchical, in which every page at
a given level linked to every page below it, then the distribution
would decay exponentially as we descended the hierarchy, and the
tail of the distribution shows something closer to this than a power
law distribution. In passing we note that it is perhaps tempting to
assign advertising value to pages on the basis of a PageRank value,
since it was intended to model the probability that a web surfer will
end up at the page. These computations at least show that it makes
a critical difference how one computes PageRank.

Figure 5: Distribution of pagerank values in the case where
teleportation is done to a randomly selected site or one of two
trusted seed sites.

7.1 Exploiting hierarchical structure

One suggestion that was made in the original PageRank paper [28]
was to jump to a randomly selected top-level page of a site. In fact
this probably models very well how people enter a site, since there
is a strong human tendency for information to be organized hierar-
chically [15]. Moreover, from our crawl of over a billion pages we
found that 62.4% of all links were internal to a site, and that links
within a site tend to show a high degree of locality [15]. We have
also found that when links are external to a site, they tend to link to
the top level of the site (see Figure 6). These structural features of

Depth
0
1
2
3
4
5
6

fraction
0.705
0.089
0.035
0.017
0.005
0.002
0.001

Figure 6: Links from outside a site tend to link to the top level of
a site. The depth here is the number of levels of hierarchy below
the top for the destination URL. Approximately 14% were to
dynamic URLs, and if we ignore those then the top level of sites
receives 82% of all external links.

the web have also shown up in other ways, allowing very high lev-
els of compression for the link graph and enabling a block-oriented
approach to accelerate the convergence of PageRank [23].

The hierarchical structure of information on a site and the fact
that most external links are to sites rather than individual pages
suggests that we consider all pages on a site as a single body of in-
formation, and assign them all a rank based on the collective value
of information on that site. This suggestion has been made in the
past (going back at least as far as [3]), and is sometimes referred
to as computing a hostrank rather than a PageRank. Formally, we
can think of the hostnames as representing nodes in a graph, and
construct a directed edge from hostname S to D if there is a URL
on S that has a link to a URL on D. It is further possible to as-
sign weights to the edges between hosts, where the weight reÔ¨Çects
the number of links from URLs on the source to URLs on the des-
tination. It is then natural to scale these edge weights so that the
total weight emanating from a hostname sums to 1. These weights

1e-351e-301e-251e-201e-151e-101e-0511101001000100001000001e+061e+071e+08PagerankPagesRandom teleportationtrusted teleportation315then replace the entries aij = 1/dj in the original PageRank cal-
culation. The inclusion of weights on the edges means that we re-
quire an additional piece of information for each edge of the graph,
but the weights can improve the quality of results by reÔ¨Çecting the
strength of the connection. As an alternative we might use weights
consisting of the number of distinct destination URLs.

The computation of hostrank is much simpler than that of com-
puting PageRank, because the graph is much smaller. In our data
set there were approximately 48 million hostnames, although for
various reasons, many of the hostnames are uncrawlable. Note that
this approach to reducing the amount of computation for PageRank
can be combined with a number of other optimization approaches,
including more sophisticated linear algebra [3, 23, 22, 24] better
I/O management [20, 8], and approximations that use only local
information [1].

8. THE DIRRANK ALGORITHM

One might argue that lumping all pages on a host into a single en-
tity is too coarse a level of granularity. There are certainly examples
such as ISPs, and notably www.geocities.com, for which it is
implausible to consider all pages on the same host as having equiv-
alent rank. Moreover, the distribution of the number of URLs per
host has been observed to have a heavy tail distribution. Thus while
there are some hosts with millions (perhaps billions or trillions?) of
possible URLs on them, most hosts have relatively few URLs. The
hostrank algorithm would have difÔ¨Åculty in distinguishing between
these.

It has previously been observed that web information tends to
have a hierarchical information reÔ¨Çected in URLs [15, 16]. This is
due in part to the fact that many web servers simply export a Ô¨Åle
system, and it has become a common practice for humans to group
related Ô¨Åles together in a single directory, and for administrative
delegation of authorship to be done at the directory level. This
hierarchical structure is even present on servers whose content is
not stored in a hierarchical Ô¨Åle system, as the URL standard was
originally designed to incorporate hierarchical structure when it is
natural to do so [4].

In [16] it was further observed that URLs can often be grouped
into ‚Äúcompound documents‚Äù that represent a single unit of informa-
tion, and that such compound documents tend to consist of URLs
that agree up to the last ‚Äô/‚Äô character. Thus it is perhaps natural to
group together URLs that agree up to the last delimiter as a single
information node, and construct a DirRank graph. The nodes in this
case correspond to URL preÔ¨Åxes up to the last ‚Äô/‚Äô character (or other
delimiter), and there is an edge from one node to another if there is
a link from a URL in the source virtual directory to a URL in the
destination virtual directory. In effect this groups URLs at a Ô¨Åner
level of granularity than entire hostnames, but still often conforms
to a human-designed hierarchical organization of information.

So-called ‚Äúdynamic‚Äù URLs containing a ‚Äô?‚Äô character tend not to
follow this hierarchical organization of information, but such URLs
are often an indicator for the existence of an underlying database
capable of serving an enormous number of URLs, and these are
often still closely related to each other and it is natural to group
them. Thus we can extend the hierarchical grouping to include all
URLs that agree up to the last ‚Äô/‚Äô or the last ‚Äô?‚Äô, whichever occurs
Ô¨Årst.

As we mentioned in the previous section, the distribution of the
number of URLs per hostname tends to have a heavy tail distri-
bution, and the same is true for the number of URLs in a virtual
directory [15]. In our crawl of a billion URLs, over 79% contained
Ô¨Åve or fewer URLs, and over 87% contained fewer than ten URLs.
Whenever a directory contains a large number of URLs, it tends to

be either the archive of a mailing list or else a database-driven set
of dynamic URLs.

It should be noted that the URL hierarchy extends into the host-
name (although the order is reversed at this point). Thus we might
also consider grouping all URLs that agree up to some point in the
complete hierarchy, thus assembling some multiple hostnames into
a single node in cases where it makes sense to do so. In particular,
we discovered second level domains that had more than a million
hostnames in them.

9. EXPERIMENTAL RESULTS

Comparison of ranking methods for web pages is complicated by
the fact that there is no universally recognized measure of quality
for a static ranking. Moreover, as we mentioned before, PageRank
is a paradigm rather than a well deÔ¨Åned algorithm, so it is not clear
what to compare with. In reality, search engines use a large number
of factors to rank results relative to a query, a user, and the context
in which the search is performed. The problem of producing a sin-
gle linear static ordering on web pages is an over-simpliÔ¨Åcation of
the search ranking problem, though it has proved to be an extremely
important tool for the original problem.

In spite of the fact that we have no reasonable methodology to
quantitatively measure the quality of the rankings, we can still com-
pare the different ranking schemes on a number of factors, includ-
ing the computational resources required, the similarity between
different ranking methods, and subjective judgments about highly
ranked pages.

In order to make some comparisons, we calculated several vari-
ations of PageRank of the graph induced by the Ô¨Årst 100 million
pages from our billion page crawl. We also calculated a standard
PageRank for 1.08 billion pages using teleportation to pages se-
lected uniformly at random. Finally we computed DirRank and
hostrank on our billion-page crawl.

For the DirRank calculation, once we collapsed the URLs into
directories, we were left with approximately 114 million nodes rep-
resenting the directories, and 15 billion edges representing links be-
tween pages in different directories. Thus we were able to reduce
the number of nodes in the graph by nearly an order of magnitude,
but the number of edges dropped only by a small factor. It is nat-
ural to expect that the ranking induced by computing DirRank on
this graph will interpolate between the results obtained with hos-
trank and the results using PageRank. The major differences are
that it breaks URLs into more Ô¨Åne-grained detail than hostrank, but
still saves an order of magnitude on the the number of nodes in the
PageRank graph. By contrast, the hostlink graph turned out to only
involve 19.7 million hosts, with 1.1 billion edges between them.
This represents a savings of nearly another order of magnitude in
the number of nodes over the DirRank graph.

In calculating PageRank on the billion pages, we discovered very
early on that single precision calculations were inadequate without
careful attention to ordering of operations or some other method
of controlling errors. We therefore had to compute these in dou-
ble precision, though we did not encounter this requirement when
computing PageRank on only a hundred million pages. We chose
to implement a fairly naive method using simple power iteration,
although we could have employed more sophisticated implementa-
tions (e.g.,[20]).
9.1 Subjective judgments and link spamming
Web search has come to be recognized as the primary control
point for electronic commerce on the web, and as a consequence,
high ranking in search engines is now perceived to have high value.
This has given rise to an industry that assists merchants and adver-

316tisers in manipulating the ranking of web pages in search engines,
and has increased the need for ranking methodologies that are re-
sistant to manipulation. This turned out to be a major consideration
in evaluating the results for different algorithms.

The competition for high ranking in search engines has made
PageRank a direct target for manipulation, and this was apparent
in our experiments. Among the top 20 URLs in our 100 million
page PageRank calculation using teleportation to random pages, 11
were pornographic, and they appear to have all been achieved us-
ing the same form of link manipulation. The speciÔ¨Åc technique
that was used was to create many URLs that all link to a single
page, thereby accumulating the PageRank that every page receives
from random teleportation, and concentrating it into a single page
of interest. This can be cascaded through trees of increasing con-
centration, using only moderate indegree for the intermediate nodes
in the tree. All that is required is to get as many pages crawled as
possible.

By contrast, the PageRank that used teleportation to only two
trusted sites had no pornography in the top 1000 pages. There has
been relatively little published on this problem of rank manipu-
lation, although there has been some anecdotal suggestions to as-
sign greater weight to non-nepotistic links, and some work has been
published on recognizing them [12]. Our subjective evaluation sug-
gests that teleportation to trusted sites can help considerably.

We also computed hostrank using a weighted graph and telepor-
tation to random nodes. After reviewing the top 100 hostnames in
this ranking, we found only 14 to be questionable, including two
porn sites, six sites offering link exchange services, and Ô¨Åve fake
companies involved in link spamming. Though there was still ev-
idence of manipulation in the host rankings, we judged them to
be less affected. The manner in which manipulation took place
was similar to that used in manipulating PageRank, and in fact we
found that the second-level .com domain with the most hostnames
was a pornographic content provider for which we had discovered
1,442,013 distinct hostnames. There is no apparent reason to main-
tain such an unusual DNS server other than to manipulate hostrank.
The technique of aggregation in DirRank can be carried over into
the domain name space in order to combat this type of rank manip-
ulation.

9.2 Comparison of rankings

Given two ranking methods, we can measure how different they
are from each other using a measure such as Kendall or Spearman
distance, or measures from [5]. When we began this study, we
hypothesized that hostrank and DirRank would be good approx-
imations for PageRank, and because they were so much cheaper
to compute, they would be preferable. In fact every form of mea-
surement that we applied showed them to result in very different
rankings. This is not in itself a bad thing, because our observa-
tions on PageRank with uniform teleportation show that it is very
subject to manipulation and does not provide a particularly good
ranking any more. The computational advantages of hostrank and
DirRank, and their enhanced resistance to rank manipulation are
probably enough to make them attractive candidates for ranking.

One aspect in which the hostrank calculation seems to differ in
from PageRank is the effect of adversarial link manipulation. We
believe much of this difference can be explained by examining the
distribution of rank in the virtual node in both cases as was de-
scribed in Section 4. Teleportation is also formulated by jumping
Ô¨Årst to this virtual node. This virtual node distributes its score uni-
formly at random to all other nodes. It turns out that because of
the size of the frontier of the web, the score that the virtual node
receives is rather signiÔ¨Åcant. In fact, in our 100 million page Page-

Rank calculation, the weight of the virtual node was .82. We be-
lieve much of the link manipulation that is used in boosting the
page rank of certain sites, involves using a large collection of pages
that all link to the page whose rank is to be boosted. This collection
of pages receives a non-negligible fraction of the probability of the
virtual node (which is distributed uniformly among all pages) and
passes it on to just one page. Hostrank cancels out the effect of this
construction because it transfers probability from the virtual node
to a host independent of the number of pages in that host. Further-
more, because the host graph is smaller, and its frontier is smaller
(since it is better connected), the virtual node receives a signiÔ¨Å-
cantly lower probability itself. In our experiments, the virtual node
probability for hostrank was a relatively low .17.
9.3 The effects of dangling nodes

Many major newspaper sites prohibit crawling of most of the
pages on their sites, thus turning all pages on those sites into dan-
gling nodes. Often home pages of a site will accumulate rank by
aggregating the PageRank score Ô¨Çowing into individual pages on
the site to a top level homepage through a site‚Äôs internal naviga-
tional link structure. For a site that is not crawlable, this struc-
ture does not exist, and the internal aggregation of score cannot
take place. This has a very noticeable effect on the rank of the
home page. Among major newspapers, the Chicago Tribune was
the highest ranking in our one billion node PageRank calculation
(its home page ranked 87th). By contrast, newspapers that are both
considered more authoritative, and have much higher circulation
such as the Wall Street Journal or the New York Times ranked much
lower because their sites are mostly blocked from crawling. In fact,
of the 55 newspapers that we examined, we found 24 newspapers
with higher rank than www.nytimes.com under PageRank with
uniform teleportation.
In all, major newspapers whose sites are
crawlable clearly dominated major newspapers whose sites cannot
be crawled, which illustrates another effect from dangling nodes.

10. CONCLUSION

The original suggestion for PageRank was a heuristic based on a
random surfer model to evaluate the probability that a page would
be viewed. In this work we have found signiÔ¨Åcant evidence that
PageRank with uniform teleportation has been targeted for manip-
ulation, and the results are now less than satisfactory. The origi-
nal paper on PageRank had multiple variations that overcome this
problem, and we have found evidence that these variations are ei-
ther helpful or else have not come under direct attack.

We have suggested the hostrank and DirRank methods as im-
provements to the basic PageRank algorithm, and our results show
that they are both cheaper to compute and currently result in rank-
ings that display less effect from manipulation. Whether these tech-
niques will continue to resist rank manipulation is open, but we
believe that aggregation is a useful technique for tuning rank algo-
rithms, and is well motivated from the hierarchical structure that is
evident in the web. There remain many opportunities for research
in new ranking methods that are better tuned to models of user be-
havior and interests.

The problem of dealing with ‚Äúdangling‚Äù pages on the frontier of
the crawled web has been largely ignored in previous discussion
of PageRank. We have provided several different alternatives for
rigorously and efÔ¨Åciently ranking these pages. Moreover, we have
distinguished between different types of dangling links, and pro-
posed four techniques for penalizing pages which point to illegiti-
mate pages, thus reducing the bias afforded to the ranking process
by these bad links.

Up until this time the analysis of ranking algorithms for the web

317has remained largely in the realm of trade secrets and economic
competition. It is our hope that this paper will elevate the discussion
to a more scientiÔ¨Åc one, and that future notions of fairness and
balance will emerge to place the problem of web page ranking on a
Ô¨Årmer scientiÔ¨Åc foundation.
Acknowledgments
The authors would like to thank Michael Dybicz for his vision and
assistance in securing resources for carrying out this research.

11. REFERENCES
[1] Serge Abiteboul, Mihai Preda, and Grgory Cobena. Adaptive

on-line page importance computation. In Proc. 12th World
Wide Web Conference, pages 280‚Äì290, 2003.

[2] Gianni Amati, Iadh Ounis, and Vassilis Plachouras. The
dynamic absorbing model for the web. Technical Report
TR-2003-137, University of Glasgow, April 2003.

[3] Arvind Arasu, Jasmine Novak, Andrew S. Tomkins, and

John A. Tomlin. Pagerank computation and the structure of
the web: Experiments and algorithms. In Poster Proc.
WWW2002, Honolulu, 2002.

[4] T. Berners-Lee, R. Fielding, and L. Masinter. Uniform

resource identiÔ¨Åers (URI): Generic syntax.
http://www.ietf.org/rfc/rfc2396.txt. RFC 2396.

[5] Allan Borodin, Gareth O. Robers, Jeffrey S. Rosenthal, and
Panayiotis Tsaparas. Finding authorities and hubs from link
structures on the world wide web. In Proc. 10th World Wide
Web Conference, pages 415‚Äì429, 2001.

[6] Sergey Brin, Rajeev Motwani, Lawrence Page, and Terry
Winograd. What can you do with a web in your pocket?
Data Engineering Bulletin, 21:37‚Äì47, 1998.

[7] Soumen Chakrabarti, Byron Dom, David Gibson, Jon M.
Kleinberg, Prabhakar Raghavan, and Sridhar Rajagopalan.
Automatic resource compilation by analyzing hyperlink
structure and associated text. In Proc. 7th World Wide Web
Conference, pages 65‚Äì74, 1997.

[8] Yen-Yu Chen, Qingqing Gan, and Torsten Suel. I/O efÔ¨Åcient

techniques for computing pagerank. In CIKM 2002, pages
549‚Äì557, McLean, Virgina, 2002.

[9] Steve Chien, Cynthia Dwork, Ravi Kumar, and

D. Sivakumar. Towards exploiting link evolution. In
Workshop on Algorithms and Models for the Web Graph,
2001.

[10] Junghoo Cho, Hector Garcia-Molina, and Lawrence Page.

EfÔ¨Åcient crawling through url ordering. In Proc. of 7th World
Wide Web Conference, 1998.

[11] Nick Craswell, David Hawking, and Stephen E. Robertson.
Effective site Ô¨Ånding using link anchor information. In Proc.
of the 24th annual international ACM SIGIR conference on
research and development in information retrieval, pages
250‚Äì257, New Orleans, Louisiana, USA, September 2001.
Association for Computing Machinery.

[12] Brian D. Davison. Recognizing nepotistic links on the web.

In ArtiÔ¨Åcial Intelligence for Web Search, pages 23‚Äì28. AAAI
Press, July 2000.

[13] Chris Ding, Xiaofeng He, Parry Husbands, Hongyuan Zha,

and Horst Simon. Pagerank, hits and a uniÔ¨Åed framework for
link analysis. In Proc. of 25th ACM SIGIR, pages 353‚Äì354,
Tampere, Finland, 2002.

[14] Nadav Eiron and Kevin S. McCurley. Analysis of anchor text
for web search. In Proc. of 26th ACM SIGIR, pages 459‚Äì460,
2003.

[15] Nadav Eiron and Kevin S. McCurley. Locality, hierarchy,

and bidirectionality in the web. In Workshop on Algorithms
and Models for the Web Graph, Budapest, May 2003.

[16] Nadav Eiron and Kevin S. McCurley. Untangling compound

documents on the web. In Proc. 14th ACM Conf. on
Hypertext, pages 85‚Äì94, 2003.

[17] Ronald Fagin, Ravi Kumar, Kevin S. McCurley, Jasmine

Novak, D. Sivakumar, John A. Tomlin, and David P.
Williamson. Searching the workplace web. In Proc. 12th
World Wide Web Conference, Budapest, 2003.

[18] Gene H. Golub and Charles van Loan. Matrix Computations.

Johns Hopkins University Press, Baltimore, 3rd edition,
1996.

[19] Siegfried Handschuh, Steffen Staab, and Raphael Volz. On
deep annotation. In Proc. 12th World Wide Web Conference,
pages 431‚Äì438, Budapest, 2003.

[20] Taher Haveliwala. EfÔ¨Åcient computation of pagerank.

Technical report, Stanford University, 1999.

[21] Taher H. Haveliwala. Topic-sensitive pagerank. In Proc. 11th

World Wide Web Conference, pages 517‚Äì526, Honolulu,
2002.

[22] Sepandar Kamvar, Taher Haveliwala, and Gene Golub.

Adaptive methods for the computation of pagerank.
Technical report, April 2003.

[23] Sepandar D. Kamvar, Taher H. Haveliwala, Christopher D.

Manning, and Gene H. Golub. Exploiting the block structure
of the web for computing pagerank. Technical report,
Stanford University, 2003.

[24] Sepandar D. Kamvar, Taher H. Haveliwala, Christopher D.

Manning, and Gene H. Golub. Extrapolation methods for
accelerating pagerank computations. In Proc. 12th World
Wide Web Conference, 2003.

[25] Ronny Lempel and Shlomo Moran. SALSA:the stochastic
approach for link-structure analysis. ACM Transactions on
Information Systems, 19(2):131‚Äì160, 2001.

[26] John Markwell and David W. Brooks. Link rot limits the

usefulness of web-based educational materials in
biochemistry and molecular biology. Biochem. Mol. Biol.
Educ., 31:69‚Äì72, 2003.

[27] Marc Najork and Janet L. Wiener. Breadth-Ô¨Årst search

crawling yields high-quality pages. In Proc. 10th World Wide
Web Conference, pages 114‚Äì118, 2001.

[28] Lawrence Page, Sergey Brin, Rajeev Motwani, and Terry

Winograd. The PageRank citation ranking: Bringing order to
the web. Technical report, Stanford Digital Library
Technologies Project, 1998. Paper SIDL-WP-1999-0120
(version of 11/11/1999).

[29] Gopal Pandurangan, Prabhakar Raghavan, and Eli Upfal.

Using pagerank to characterize web structure. In COCOON
2002, pages 330‚Äì339, Singapore, 2002. Springer-Verlag.
LNCS 2387.

[30] Davood RaÔ¨Åei and Alberto Mendelzon. What is this page
known for? Computing web page reputations. In Proc. 9th
World Wide Web Conference, Amsterdam, 2000.

[31] Diomidis Spinellis. The decay and failures of web

references. Comm. ACM, 46(1):71‚Äì77, 2003.

[32] John A. Tomlin. A new paradigm for ranking pages on the

world wide web. In Proc. 12th World Wide Web Conference,
pages 350‚Äì355, Budapest, May 2003.

318