A Dynamic Bayesian Network Click Model

for Web Search Ranking

Olivier Chapelle

Yahoo! Labs

Santa Clara, CA

chap@yahoo-inc.com

Ya Zhang
Yahoo! Labs

Santa Clara, CA

yazhang@yahoo-inc.com

ABSTRACT
As with any application of machine learning, web search
ranking requires labeled data. The labels usually come in
the form of relevance assessments made by editors. Click
logs can also provide an important source of implicit feed-
back and can be used as a cheap proxy for editorial labels.
The main diﬃculty however comes from the so called posi-
tion bias — urls appearing in lower positions are less likely
to be clicked even if they are relevant.
In this paper, we
propose a Dynamic Bayesian Network which aims at pro-
viding us with unbiased estimation of the relevance from
the click logs. Experiments show that the proposed click
model outperforms other existing click models in predicting
both click-through rate and relevance.

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]; H.3.5 [Online
Information Services]; I.2.6 [ARTIFICIAL INTELLI-
GENCE]: Learning; I.6.m [SIMULATION AND MOD-
ELING]: Miscellaneous

General Terms
Algorithms, Design, Experimentation

Keywords
Click-through rate, Click modeling, Web search, Ranking,
Dynamic Bayesian network

1.

INTRODUCTION

Web page ranking has been traditionally based on hand
designed ranking functions such as BM25 [18]. With the
inclusion of thousands of features for ranking, hand-tuning
of ranking function becomes intractable. Several machine
learning algorithms have been applied to automatically op-
timize ranking functions [4, 5]. Machine learned ranking re-
quires a large number of training examples, with relevance
labels indicating the degree of relevance for each query-
document pair. The cost of the editorial labeling is usually
quite expensive. Moreover, the relevance labels of the train-
ing examples could change over time. For example, if the
query is time sensitive or recurrent (e.g. ”www” or ”presi-
dential election”), a search engine is expected to return the
Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2009, April 20–24, 2009, Madrid, Spain.
ACM 978-1-60558-487-4/09/04.

most up-to-date documents/sites to the users. However, it
would be prohibitive to keep all the relevance labels up to
date. Click logs embed important information about user
satisfaction with a search engine and can provide a highly
valuable source of relevance information. Compare to edi-
torial labels, clicks are much cheaper to obtain and always
reﬂect current relevance.

Clicks have been used in multiple ways by a search en-
gine: to tune search parameters, to evaluate diﬀerent rank-
ing functions [7, 13, 14, 15], or as signals to directly inﬂuence
ranking [1, 13]. However, clicks are known to be biased, by
the presentation order, the appearance (e.g. title and ab-
stract) of the documents, and the reputation of individual
sites. Many studies [8, 10] have attempted to account the
position-bias of click. Carterette and Jones [7] proposed to
model the relationship between clicks and relevance so that
clicks can be used to unbiasedly evaluate search engine when
lack of editorial relevance judgment. Other research [10, 21,
16] attempted to model user click behavior during search
so that future clicks may be accurately predicted based on
observations of past clicks.

Two diﬀerent types of the click models are position mod-
els [8, 10, 17] and the cascade model [8]. A position model
assumes that a click depends on both relevance and exami-
nation. Each rank has a certain probability of being exam-
ined, which decays by rank and depends only on rank. A
click on a url1 indicates that the url is examined and con-
sidered relevant by the user. However this model treats the
individual urls in a search result page independently and
fails to capture the interaction among urls in the examina-
tion probability. Take for example two equally relevant urls
for a query: a user may only click on the top one, feel satis-
ﬁed, and then leave the search result page. In this case, the
positional bias cannot fully explain the lack of clicks for the
second url.

The cascade model assumes that users examine the re-
sults sequentially and stop as soon as a relevant document
is clicked. Here, the probability of examination is indirectly
determined by two factors: the rank of the url and the rel-
evance of all previous urls. The cascade model makes a
strong assumption that there is only one click per search and
hence it could not explain the abandoned search or search
with more than one clicks. Even though the cascade model
is quite restrictive, the authors of that paper showed that

1We refer to url (or equivalently document) as a shorthand
for the entire display block consisting of the title, abstract
and url of the corresponding result.

WWW 2009 MADRID!Track: Data Mining / Session: Click Models1it can predict click-through rates (CTRs)2 more accurately
than the position models described above.

None of the above models distinguish perceived relevance
and actual relevance3. Because users cannot examine the
content of a document until they click on the url, the decision
to click is made based on perceived relevance. While there is
a strong correlation between perceived relevance and actual
relevance, there are also many cases where they diﬀer.

In this paper, a dynamic bayesian network (DBN) model
is proposed to model the users’ browsing behavior. As in
the position model, we assume that a click occurs if and
only if the user has examined the url and deemed it rele-
vant. Similar to the cascade model, our model assumes that
users make a linear transversal through the results and de-
cide whether to click based on the perceived relevance of
the document. The user chooses to examine the next url
if he/she is unsatisﬁed with the clicked url (based on actual
relevance). Our model diﬀers from the cascade model in two
aspects: 1. because a click does not necessarily mean that
the user is satisﬁed with the clicked document, we attempt
to distinguish the perceived relevance and actual relevance.
2. We do not limit the number of clicks that a user can make
during a search.

We compare the proposed model with pervious models
and show that the dynamic bayesian network based model
outperforms the others. The predicted relevance for each url
are then used in two ways: either as a feature in a ranking
function or used as supplementary data to learn a ranking
function. We show that the function learned with these pre-
dicted relevance is not far from being as good as a function
trained with a large amount of editorial data. We further
show that combining both type of data can lead to an even
more accurate ranking function.

2. MODELING PRESENTATION BIAS

As explained above, the presentation bias refers to the
fact that users are more likely to click on documents at the
top of the ranking.
2.1 Position models

A popular class of methods for dealing with this presen-
tation bias problem are the position based models [8, 10,
17]. A core assumption in these methods is that the user
clicks on a link if the following two conditions are met: the
user examined the url and found it relevant; in addition, the
probability of examination depends only on the position.
More precisely, given a url u at position p, the probability
of a click is modeled through a hidden variable E denoting
if u was examined or not:

P (C = 1|u, p)

X

=

e∈{0,1}

|

P (C = 1|u, p, E = e)P (E = e|u, p)

= P (C = 1|u, E = 1)

P (E = 1|p)

.

{z

:=αu

}

|

{z

:=βp

}

2The click-through rate of an url is deﬁned as the ratio be-
tween the number of times this url was clicked and the num-
ber of times it was shown.
3Perceived relevance is the relevance of the url presented by
a search engine. Actual relevance means the relevance of the
landing page.

The last equation made use of the following assumptions:
there is no click if the user did not examine the url; if the
url is examined, the probability of click depends only on its
relevance; the probability of examination depends only the
position. As a result the probability of a click is the product
between two probabilities αu and βp: the ﬁrst one models
the relevance of the url to the query while the second one
captures the position eﬀect. Remember that our goal is to
infer the relevance of an url based on the click logs. That
is exactly what αu represents: the perceived relevance of an
url to the user, independent of the position. If we make the
additional assumption that β1 = 1 — that is the user always
examine the ﬁrst result — then αu can be interpreted as an
equivalent CTR at position 1, i.e. the CTR of that url had
it been placed in the ﬁrst position. Note that the query q is
implicit here; more formally, we should write αuq to stress
the dependence to the query, but in the rest of paper we
assume that the query is ﬁxed.

2.1.1 COEC model
A cheap and straightforward approach is to estimate βp
as the aggregated CTR (over all queries and sessions) in po-
sition p. Suppose there are N sessions in which u appeared
and for the i-th session ci ∈ {0, 1} indicates if there was
a click and pi is the position in which the url u appeared.
Then αu is computed as [19]:

PN
PN

i=1 ci
i=1 βpi

αu =

.

(1)

As in [19] we refer to this method as clicks over expected
clicks (COEC), because the denominator can be seen as the
number of “expected” clicks given the positions that the url
appeared in.

The problem with the COEC model is that the estimation
of β is biased. It would be valid if the search engine gives re-
sults in a random order. But since more relevant documents
tend to appear higher in the ranking, the observed CTR at
a given position captures not only the position bias, but also
the typical relevance at this position.

2.1.2 Examination model
Another approach is to ﬁnd αu and βp by maximum like-
lihood. Note that the urls need to have been shown in
diﬀerent positions for this approach (and other below) to
be meaningful. Otherwise the solution is ill-deﬁned. This
makes sense because in order to capture the position eﬀect,
one needs to observe the CTR of the same url at diﬀerent
positions. That is usually the case because of the constant
variations in a search engine. Given the vector βp, the max-
imum likelihood solution for αu is:

αu = arg max

α

ci log(αβpi ) + (1 − ci) log(1 − αβpi ). (2)

NX

i=1

The vector βp is estimated by an alternate (or joint) maxi-
mization of the likelihood between αu and βp.

A drawback of the above approach is that it can lead
to αu > 1. This is not desirable since αu is supposed to
represent a probability. Instead of maximizing the likelihood
directly, one can use the Expectation-Maximization (EM)
algorithm where the hidden variables are the examination
variables E [10]. This ensures that αu ≤ 1. We used the

WWW 2009 MADRID!Track: Data Mining / Session: Click Models2EM algorithm in our implementation of the the examination
model.
2.1.3 Logistic model
Another alternative is to use a slightly diﬀerent model

related to logistic regression [8]:

P (C = 1|u, p) :=

1

1 + exp(− ˜αu − ˜βp)

.

(3)

The click probability is not a product of probabilities any
longer, but it is still a function of the url and of the position.
The main advantage is that it ensures that the resulting
probability is always between 0 and 1; also the optimization
is much easier since it is an unconstrained and jointly convex
problem.
2.2 Cascade Model

Ei−1

Ei

Ei+1

Ci

Ai

au

Si

su

i−1Y

Cascade model [8] diﬀers from the above position models
in that it considers the dependency among urls in a same
search results page and model all clicks and skips simulta-
neously in a session. It assumes that the user views search
results from top to bottom and decides whether to click each
url. Once a click is issued, documents below the clicked re-
sult are not examined regardless of the position. With the
cascade model, each document d, is either clicked with prob-
ability rd (i.e. probability that the document is relevant)
or skipped with probability (1-rd). The cascade model as-
sumes that a user who clicks never comes back, and a user
who skips always continues. A click on the i-th document
indicates: 1. the user must have decided to skip the ranks
above; 2. the user deem the i-th document relevant. The
probability of click on i-th document can thus be expressed
as:

P (Ci = 1) = ri

(1 − rj).

(4)

j=1

3. DYNAMIC BAYESIAN NETWORK

We now introduce another model which considers the re-
sults set as a whole and takes into account the inﬂuence of
the other urls while estimating the relevance of a given url
from click logs. The reason to consider the relevance of other
urls is the following: take for instance a relevant document
in position 3; if both documents in position 1 and 2 are very
relevant, it is likely that this document will have very few
clicks; on the other hand, if the two top documents are irrel-
evant, it will have a lot of clicks. A click model depending
only on the position will not be able to make the distinction
between these two cases. We extend the idea of cascade
model and propose a Dynamic Bayesian Network (DBN)
[11] to model simultaneously the relevance of all documents.
3.1 Model

The Dynamic Bayesian Network that we propose is illus-
trated in ﬁgure 1. The sequence is over the documents in
the search result list. For simplicity, we keep only the top
10 documents appearing in the ﬁrst page of results, which
means that the sequence goes from 1 to 10. The variables
inside the box are deﬁned at the session level, while those
out of the box are deﬁned at the query level. As before, we
assume that the query is ﬁxed.

For a given position i, in addition to the observed vari-
able Ci indicating whether there was a click or not at this

Figure 1: The DBN used for clicks modeling. Ci is
the the only observed variable.

position, the following hidden binary variables are deﬁned
to model examination, perceived relevance, and actual rele-
vance, respectively:

• Ei: did the user examine the url?
• Ai: was the user attracted by the url?
• Si: was the user satisﬁed by the landing page?
The following equations describe the model:
Ai = 1, Ei = 1 ⇔ Ci = 1

P (Ai = 1) = au
P (Si = 1|Ci = 1) = su

P (Ei+1 = 1|Ei = 1, Si = 0) = γ

Ci = 0 ⇒ Si = 0
Si = 1 ⇒ Ei+1 = 0

Ei = 0 ⇒ Ei+1 = 0

(5a)

(5b)

(5c)

(5d)

(5e)

(5f)

(5g)

As in the examination model, we assume that there is a click
if and only if the user looked at the url and was attracted by
it (5a). The probability of being attracted depends only on
the url (5b). Similar to the cascade model, the user scans
the urls linearly from top to bottom until he decides to stop.
After the user clicks and visits the url, there is a certain
probability that he will be satisﬁed by this url (5c). On the
other hand, if he does not click, he will not be satisﬁed (5d).
Once the user is satisﬁed by the url he has visited, he stops
his search (5e).
If the user is not satisﬁed by the current
result, there is a probability 1 − γ that the user abandons
his search (5f) and a probability γ that the user examines
the next url. In other words, γ measures the perseverance of
the user4. If the user did not examine the position i, he will
not examine the subsequent positions (5g). In addition, au
and su have a beta prior. The choice of this prior is natural
because the beta distribution is conjugate to the binomial
distribution.
It is clear that some of the assumptions are
not realistic and we discuss in section 8 how to extend them.
However, as shown in the experimental section, this model
can already explain accurately the observed clicks.
4it would be better to deﬁne the perseverance γ at the user
level, but we simply take the same value for all users.

WWW 2009 MADRID!Track: Data Mining / Session: Click Models3Unlike the examination model, our model has two vari-
ables au and su related to the relevance of the document.
The ﬁrst one models the perceived relevance since it mea-
sures the probability of a click based on the url. The second
one is the probability that the user is satisﬁed given that
he has clicked on the link; so it can been understood as a
”ratio” between actual and perceived relevance. Indeed, if
we deﬁne the relevance of the url as the probability that the
user is satisﬁed given that he has seen the url, we have:

ru :=P (Si = 1|Ei = 1)

=P (Si = 1|Ci = 1)P (Ci = 1|Ei = 1)
=ausu

(6)

As far we know, this is the ﬁrst click model which attempts
to model the actual relevance rather than the perceived rel-
evance only.
3.2 Link with other models

The examination model can be seen as a special case of our
model where the Ei are independent and have a distribution
that only depends on the position. In that case the Si are
meaningless because they cannot be inferred.

The cascade model of [8] is a special case of our model
with γ = 1 and su = 1. That is, the user keeps examining
until he ﬁnds a document that appears relevant. He then
clicks and stops.

In [7] the relevance of the documents is predicted from
click logs and the CTRs of the other documents is used dur-
ing the prediction. Their motivation is diﬀerent (which is to
evaluate the quality of search engine), but they address the
same problem: the inﬂuence of other documents on CTRs.
However the deﬁnition of CTRs in other positions is prob-
lematic if the set of documents changed between sessions.

Joachims [13] introduced the so-called skip-above pairs:
when the user has not clicked at position i but has clicked
at position j > i, it is an indication that the document in
position j is preferred to the document in position i. We
recover this type of pair with our model because in that
case 1 = Aj > Ai = 0. However, the problem with learning
a ranking function with skip-above pairs is that one tends to
learn the reverse function than the one in production (there
are only ”negative” instances in some sense).

For the position models, it has been observed that it is
better to use diﬀerent vectors β for diﬀerent type of queries
such as navigational vs informational5. The reason is that
for navigational queries, the CTRs decay much faster with
the position. But we argue that this decay is not a func-
tion of the query type, but of the quality of the top urls.
For a navigational query, the top result is usually excellent
and there are very few clicks in lower positions. Our click
model captures this eﬀect directly and does not need diﬀer-
ent browsing models for diﬀerent type of queries.
3.3 Inference

The Dynamic Bayesian Network of ﬁgure 1 is just slightly
more complex than a standard Hidden Markov Model (HMM)
because of the conditional dependence between the hidden
state at position i + 1 and the observation at position i
given the hidden state at position i. The Expectaction-
Maximization (EM) algorithm [9] is used to ﬁnd the maxi-

mum likelihood estimate of the variables au and su and the
forward-backward algorithm is used to to compute the pos-
terior probabilities of the hidden variables. Please refer to
the Appendix of the paper for a detailed derivation of the E-
step and M-step, as well as a conﬁdence computation for the
latent variables. Even though we could have also estimated
γ with EM, we simply treated γ as a conﬁgurable parameter
for the model.

4. EXPERIMENTS

Three types of experiments are performed to validate our
model. First we evaluate the click model in terms of pre-
dicted CTR at position 1. Then we use the predicted rele-
vance as signals in ranking. In a third type of experiments,
we use the predicted relevance as targets to train a ranking
function.

The click log is obtained from a commercial search engine.
A session can be deﬁned in various ways, but for all our
experiments, it is deﬁned as as follows. A session always
has a unique user and unique query. It starts when a user
issues a query and ends with 60 minutes idle time on the
user side. For each session, we get the query, list of urls in
result sets, and list of clicked urls. Simple normalization is
applied to queries and urls. As explained before, we restrict
ourselves to the top 10 urls on the ﬁrst result page. Sessions
for which the clicks are not in the same order as the ranking
(for instance the user clicks ﬁrst on the 4th link then on
the 2nd link) have zero probability under our model. On
average, roughly 3% of the sessions contain one or more
out-of-the-order clicks. We could have decided to swap the
click order for these sessions, but we simply discarded them.
We also discarded all the queries for which we have less than
10 sessions.
4.1 Predicting click-through rate

We ﬁrst assess how accurate is our proposed click model.
For this task, we get 58M sessions and 682k unique queries
from the click log of the UK market. A natural way to
evaluate a click model would be to proceed as in [10]: for
a given query, take some sessions for training and evaluate
the likelihood of clicks on the other sessions. But our goal is
diﬀerent: we are not so much interested in click prediction,
but more on how accurate the latent variables au and su
are: they are indeed the variables that will be used later for
ranking. It is diﬃcult to assess the quality of su, but it is
easy for au. Indeed au (like αu for the examination model)
is a prediction of the CTR in position 1. The following
leave-one-out experimental protocol is used:
1: Retrieve all the sessions related to a given query;
2: Consider an url that appeared both in position 1 and

some other positions;

3: Hold out as test sessions all the sessions in which that

url appeared in position 1;

4: Train the model on the remaining sessions and predict

au;

5: Compute the test CTR in position 1 on the held-out

sessions;

6: Compute an error between these two quantities;
7: Average the error on all such urls and queries, weighted

by the number of test sessions.

5A query is said to be navigational when the user intent is
to reach a particular site [3].

When computing the error between the actual CTR in
position 1 and the predicted one, two types of error metrics

WWW 2009 MADRID!Track: Data Mining / Session: Click Models4Figure 2: Accuracy of diﬀerent clicks models in predicting the CTR at position 1 as measured by the mean
squared error (left) and KL divergence (right). The x-axis shows the minimum number of training sessions
that have been used for estimating this CTR.

are used: the Mean-Square-Error (MSE) and the KL diver-
gence. Note that the KL divergence is, up to a constant
value, the same as the likelihood of the clicks on the test
sessions.

The DBN model requires the input of a parameter γ. We
ﬁrst perform the above leave-one-out procedure to empiri-
cally determine the optimal value of γ. The result is shown
in ﬁgure 3. The DBN model reaches the smallest MSE when
γ = 0.9, which indicates that users are persistent in ﬁnding
relevant documents. For the rest of the experiments, we set
γ to 0.9.

N +ap

D+ap+bp

D by their smoothed version

the entire dataset. We then replace the raw CTRs of the
form N
. The COEC
model predicts sometimes CTR larger than 1: in that case,
we clamp the prediction to 1 when computing the MSE er-
ror. However, even after clamping to 1, the KL divergence
is inﬁnite when the test CTR is smaller than 1. That is the
reason why COEC does not appear in the right hand side
of ﬁgure 2. As for the cascade model described by equation
(4), it can only handle sessions which have exactly one click.
When testing this method we thus discarded all the sessions
which have zero or more than one click. While the cascade
model is rather restricted by assuming that users are very
persistent in examining until they ﬁnd a relevant document,
we see that it performs better than the COEC and examina-
tion model and comparably to the logistic model, indicating
it is useful to consider the interaction between urls in the
same search result page. These results are consistent with
the ones reported in [8].

In ﬁgure 2, we broke down the errors as a function of the
minimum number of sessions used for training. For instance
1000 means that the MSE has been computed on all the
(query,urls) pairs such that the number of sessions in which
this url does not appear in position 1 is at least 1000. When
the number of sessions is large, the choice of the prior distri-
butions on au and su does not play an important role. One
would typically expect that the accuracy improves with the
number of sessions. That is roughly the case for the cas-
cade and our model, but surprisingly, the accuracy of the
other models deteriorates when the number of sessions is
very large. We investigated this issue by looking at the
(query,url) pairs with a large error. An example was the
query ”myspace” and the url www.myspace.com in the UK
market. The url in postion 1 should be uk.myspace.com,
but because of variations in the ranking due to tests for in-
stance, the url www.myspace.com appeared several times in
position 1. For these sessions, the CTR was very high, 0.97,
as expected. In other sessions, it appeared in position 2 and
had a low CTR of 0.11. This low CTR is expected because
the url in ﬁrst position is then uk.myspace.com and most
users do not even look at www.myspace.com. However, the
logistic model predicted a CTR of 0.21. The predicted CTR

Figure 3: Mean squared error of the DBN model in
predicting the CTR at position 1 as a function of γ.

We compare the DBN model with other click models de-
scribed in section 2. The results are shown in ﬁgure 2. The
COEC, examination, and logistic models are respectively de-
scribed by equations (1), (2), and (3). For these methods
we use the empirical Bayes method [6] with a beta prior to
smooth the observed probabilities of clicks at diﬀerent po-
sitions. This yields more stable and accurate results. More
precisely, the CTRs at a given position p are assumed to
be drawn from a beta distribution with parameters ap and
bp. These parameters are found by maximum likelihood on

10010110210310400.020.040.060.080.10.120.140.160.18Minimum number of sessionsMSE  LogisticExaminationDBNCascadeCOEC10010110210310400.050.10.150.20.250.30.350.4Minimum number of sessionsKL divergence  LogisticExaminationDBNCascade0.20.40.60.810.0360.0380.040.0420.0440.0460.0480.05MSEγWWW 2009 MADRID!Track: Data Mining / Session: Click Models5of the logistic model is understandable because in average
there is roughly a factor 2 diﬀerence between the CTR of an
url in position 1 and in position 2. The reason why there is
a factor 9 for the query myspace is because the url shown in
position 1 was already excellent and the users hardly looked
at position 2. The examination and logistic model do not
take this information into account but the DBN does:
it
predicted a CTR of 0.95 for www.myspace.com. Perfect urls
in position 1 tend to happen more often for navigational
queries. For this type of query, we have a lot of sessions; that
explains why the bad performance of the position models is
exacerbated when a large number of sessions is considered.
We have not done a formal comparison with the model of
[10], but preliminary experiments show that it suﬀers from
the same problem as the other position model described
above.

Figure 4: The β vector from COEC (1) and the ex-
amination (2) models. The former corresponds to
the observed CTRs, while the latter aims at isolat-
ing the position eﬀect in CTR modeling.

Of peripheral interest, but still noteworthy is ﬁgure 4
which compares the vector β for the two models of the form
P (C = 1|u, p) = αuβp. As explained in section 2, the val-
ues of βp decay too fast for the COEC model, because this
model does not make the distinction between the position
eﬀect and the fact that more relevant documents tend to
appear at the top of the ranking. The logistic and examina-
tion models are able to make this distinction, intuitively by
taking advantage of the position change of some of the urls
in the click logs.
4.2 Predicted relevance as a ranking feature
The accuracy of CTR prediction may not directly trans-
late to relevance. In the second set of experiments, we use
the predicted relevance directly to rank urls. In this case, the
relative order of click prediction is more important than its
absolute values. We compare the DBN model with the cas-
cade model and the logistic model. We also include in the
comparison a baseline ranking function φ that uses many
other ranking signals (e.g. BM25 scores). This function is
typical of ranking functions used by web search engines.

The data is diﬀerent from above: we considered only the
queries for which we have editorial judgments and for which
we have at least 10 sessions over a period of several months.
This resulted in 3153 queries and 44.5M sessions.

The following experimental protocol is used:

1: Retrieve all the sessions related to a given query;
2: Consider all urls with editorial relevance judgments;
3: Train the model on the sessions and predict au and su

for the urls;

4: Sort the urls with respect to the predicted relevance (6)

given by the DBN model;

5: Compute the Normalized Discounted Cumulated Gain

(NDCG) [12] at rank 5;

6: Average the NDCG5 on all queries.

The results are shown in ﬁgure 5 where we broke down the
NDCG5 as a function of the minimum number of sessions
required for a url. For instance, the minimum number of ses-
sions of 1000 means that the NDCG5 has been computed on
all the (query,urls) pairs such that the number of sessions
for this url is at least 1000. With the increasing number
of sessions for each url, we expect the click prediction to
be more accurate and more conﬁdent, leading to improved
ranking of the urls. This is exactly what we have observed
in ﬁgure 5 – for all the click models, NDCG5 improves with
the number of sessions. However, when we restrict the ex-
periment to higher number of sessions for the urls, fewer urls
are left for each query. In the extreme case, a query could
contain only one url and the NDCG5 would always be one.
Indeed the average number of urls per query is 10.5 overall,
but if we restrict ourselves to urls with more than 10,000
sessions, this number goes down to 8. The NDCG5 is thus
less discriminative when the number of sessions for each urls
is very high and as a result, the performance of the baseline
function is not constant. That is the reason why we also
plot the NDCG5 relative to the baseline function (right of
ﬁgure 5) to remove the eﬀect of varying number of urls per
query.

In general, the DBN model is able to rank the urls bet-
ter than the logistic model and the cascade model. As ex-
pected, with the increasing number of sessions for the urls
(i.e. predictions are more conﬁdent), both the NDCG5 and
the relative NDCG5 increases. As a special case of the DBN
model (γ = 1 and su = 1), the cascade model behaves very
similarly as the DBN model but with a lower NDCG5, which
conﬁrms the necessity of introducing the notion of satisfac-
tion su and perseverance γ. The cascade model suﬀers in-
deed from being able to only consider sessions with exactly
one click. On the other hand, the logistic model behaves very
diﬀerently than the cascade model and the DBN model. The
diﬀerence of relative NDCG5 between the DBN model and
logistic model is large when the number of sessions is small.
When the number of sessions becomes larger, the diﬀerence
between DBN and logistic models gets smaller, mainly be-
cause there are less number of urls for each query.

Given the above observations, we then ﬁx the minimum
number of sessions to 10 and the minimum number of urls
per query to 10. As a result 392 queries pass the criteria
and on average each query contains 13.4 urls. We then com-
pute NDCG5 on this data set. As shown in table 1, the
NDCG5 for the DBN model is 5.8% and 2.4% better than
that of the logistic model and the cascade model respectively.
All the diﬀerence are statistically signiﬁcant according to a
Wilcoxon sign-rank test (p ≤ 0.001). In order to quantify
the inﬂuence of the satisfaction variable su, we also ranked
according to au only instead of (6). The diﬀerence is only
0.5%, not statistically signiﬁcant. We will discuss in section
7 an extension of our model resulting in a better modeling
of the satisfaction.

24681000.10.20.30.40.50.60.70.80.91Positionβp  COECExaminationWWW 2009 MADRID!Track: Data Mining / Session: Click Models6Figure 5: The predicted relevance is used as the only signal to rank urls. NDCG5 is as a function of the
minimum number of sessions for each url (left). The NDCG5 is also plotted relative to the baseline function
(right).

Table 1: NDCG5 computed when requiring at least
10 sessions per url and at least 10 urls for each query.
Right column shows the relative diﬀerence with re-
spect to the DBN model.

Logistic
Cascade
DBN
DBN (au only)
Baseline φ

-5.8%
-2.4%

0.705
0.73
0.748
0.744
-0.5%
0.795 +6.3%

–

Compared to the baseline ranking function φ, the NDCG5
for the DBN model is only 6.3% worse. Given the baseline
function φ uses more than hundreds of ranking signals and
is trained with rather large set of editorially labeled data,
this indicates the predicted relevance are very accurate in
terms of ranking.

We then try to improve the baseline function φ by using
the predicted relevance from the DBN model as an addi-
tional ranking signal. About 0.8% NDCG5 gain was achieved.
Furthermore, this signal is observed to be one of the top 10
important ranking signals among hundreds of ranking sig-
nal, indicating the high correlation between the predicted
relevance and relevance.
4.3 Learning a ranking function with predicted

relevance

Machine learning for web search ranking has ﬁrst been
introduced in [4]; we follow here the gradient boosted de-
cision trees framework applied to pairwise preferences [20].
We have two sets of pairwise preferences:

1. PE comes from editorial judgments on 4180 queries
and 126k urls, resulting in about 1M preference pairs;
2. PC comes from our click model: we keep only 1.1M
urls (corresponding to 420k unique queries) by ﬁlter-
ing based on a threshold on the conﬁdence (equation

(8)); converting the relevance scores (equation (6)) into
preferences yields about 2M pairs.

For each (query,url) pair we extract a feature vector x. A
pair (xi, xj) in the preference set indicates that xi is pre-
ferred to xj, which should ideally translates to f (xi) >
f (xj), where f is the ranking function.

The boosting algorithm optimizes the following objective

function (see [20] for details on the boosting procedure):

max(0, 1 − (f (xi) − f (xj)))2 +

max(0, 1 − (f (xi) − f (xj)))2.

(7)

P
P

(xi,xj )∈PE

(xi,xj )∈PC

1 − δ
|PE|
δ
|PC|

Figure 6: Relative DCG5 for various values of δ (see
equation (7)). The relative DCG5 is normalized to
be 1 for δ = 0, which corresponds to learning only
based on editorial judgments. On the other hand
when δ = 1 only click data are used for learning.

The objective function is thus a combination of editorial
based and click based preference. The test set is a held

1001011021031040.720.740.760.780.80.820.840.860.880.90.92Minimum number of sessionsNDCG5  DBNLogisticCascadeBaseline φ1001011021031040.880.90.920.940.960.9811.02Minimum number of sessionsRelative NDCG5  DBNLogisticCascade00.20.40.60.810.970.980.9911.011.02δRelative DCG5WWW 2009 MADRID!Track: Data Mining / Session: Click Models7out set of editorial judgments on which the Discounted Cu-
mulative Gain (DCG) at rank 5 is computed. The relative
performance as a function of δ is plotted in ﬁgure 6.

We can draw two interesting conclusions from this plot:

1. Learning only from clicks, the DCG is only 4% worse
than a standard model learned with editorial judg-
ments; this is remarkable because in this experiment
the set of editorial judgments is relatively large. This
is an indication that learning from clicks can be very
valuable for markets where there are few or no editorial
judgments.

2. Combining both type of data lead to a 2% gain on
DCG, which is considered substantial in the web search
ranking community. So even in markets where a lot of
editorial judgments are available, we can still leverage
clicks to reach higher DCG.

Finally note that the evaluation has been done with an edito-
rial metric. But because of the discrepancies between clicks
and editorial judgments discussed below, we expect that our
model trained on clicks would perform even better had we
evaluated it on a click based metric.

5. A SIMPLIFIED MODEL

As we have shown in ﬁgure 3, the best prediction of the
CTR at position 1 was obtained for γ = 0.9. But γ = 1
produces only slightly worse prediction. And this particu-
lar setting is interesting because in this case, the inference is
much simpler. Indeed the user then keeps examining until he
is satisﬁed, which means that the last click provided a satis-
fying result and the results below it were not examined. The
forward-backwards algorithm and EM are thus not needed
because there is no ambiguity on the examination variables:
E1 = ··· = E(cid:96) = 1 and E(cid:96)+1 = ··· = E10 = 0 with (cid:96) be-
ing the position of the last click. The latent variables au
and su are estimated using simple counting as described in
algorithm 1.

Algorithm 1 Simpliﬁed model estimation for γ = 1.

u , sN

u , aD

Initialize aN
current query.
for all sessions do

u , sD

u to 0 for all urls u associated with

for all u above or at the last clicked url do

u ← aD
aD

u + 1

end for
for all u that got clicked do

u + 1
u + 1

u ← aN
aN
u ← sD
sD
end for
u ← sN
sN

u + 1, where u is the last clicked url.

end for// αa, βa, αs, βs are prior Beta parameters for au
and su.
for all urls u do

au = (aN
su = (sN

u + αa)/(aD
u + αs)/(sD

u + αa + βa).
u + αs + βs).

end for

6. CLICKS VS EDITORIAL JUDGMENTS
In the last two sets of experiments, clicks are used as a
proxy for editorial judgments. It is important to evaluate the

correlation between relevance estimated by our click model
and actual relevance given by editors. A natural measure is
the number of contradicting pairs (related to Kendall’s tau
test). We converted all the editorial judgments in pairwise
preferences and did the same from the relevance scores ex-
tracted from our model. On the intersection of both sets of
pairs, there is a disagreement in the preference for 20% of
the pairs. We investigated the reasons for these discrep-
ancies and found that excluding errors in editorial judg-
ments, theses reasons can be summarized into two main
categories:
1: popularity is not necessary aligned with relevance;
2: clicks measure mostly perceived relevance, while editors

judge the relevance of the landing page;

An example for the ﬁrst category is the query “adobe”: the
home page www.adobe.com seems to be the most relevant url
but most users click on the acrobat reader link. Another ex-
ample query is “bank of america”. Most users prefer to click
on the online banking page http://www.bankofamerica.
com/onlinebanking/ while editors tend to consider the home
page http://www.bankofamerica.com as the destination page
for this query. Given this inherent discrepancy between rel-
evance and clicks, we may never be able to close the gap
completely. On the other hand, it may be useful to leverage
the predicted relevance to reﬁne the deﬁnition of relevance
and hence the guideline for editorial relevance judgment.

The second type of inconsistency can be further divided
in two sub-categories: cases where relevance of the search
result snippets are very diﬀerent from that of the landing
page; and cases where users click based on the trustworthi-
ness of the page rather than the relevance of the page. The
ﬁrst sub-category most often is related to the presentation
of the title and summary of the url. An example query for
the second sub-category is “travel insurance”. While there
are many small insurance companies focus on selling travel
insurance (more relevance in terms of relevance judgment),
the users still tend to click more often on the sites of branded
insurance companies, where travel insurance is only a small
fraction of their business.

In summary this study made clear that deﬁning relevance
is a complex matter and that clicks and editorial judgments
are two related but distinct ways of answering the user needs.

7. EXTENSION

So far all the above experiments only considered web search
results. In fact, a wealth of information has been blended
into the search result page: the most noticeable one is spon-
sored search results.In addition, search engines nowadays
tend to contain many links to help users to quickly navigate
to their search destination, including related search, query
mis-spelling suggestion and short cuts.

We discovered that in many cases, lack of clicks on search
results is due to the fact that users have chosen to click
on urls from one or more of the above sections. We thus
would like to model the clicks on the entire search result
page. We modify the DBN model slightly to consider the
whole page clicks. Previously we only considered the top
10 retrieved results in a session. Here we deﬁne two virtual
urls: the leading url deﬁned as the urls at the top of search
result page (e.g. sponsored search, spelling suggestion); the
trailing url deﬁned as the urls at the bottom of the search
result page (e.g. pagination).

WWW 2009 MADRID!Track: Data Mining / Session: Click Models8Clicks on the leading url may suggest that the users never
examined the urls in the search result section; while clicks
on the trailing url indicates the users most likely are unsat-
isﬁed with previous urls. Again, we use the predicted rele-
vance from the reﬁned DBN model to rank urls and compute
NDCG5. The results are summarized in table 2. As for ta-
ble 1, we only keep urls with at least 10 sessions and the
queries with at least 10 urls. The reﬁned DBN model out-
performs the original DBN model by 2.2% and the diﬀerence
is statistically signiﬁcant (p ≤ 0.001).

Table 2: NDCG5 for DBN when considering whole
page relevance. Setting is the same as table 1. Right
column shows the relative diﬀerence with respect to
the improved DBN model.

DBN – 10 nodes
DBN – 12 nodes
DBN – 12 nodes (au only)
Baseline φ

-2.2%

0.748
0.765
0.756
-1.2%
0.795 +3.9%

–

In addition, for this improved DBN model, the satisfaction
variables su seem to be better estimated – there is a 1.2%
drop (p ≤ 0.001) in accuracy if we rank according to au only
(as compared to 0.5% for the original model, see table 1).
This is probably because we now model clicks on the bottom
of search page such as ‘next’: when a user click on the next
button, it is likely that he is not satisﬁed with the last url he
visited. Our original DBN ignored this fact and incorrectly
attributed high satisfaction for this kind of url.

8. CONCLUSION AND FUTURE WORK

Extracting relevance information from click logs is a chal-
lenging but valuable task for web search ranking. In this pa-
per we have proposed a novel click model based on dynamic
bayesian network. The major contribution of the work is
to introduce the notion of satisfaction to separately model
the relevance of the landing page and perceived relevance at
the search result page. We have demonstrated in this paper
that the DBN model outperforms other click models.

There are several extensions which can improve the accu-
racy of our model. Other than the preliminary experiments
we have done to consider urls in the entire search result page
for click modeling, another extension is to incorporate the
time users spent on a page, which is expected to be very
helpful in predicting the user satisfaction. We can also al-
low the user to be satisﬁed even if he does not click (e.g. he
might have fulﬁlled his request just by reading the abstract).
In addition, the satisfaction variable can be continuous in-
stead of binary: for informational queries, the user typically
ﬁnds bits of information on each page and stops when his
overall information need is fulﬁlled. This can be done by
introducing a dependency between the Si variables. Finally,
a more challenging extension is to consider a non-linear ex-
amination model: this would require to model both forward
and backward jumps.

Most of existing click modeling methods are biased by the
search engine used to collect the clicks, and they mostly serve
as ’positive feedback’: if a document was never presented to
the user, then the document would not be clicked. Another
direction to extend the work would be to utilize the query
smoothing to infer relevance for extra documents.

9. ACKNOWLEDGMENTS

The authors would like to thank Ralf Gutsche for valuable
assistance on click data processing. The authors also wish
to thank Georges Dupret, Narayanan Sadagopan and Belle
Tseng for insightful discussions.

10. REFERENCES
[1] E. Agichtein, E. Brill, S. Dumais, and R. Ragno.

Learning user interaction models for predicting web
search result preferences. In Proceedings of the 29th
annual international ACM SIGIR conference on
Research and development in information retrieval
(SIGIR), pages 3–10, 2006.

[2] M. Beal and Z. Ghahraman. Variational bayesian
learning of directed graphical models with hidden
variables. Bayesian Analysis, 1(4):793–832, 2006.

[3] A. Broder. A taxonomy of web search. SIGIR Forum,

36(2):3–10, 2002.

[4] C. Burges, T. Shaked, E. Renshaw, A. Lazier,

M. Deeds, N. Hamilton, and G. Hullender. Learning to
rank using gradient descent. In Proceedings of the
22nd international conference on Machine learning,
pages 89–96, 2005.

[5] Y. Cao, J. Xu, T.-Y. Liu, H. Li, Y. Huang, and H.-W.
Hon. Adapting ranking svm to document retrieval. In
Proceedings of the 29th annual international ACM
SIGIR conference on Research and development in
informat ion retrieval, 2006.

[6] B. Carlin and T. Louis. Bayes and Empirical Bayes
Methods for Data Analysis. Chapman & Hall/CRC,
2000.

[7] B. Carterette and R. Jones. Evaluating search engines

by modeling the relationship between relevance and
clicks. In J. Platt, D. Koller, Y. Singer, and S. Roweis,
editors, Advances in Neural Information Processing
Systems 20, pages 217–224. MIT Press, 2008.

[8] N. Craswell, O. Zoeter, M. Taylor, and B. Ramsey. An
experimental comparison of click position-bias models.
In WSDM ’08: Proceedings of the international
conference on Web search and web data mining, pages
87–94. ACM, 2008.

[9] N. M. Dempster, A. Laird, and D. B. Rubin.

Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society B,
39:185–197, 1977.

[10] G. Dupret and B. Piwowarski. User browsing model to

predict search engine click data from past
observations. In SIGIR 08: Proceedings of the 31st
Annual International Conference on Research and
Development in Information Retrieval, 2008.

[11] Z. Ghahramani. Learning dynamic bayesian network.

In C. L. Giles and M. Gori, editors, Adaptive
processing of temporal information, Lecture notes in
artiﬁcial intelligence. Springer-Verlag, 1998.

[12] K. Jarvelin and J. Kekalainen. Cumulated gain-based

evaluation of IR techniques. ACM Transactions on
Information Systems, 20(4):422–446, 2002.

[13] T. Joachims. Optimizing search engines using

clickthrough data. In ACM SIGKDD Conference on

WWW 2009 MADRID!Track: Data Mining / Session: Click Models9Knowledge Discovery and Data Mining (KDD), pages
133–142, 2002.

[14] T. Joachims. Evaluating retrieval peformance using

clickthrough data. In Text mining, pages 79–96, 2003.

[15] T. Joachims, L. A. Granka, B. Pan, H. Hembrooke,

and G. Gay. Accurately interpreting clickthrough data
as implicit feedback. In Proceedings of the 28th annual
international ACM SIGIR conference on Research and
development in information retrieval, pages 154–161,
2005.

[16] M. Richardson, E. Dominowska, and R. Ragno.

Predicting clicks: estimating the click-through rate for
new ads. In Proceedings of the 16th international
conference on World Wide Web (WWW), pages
521–530, 2007.

[17] M. Richardson, E. Dominowska, and R. Ragno.

Predicting clicks: estimating the click-through rate for
new ads. In WWW ’07: Proceedings of the 16th
international conference on World Wide Web, pages
521–530. ACM, 2007.

[18] S. E. Robertson and S. Walker. Some simple eﬀective

approximations to the 2-poisson model for
probabilistic weighted retrieval. In Proceedings of the
17th annual international ACM SIGIR conference on
Research and development in informat ion retrieval,
1994.

[19] V. Zhang and R. Jones. Comparing click logs and

editorial labels for training query rewriting. In Query
Log Analysis: Social And Technological Challenges. A
workshop at the 16th International World Wide Web
Conference, 2007.

[20] Z. Zheng, H. Zha, T. Zhang, O. Chapelle, K. Chen,

and G. Sun. A general boosting method and its
application to learning ranking functions for web
search. In Advances in Neural Information Processing
Systems 20, pages 1697–1704. MIT Press, 2008.

[21] D. Zhou, L. Bolelli, J. Li, C. L. Giles, and H. Zha.
Learning user clicks in web search. In International
Joint Conference on Artiﬁcial Intelligence (IJCAI07),
2007.

APPENDIX
We give here some details about the inference in our DBN
outlined in section 3.3. Suppose that there are N sessions
and denote Aj, Sj and Ej the vector of hidden variables
associated with the j-th session. Also let dj
i be the url in
position i of the j-th session.

M step
Given some posterior distributions Q(Aj
hidden variables, the update of au and su are as follows:

i ) and Q(Sj

i ) on the

NX

10X

j=1

i=1

I(dj

i = u)

au = arg max

a

“

”

Q(Aj

i = 0) log(1 − a) + Q(Aj

i = 1) log(a)

+ log P (a).

j=1

i=1

NX

10X

j=1

i=1

I(dj

i = u, C j

i = 1)

su = arg max

s

“

Q(Sj

i = 0) log(1 − s) + Q(Sj

i = 1) log(s)

”

+ log P (s).

In the above equations, I is the indicator function; P (a)
and P (s) are the prior beta distributions. We simply took
a beta distribution with parameters (1,1), but these priors
can be learned using a variational approximation [2]. The
maximizers can of course be easily computed in closed form.
Because of the priors, this EM algorithm does not converge
to the maximum likelihood solution but to a mode of the
posterior: it is a maximum a posteriori (MAP) solution.
E step
The M steps consists in computing the posterior probabili-
ties:

Q(Aj

Q(Sj

i ) := P (Aj
i ) := P (Sj

i|C j, au, su, γ)
i |C j, au, su, γ).

In the rest of this section, we drop for convenience the con-
ditioning on au, su and γ. As in the forward-backward algo-
rithm, we deﬁne the following variables:

αi(e) = P (C j

1, . . . , C j

βi(e) = P (C j

i , . . . , C j

i−1, Ei = e),
10|Ei = e)

And one can easily derived the recursion formula:
)P (Ei+1 = e, Ci|Ei = e
(cid:48)

αi+1(e) =

αi(e

(cid:48)

).

X
X

e(cid:48)∈{0,1}

e(cid:48)∈{0,1}

βi−1(e) =

(cid:48)

βi(e

)P (Ei = e

(cid:48)

, Ci−1|Ei−1 = e).

The conditional probabilities in the above equation are

computed as follows:

P (Ei+1, Ci|Ei) =X

P (Ei+1|Si = s, Ei)P (Si = s|Ci)P (Ci|Ei).

s∈{0,1}
Conﬁdence
Remember that the latent variables au and su will later be
used as targets for learning a ranking function. It is thus im-
portant to know the conﬁdence associated with these values.
A standard way of deriving a conﬁdence is to compute the
second derivative of the log likelihood function at the MAP
solution. This can be seen as doing a Laplace approximation
of the posterior distribution.

ple expression because P (C j|au, su, γ) =P

The second derivative in our case turns out to have a sim-
aj ,ej ,sj P (C j|Aj =
aj, Ej = ej)P (Aj = aj, Ej = ej, Sj = sj|au, su, γ) is linear
in au and su. The result is simply the average squared gra-
dient (similar equation stands for su):

∂2
∂a2
u

NX
10X
NX

j=1

log P (C j|au, su, γ) =

„ Q(Aj

i = 1)
au

«2

.

(8)

− Q(Aj

i = 0)

1 − au

I(dj

i = u)

WWW 2009 MADRID!Track: Data Mining / Session: Click Models10