Estimating the ImpressionRank of Web Pages

Ziv Bar-Yossef

Dept. of Electrical Engineering
Technion, Haifa 32000, Israel

and

Google Haifa Engineering Center, Israel

zivby@ee.technion.ac.il

ABSTRACT
The ImpressionRank of a web page (or, more generally, of
a web site) is the number of times users viewed the page
while browsing search results. ImpressionRank captures the
visibility of pages and sites in search engines and is thus an
important measure, which is of interest to web site owners,
competitors, market analysts, and end users.

All previous approaches to estimating the ImpressionRank
of a page rely on privileged access to private data sources,
like the search engine‚Äôs query log. In this paper we present
the Ô¨Årst external algorithm for estimating the Impression-
Rank of a web page. This algorithm relies on access to three
public data sources: the search engine, the query suggestion
service of the search engine, and the web. In addition, the
algorithm is local and uses modest resources. It can therefore
be used by almost any party to estimate the ImpressionRank
of any page on any search engine.

En route to estimating the ImpressionRank of a page, our
algorithm solves a novel variant of the keyword extraction
problem:
it Ô¨Ånds the most popular search keywords that
drive impressions of a page.

Empirical analysis of the algorithm on the Google and Ya-
hoo! search engines indicates that it is accurate and provides
interesting insights about sites and search queries.

Categories and Subject Descriptors: H.3.3: Informa-
tion Search and Retrieval.
General Terms: Measurement, Algorithms.
Keywords: search engines, estimation, ImpressionRank,
popular keyword extraction, suggestions, auto-completions,
data mining.

INTRODUCTION

1.
Background.
In recent years search engines have become
an indispensable tool for information discovery. Billions
of searches are performed by hundreds of millions of users
around the world [2]. Since search engines drive a signiÔ¨Å-
cant fraction of the traÔ¨Éc to web sites (see, e.g., [15]), these
sites have become dependent on their visibility in search en-
gines. For commercially oriented sites, this visibility directly
aÔ¨Äects the number of potential clients and revenues.
‚àóSupported by the Eshkol Fellowship of the Israeli Ministry
of Science.
Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2009, April 20‚Äì24, 2009, Madrid, Spain.
ACM 978-1-60558-487-4/09/04.

Maxim Gurevich‚àó

Dept. of Electrical Engineering
Technion, Haifa 32000, Israel
gmax@tx.technion.ac.il

The visibility of a web site in search engines is therefore an
important metric that web site owners, competitors, market
researchers, and even users are interested in. Some search
engines (like Google Trends for WebSites1) and online mar-
ket research Ô¨Årms (like Alexa2, Nielsen3, and comScore4)
provide traÔ¨Éc estimates for popular web sites. These rely
on private data sources, like search query logs, web server
logs, and network router logs.

In this paper we study external methods for estimating
the visibility of a web page or a web site in a search engine.
These methods rely only on public data sources to produce
their estimates. Such methods are appealing for a number
of reasons. First, they can be used by anyone, not just by
search engines or other major companies. Second, they can
be applied to any web page or web site, not just the ones
for which search engines have decided to disclose visibility
information (namely, the popular sites). Third, they can
be applied to almost all search engines, even ones that cur-
rently do not disclose visibility information. Finally, they
can be used to compare visibility of sites and pages in dif-
ferent search engines.

Problem statement. Visibility of a page or site in a
search engine is formalized through the notion of Impres-
sionRank. We say that a page/site x has an impression on
a query q in a search engine, if the user who sent q to the
search engine viewed x as one of the results. The Impres-
sionRank of x is the total number of impressions x has on
queries in the search engine within a certain time frame.

Due to the power law distribution of query frequencies [11,
16, 1], most of the impressions of pages and sites come from
a small number of queries. For example, in our study we
found that 73% of the impressions of the site www.cnn.com
in Google come from the following 3 queries: ‚Äúcnn‚Äù, ‚Äúelec-
tion results‚Äù, and ‚Äúnews‚Äù. Hence, estimating the Impression-
Rank of a page/site x can be reduced to Ô¨Ånding the number
of impressions x has on these top queries.
In the popular
keyword extraction problem we are given x and would like
to Ô¨Ånd the k queries (a.k.a. keywords) on which x has the
most impressions. This problem can be viewed as a variant
of the classical keyword extraction problem, which has been
studied in Information Retrieval for many years.

Our contributions. Our main contribution is the Ô¨Årst
external algorithm for popular keyword extraction. By the

1http://trends.google.com/websites.
2http://www.alexa.com.
3http://www.nielsen-online.com.
4http://www.comscore.com.

WWW 2009 MADRID!Track: Data Mining / Session: Graph Algorithms41aforementioned reduction, this algorithm can be used also
to estimate ImpressionRank. The algorithm relies on three
public data sources: (a) the search engine; (b) the query
suggestion service of the search engine; (c) the web. Thus,
the algorithm can work with any search engine that provides
query suggestions (all major search engines do).

Our algorithm is local. That is, in order to extract the
popular keywords from a page or site x, the algorithm probes
its data sources only for information that is related to x. The
algorithm does not need to perform a global computation
on the entire search index or query log. Consequently, the
algorithm requires modest resources.
It sends a relatively
small number of requests to the search engine (hundreds)
and to the query suggestion service (tens of thousands) and
fetches a relatively small number pages from the web (hun-
dreds). The algorithm can run on a single PC using a stan-
dard broadband Internet connection and takes up to a few
hours5 to complete its task.

Empirical study we performed with the Google and Ya-
hoo! search engines suggests that the algorithm is accurate:
given a page, it Ô¨Ånds on average 93% of its top keywords in
Google and 84% of its top keywords in Yahoo!.

We used our algorithm to measure the visibility of popular
and less popular sites as well as to Ô¨Ånd which keywords are
the most popular for these sites.

Motivation. We believe that external algorithms for pop-
ular keyword extraction and for ImpressionRank estimation
could be used as primitives in a variety of applications. We
mention below some possible directions.

Popularity rating of pages and sites. Measuring usage
metrics for web sites, and in particular the amount of traÔ¨Éc
they get, is a well-established business, involving Ô¨Årms like
Nielsen, comScore, and Alexa. Web site popularity ratings
are important for marketing and PR as well as for deter-
mining advertising rates. The techniques used by the above
market research Ô¨Årms require access to private information,
like router logs and web server logs.

Assuming that search engine visibility is correlated with
the amount of traÔ¨Éc web sites receive, external algorithms
for ImpressionRank estimation provide a low-cost alterna-
tive for measuring the popularity of sites. In addition, anal-
ysis of the most popular queries on which a site has impres-
sions can be used to derive demographic and geographical
proÔ¨Åles of the users to whom the site is visible.

Site analytics.

In order for web site owners to be able to
enhance the amount of traÔ¨Éc they get from search engines,
they need tools to measure and analyze their visibility in
search engines. There are several products like Google Ana-
lytics6 or OneStat7 that help web masters analyze the traÔ¨Éc
to their site based on web server logs or on scripts embedded
in their pages. These products can show information about
the search engine queries that resulted in actual clicks that
led users to the site. However, they do not provide informa-
tion about impressions of the site that have not resulted in
clicks. Our algorithms can help site owners compare impres-
sions and clicks and consequently derive clickthrough rates
for their site on diÔ¨Äerent queries and in diÔ¨Äerent search en-
gines. From these they can learn about the strengths and
weaknesses of their site.

5Mainly due to rate limiting requests to the search engine.
6http://www.google.com/analytics.
7http://www.onestat.com.

Market research. While all the existing site analytics
products rely on information that is available only to site
owners, our algorithms can be used by anybody. Hence,
they can be used to compare the visibility of diÔ¨Äerent sites
on diÔ¨Äerent queries in diÔ¨Äerent search engines. This could
be useful for site owners to understand their competitive
advantages and disadvantages as well as for market analysts
who wish to study the whole market.

Search engine evaluation.

Search engines try to Ô¨Ålter
out negative content, such as spam, hate sites, porn, and
virus infected pages, from their search results. By estimating
the ImpressionRank of a sample of such negative sites on
diÔ¨Äerent search engines, one can evaluate the eÔ¨Äectiveness
of the mechanisms these search engines apply for Ô¨Åltering
negative content.

Our methodology. Search engines can compute the pop-
ular keywords and the ImpressionRanks for all pages and
sites in their index quite easily using the MapReduce frame-
work [3]. Without direct access to the search engine‚Äôs query
log or search index, this task becomes very challenging.
First, it is not clear how, given a page, we can Ô¨Ånd the
queries on which the search engine returns the page. More-
over, even if we had these queries, it seems impossible to
Ô¨Ånd how many impressions of the page these queries have
generated without accessing the query log. Finally, search
engines pose strict limits on the rate of requests they are
willing to accept from a single user/IP address.

Our algorithm overcomes these challenges as follows. In
order to Ô¨Ånd queries on which the search engine returns the
page, the algorithm resorts to standard IR techniques, like
TF-IDF scoring and term proximity analysis, to extract can-
didate keywords from the page‚Äôs content, from anchor text,
and from similar pages. The algorithm determines which of
these candidates generate impressions for the page by check-
ing whether the search engine returns the page on them. In
order to calculate the amount of impressions generated, the
algorithm estimates the frequency of each candidate in the
search engine‚Äôs query log. To this end, the algorithm re-
sorts to a technique developed in our previous work [1] for
estimating query frequencies using the search engine‚Äôs query
suggestion service.

Implementing the above work plan naively is not feasible,
because the algorithm may need to sift through numerous
possible candidate queries. Evaluating all of them would
have required sending a prohibitive number of requests to
the search engine and to its suggestion service. To cope
with the large amount of candidates, the algorithm applies
a best-Ô¨Årst search methodology [10] to identify the most
promising candidates eÔ¨Éciently. A crucial ingredient of the
algorithm is a low-cost procedure for estimating the frequen-
cies of many candidates in bulk using only a small number
of requests to the suggestion service.

For lack of space, some details are omitted from the main
body of the paper. The interested reader will be able to Ô¨Ånd
most of them in the full draft of the paper.8

2. SUGGESTION SERVICES

In this section we overview query suggestion services and

how they can be used to estimate query frequencies.

Suggestion services. Query suggestion services such as

8Available at http://www.ee.technion.ac.il/people/zivby.

WWW 2009 MADRID!Track: Data Mining / Session: Graph Algorithms42Google Suggest, Yahoo! Search Assist, Windows Live Tool-
bar, Ask Suggest, and Answers.com9 are increasingly popu-
lar tools that assist users in choosing search queries. While
the user is typing her query, the suggestion service oÔ¨Äers
the user auto-completions of the string she has already en-
tered. These auto-completions help users save typing time
and also guide them to better query formulations. Most
suggestion services leverage ‚Äúthe wisdom of crowds‚Äù to gen-
erate the auto-completions (or, suggestions). Given a string
Œ± entered by the user, the suggestion service extracts from
its query log the most popular queries of whom Œ± is a preÔ¨Åx
and returns them to the user, ordered by popularity.

Frequencies vs. volumes. The frequency of a query q,
denoted freq(q), is the number of instances q has in the query
log. The volume of a string Œ±, denoted vol(Œ±), is the number
of distinct queries in the log of whom Œ± is a preÔ¨Åx.

The shortest exposing preÔ¨Åx of a query q is the shortest
preÔ¨Åx Œ± of q for which the suggestion service returns q as
one of the suggestions for Œ±.
In a previous work [1], we
studied the correlation between the frequency of queries and
the volumes of their shortest exposing preÔ¨Åxes. We found
that both measures have power law distributions, but with
slightly diÔ¨Äerent exponents. Moreover, we discovered that
the two measures are ‚Äúorder-correlated‚Äù. That is, if we order
queries once by their frequencies and once by the volume of
their shortest exposing preÔ¨Åxes, then the two orders have a
high Kendall tau correlation.

As estimating string volumes is more feasible than esti-
mating query frequencies directly, we rely on the above cor-
relation in this paper too. We simply measure the popularity
of queries by the volumes of their shortest exposing preÔ¨Åxes.

String volume estimation.
In our previous work [1],
we showed two techniques for estimating string volumes us-
ing only external access to the suggestion service. The Ô¨Årst
technique is an importance sampling procedure that pro-
duces relatively accurate volume estimates, but requires up
to thousands of requests to the suggestion service to produce
each estimate. The second technique is a heuristic that uses
only a few dozens of requests to generate a rough volume
approximation.

remark. Some suggestion services work diÔ¨Äerently from
what we described above. They may generate suggestions
from document titles or contents and not from query logs,
they may rank suggestions not by their popularity, or they
may oÔ¨Äer suggestions that are not strict completions of the
string entered by the user (e.g., spelling corrections).

Our algorithm‚Äôs frequency estimates rely on the ranking
provided by the suggestion service.
If this ranking is not
by query frequency but by some other measure (e.g., pop-
ularity in some document corpus), then our algorithm will
reÔ¨Çect the same notion of popularity as well. As for sugges-
tions that are not strict string completions, our algorithm
simply ignores them and uses only the completion-type sug-
gestions.

3. PROBLEM STATEMENT

Keywords and query logs. Throughout, we Ô¨Åx some
search engine in which we want to measure visibility. A
keyword is a sequence of one or more terms. A query q is an

9www.google.com, search.yahoo.com, toolbar.live.com, www.
ask.com, www.answers.com.

event in which a user probes the search engine for a certain
keyword w in hope to get matching relevant documents. We
say in this case that q is a query instance of w.

Fix a ‚Äúquery log‚Äù, consisting of queries sent to the search
engine in a certain time frame, in a certain language, and/or
in a certain geographical region. For each keyword w, let
freq(w) be the frequency of w in the log, i.e., the number of
query instances of w in the log.

Impressions. We say that a document x has an impression
on a query q, if the user who sent q to the search engine
viewed x as one of the results. This means: (a) that the
search engine returned x as one of the results of q; and (b)
that the user actually looked at the result corresponding to
x (but not necessarily clicked it).

Previous studies (e.g., Joachims et al. [8]) studied the cor-
relation between impressions/clicks and the position of the
document in the result set. In this paper, for simplicity, we
assume a naive impression model: every document in the
results of a query has an impression. This assumption is not
fundamental to our study. One can modify our algorithm
by plugging in a more realistic impression model.

For a document x and a keyword w, let impressions(x, w)
be the impression contribution of w to x, that is the number
of impressions x has on query instances of w in the query
log. A simplifying assumption we make in this paper is that
the search engine‚Äôs index stays static throughout the gener-
ation of the query log and the time we perform our measure-
ments. This assumption implies a simple characterization of
impressions(x, w):

impressions(x, w) = incidence(x, w) ¬∑ freq(w),

where incidence(x, w) is a Boolean predicate specifying whether
the search engine returns x as one of the results of w at mea-
surement time. The assumption obviously does not hold in
reality, because search engines continuously update their in-
dices. Hence, our measurements can be viewed only as a
static approximation of real impressions on search engines.
See more discussion about this in the conclusions section.

We model impressions as a bipartite graph G, which we
call the impression graph. One side of this graph consists of
all the documents indexed by the search engine and the other
side consists of all the keywords that have query instances
in the log. A document x is connected to a keyword w by
an edge if and only if impressions(x, w) > 0.

Popular keyword extraction.
In the classical keyword
extraction problem, one is given a document x and would
like to Ô¨Ånd keywords that best ‚Äúsummarize‚Äù x. We leverage
the indexing mechanisms of search engines, and rephrase
this problem as follows: given a document x, Ô¨Ånd all the
keywords w on which a search engine returns x. Under our
deÔ¨Ånition, keyword extraction is search engine-dependent,
which we view as a positive side-eÔ¨Äect, as it allows compar-
ing the diÔ¨Äerent indexing approaches taken by search en-
gines.

In the classical keyword extraction problem, the quality
of the matching between a keyword and a document de-
pends only on intrinsic properties of the keyword and the
document. In this paper, we look also at the popularity of
keywords among users, in order to determine their quality.
Hence, our focus is on the following popular keyword extrac-
tion problem:

WWW 2009 MADRID!Track: Data Mining / Session: Graph Algorithms43The Popular Keyword Extraction Problem

Input: A document x and an integer k ‚â• 1.
Output: The k keywords on which x had the most
impressions in the query log.

Using the impression graph modeling, what we would like
to Ô¨Ånd is the k neighbors of x of highest frequency in the
log.

An optional requirement from algorithms that extract pop-
ular keywords is that they extract a diverse list of key-
words. In particular, we would prefer not to get many key-
words all of which are simple variations of the same key-
word (e.g., ‚Äúbritney spears‚Äù, ‚Äúbritney spears songs‚Äù, ‚Äúbritney
spears lyrics‚Äù, etc.), even if, strictly speaking, these are the
most popular keywords incident to the document x. We
use a simple notion of diversity in this work: the algorithm
should return a preÔ¨Åx-free list of keywords; that is, no key-
word can be a preÔ¨Åx of any other keyword.

We focus on external algorithms for popular keyword ex-
traction. That is, the algorithms are allowed to use only
public data sources, and cannot rely on privileged access to
internal search engine data, like its query logs, its index, its
document cache, etc. The algorithms can send requests to
the search engine and to its suggestion service and they can
fetch pages from the web on their own.

The main cost measures for external algorithms are the
number of search requests and the number of suggestion re-
quests they send per input instance. Secondary cost metrics
are the number of pages the algorithms fetch, their run-time,
and their local storage requirements.

Impressions induce an order on docu-
ImpressionRank.
ments, based on the number of impressions they had in the
query log.
ImpressionRank (or, irank, for short) captures
this:

irank(x) = X

impressions(x, w) = X

freq(w),

w

w‚ààN (x)

where N (x) denotes the keywords incident to x in the im-
pression graph. This deÔ¨Ånition can be trivially generalized
to measure the number of impressions of a set of pages (e.g.,
a web site): irank(X) = Px‚ààX irank(x).

Keyword frequencies in query logs are known to follow a
power law distribution [11, 16, 1]. Due to the scale-free na-
ture of power laws, it is reasonable to assume that also the
frequencies of keywords generating impressions for a spe-
ciÔ¨Åc document follow a power law (the exponent may diÔ¨Äer,
though, from document to document). We use this obser-
vation to reduce the problem of estimating ImpressionRank
to the popular keyword extraction problem.

If the frequencies of keywords incident to a document x
perfectly follow a power law, we can use the frequencies of
the top k keywords to infer the exponent of the power law
through linear regression at the log-log plot. The exponent
and the frequency of the most popular keyword can then
be used to estimate the total frequency of the distribution‚Äôs
‚Äútail‚Äù (the keywords beyond the top k). Summing up the
total frequency of the tail with the frequencies of the top k
keywords gives us an estimate of the ImpressionRank.

4. POPULAR KEYWORD EXTRACTION

In this section we describe our external algorithm for pop-
ular keyword extraction. The algorithm works with any

search engine that has a query suggestion service in which
ranking of suggestions is based on frequency in a query log.
The algorithm uses the suggestion service to infer keyword
frequencies, and thus the algorithm computes impressions
relative to the query log that underlies the suggestion ser-
vice.
Overview.
Recall that our goal is to Ô¨Ånd the k most
frequent keywords among the neighbors of a given target
document x. However, we face two challenges: Ô¨Årst, we do
not know a priori who are the neighbors of x; second, even if
we knew these neighbors, we do not know their frequencies.
Our algorithm generates a comprehensive list of candidate
keywords, which is likely to cover most, if not all, the docu-
ment‚Äôs neighbors. The candidate keywords are combinations
of terms from the document‚Äôs text as well as from related
text sources, like anchor text and similar documents. Ide-
ally, the next steps of the algorithm would have been to Ô¨Ånd
which of these candidate keywords are actual neighbors of
the target document and then estimate their frequencies us-
ing the suggestion-based volume estimation algorithm (see
Section 2). The algorithm could have then output the k
most frequent neighbors. This naive approach, however, is
not feasible, as the number of candidate keywords is typi-
cally very large: the document and the related text sources
may consist of thousands of distinct terms and the number
of their combinations is exponentially larger. Evaluating all
these candidates requires sending numerous requests to the
search engine and to its suggestion service, much beyond the
limits posed by search engines.

Our algorithm therefore applies best-Ô¨Årst search [10], in
order to quickly track down the most promising candidates,
evaluate them, and report the top keywords found. To this
end, the algorithm starts with a seed set of candidates, con-
sisting only of single terms, and assigns to each candidate
a score. The score of a candidate is a low-cost predictor
of its chances to be a preÔ¨Åx of a high frequency neighbor
of the target document. At each iteration, the algorithm
picks the candidate of highest score and determines: (a)
whether the candidate itself is a high frequency neighbor of
the target document; and (b) whether the candidate should
be further expanded into additional candidates. To this end,
the algorithm estimates the frequency of the candidate us-
ing the suggestion-based volume estimator. The algorithm
stops whenever no candidates remain (this usually never
happens) or when the search engine request budget has been
exhausted.
Candidate keywords. To generate its candidate key-
words, the algorithm collects a seed text. The seed text
consists of the target document‚Äôs text body, title, url, meta
keywords, and meta description, of the anchor text of hy-
perlinks pointing to the target document, of keywords that
have already been found to be incident to the target doc-
ument, and of the contents of documents that the search
engine returns alongside the target document on these key-
words. Every time the algorithm Ô¨Ånds a new keyword that
is incident to the target document, it expands the seed text
with the terms of this keyword as well as with the contents
of the other documents that are incident to this keyword.

All the distinct terms that are found in the seed text
comprise the term pool. The candidate keywords are all
the possible Ô¨Ånite-length sequences of terms from the term
pool. Note that the order of terms in a keyword matters:
‚Äúbrad pitt‚Äù and ‚Äúpitt brad‚Äù are diÔ¨Äerent keywords. Clearly,

WWW 2009 MADRID!Track: Data Mining / Session: Graph Algorithms44the number of candidate keywords is inÔ¨Ånite. At any given
point of its execution, however, the algorithm considers only
a Ô¨Ånite subset of the candidate keywords.

Candidate keywords can be thought of as nodes in a TRIE,
whose alphabet is all the terms in the term pool. Thus, the
descendants of a given keyword are all the possible exten-
sions of the keyword by terms from the pool. We call this
TRIE the candidate tree.
4.1 Main Ô¨Çow

The algorithm, whose main Ô¨Çow is given in Figure 1, per-
forms best-Ô¨Årst search on the candidate tree, in order to
identify the neighbors of highest frequency.

ExtractPopularKeywords(x,k)
1: crawl the seed text
2: add the terms in the seed text to the term pool
3: score all the terms in the pool
4: insert the terms into the candidate heap
5: while candidate heap 6= ‚àÖ and budget not reached do
6:
7:
8:
9:
10:
11:
12:
13:
14:

w := top candidate from the candidate heap
remove w from the candidate heap
send w to the suggestion service
Sw := w ‚à™ {top suggestions for w}
for all u ‚àà Sw do

estimate freq(u)
add u to the top keywords heap, if possible
expand the seed text with u and the documents
that are incident to u
add all the new terms to the term pool
rescore all terms in the pool
clear the candidate heap
insert all terms into the candidate heap

if u is incident to x then

15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25: end while
26: return the keywords in the top keyword heap

end if
end for
if should expand w then
score the children of w
add the children of w to the candidate heap

end if

Figure 1: The main Ô¨Çow of the algorithm for extract-
ing the k most popular keywords from a document.

Before we start describing the algorithm, we note that an
important ingredient of the algorithm is a cache. The algo-
rithm caches all the requests it sends to the search engine
and to its suggestion service, all the responses it receives
from them, and all the web pages that it fetches. Therefore,
whenever the algorithm recalculates quantities it has calcu-
lated before (e.g., candidate scores), the recalculation can
be done very quickly as no requests or fetches are needed.

The algorithm maintains a candidate heap, which consists
of the ‚Äúfrontier‚Äù of the search space. Candidates are inserted
into the heap based on their scores, which predict the poten-
tial of these candidates to lead to good keywords. Scoring
is described in Section 4.2.

In addition, the algorithm holds a top keywords heap. This
is a bounded-size heap of size at most k that consists of the
currently known highest frequency keywords that are inci-
dent to the target document. The keywords are organized in

the heap based on their estimated frequencies. The top key-
words heap and the candidate heap are mutually exclusive:
a keyword can reside only in one of them at a time.

Initially, the candidate heap is initialized with all the
terms from the term pool (lines 1‚Äì4). At each iteration,
the algorithm picks the candidate whose score is highest,
removes it from the candidate heap, and gets the top sug-
gested keywords for this candidate from the suggestion ser-
vice (lines 6‚Äì9).

If the candidate or any of its suggestions are incident to
the target document, the algorithm estimates their frequen-
cies using the expensive volume estimator (see Section 2)
and tries to add them to the top keywords heap (lines 9‚Äì
10). A keyword is added to the heap, if the heap contains
less than k keywords or if the keyword‚Äôs frequency is higher
than that of the bottom keyword in the heap.

A newly found incident keyword also opens up an oppor-
tunity for Ô¨Ånding more keywords that are incident to the
target document. The algorithm therefore augments (line
14) the seed text with the terms of the keyword as well as
with the contents of all the documents that are incident to
the keyword. The new terms are added to the term pool
and all the terms are rescored (lines 15‚Äì16). This step is
necessary, because the additional text may change scores of
existing terms signiÔ¨Åcantly. The term rescoring also aÔ¨Äects
candidate scores, and thus rebuilding the candidate heap
is necessary too (lines 17‚Äì18). This step essentially resets
the search process. Note that due to the cache used by the
algorithm, the work done earlier is not lost.

Next, the algorithm determines whether the candidate
should be expanded or pruned. In the former case, the algo-
rithm scores the candidate‚Äôs children and adds them to the
candidate heap (lines 22‚Äì23).

The algorithm stops when there are no more candidates
in the candidate heap or when the search requests budget
or the suggestion requests budget have been exhausted (see
more below on how these budgets are determined).

Deciding whether to expand a candidate. The pro-
cedure that decides whether to expand a candidate w Ô¨Årst
checks whether w itself, or any of the top suggestions for
w, is a neighbor of x. If it is and if we require diversity in
the keywords found by the algorithm (see Section 3), there
is no point in further expanding w because even if we Ô¨Ånd
matching keywords among its descendants, they will be sup-
pressed.

Next, the procedure tests whether any of the descendants
of w has a chance to be a neighbor x. If none of them does,
there is no point in further expanding w and it is pruned.
See below how the incidence test is carried out.

The procedure then determines whether any of the descen-
dants has positive frequency. If none of them does, again
there is no point in expanding w. This test is easy to do: if
the suggestion service returns no suggestions for w, we know
none of its descendants has positive frequency.

At this point, the procedure tries to assess whether any
descendant of w has a chance to make it to the top key-
words heap. If the top keywords heap hasn‚Äôt reached it size
limit yet, then any positive frequency descendant of w has
a chance to make it to the top k, and thus the procedure
decides to expand w. If the heap has reached its size limit,
the procedure compares the bottom keyword on the heap
with the top descendant of w (which is the top suggestion
returned for w by the suggestion service). If the estimated

WWW 2009 MADRID!Track: Data Mining / Session: Graph Algorithms45frequency of the top descendant is lower than the estimated
frequency of the bottom keyword on the heap, we are guar-
anteed that none of the descendants of w can make it to the
top k. It is therefore safe in this case to prune w. Otherwise,
w is expanded.
Testing incidence. Testing whether a given candidate
keyword w is incident to the target document x is very easy:
we simply send w as a query to the search engine and check
whether x is returned as one of the top N results, where N
is a tunable parameter (N = 10 is a typical choice).

Testing whether no descendant of w is incident to x is a bit
trickier, because it is infeasible to send all the descendants of
w to the search engine. To this end, we resort to the ‚Äúinurl:‚Äù
search option, which enables a user to restrict her search to
documents whose url starts with a given string. We send
the query ‚Äúinurl:url(x) w‚Äù to the search engine. If there are
no results returned, we conclude that neither w nor any of
its descendants are incident to x. If results are returned, we
cannot conclude one or the other.
Cost analysis. A simple analysis shows that the algo-
rithm requires at most N + 2 search requests per iteration,
where N is the maximum number of suggestions returned by
the suggestion service. The number of suggestion requests is
few dozens if the expensive frequency estimation is not per-
formed during the iteration, and may reach few thousands if
it is performed. We found that on average, a few hundreds
of suggestion requests are needed for each iteration.
Setting the requests budgets.
Budgeting search re-
quests is easy, because we know each iteration takes at most
N + 2 search requests. The budget can then be set accord-
ing to the number of candidates we want the algorithm to
go over. We found that usually a few dozens of iterations
are suÔ¨Écient (in extreme cases, we needed a few hundreds).
Budgeting suggestion requests is more diÔ¨Écult. Sugges-
tions are used by two processes: (1) during candidate pro-
cessing; (2) during frequency estimation. The former con-
sumes a bounded number of suggestion requests, while the
latter may need a variable number of requests, depending
on the popularity of the keyword whose frequency is being
estimated. We thus set separate budgets for candidate pro-
cessing and for frequency estimation. A budget of a few tens
of thousands of requests for candidate processing and of a
few hundreds of thousands of requests for frequency estima-
tion is suÔ¨Écient in most cases. Note that the suggestion
requests are more lightweight than regular search requests
and are thus subject to much higher rate limits.
4.2 Candidate scoring

The most critical part of the popular keyword extraction
algorithm is the scoring of candidate keywords. Good scor-
ing will enable the algorithm to zero in on the most promis-
ing keywords quickly. Weak scoring will cause the algorithm
to get lost in the large search space.

While the quality of the scoring function is very impor-
tant, its eÔ¨Éciency (i.e., how many requests to the search
engine/suggestion service it requires) is critical too. Since
we need to calculate scores for thousands of candidates the
feasibility of the whole algorithm depends on the cost of the
scoring function. The scoring procedure we present below is
eÔ¨Äective at Ô¨Ånding good candidates early and at the same
time uses manageable resources.

The scoring function needs to promote candidate key-
words that are more likely to be both neighbors of the target

document and of high frequency. Recall that the impression
contribution of a keyword w to the document x is deÔ¨Åned
as impressions(x, w) = freq(w) ¬∑ incidence(x, w). The scor-
ing function we introduce tries to produce (unnormalized)
estimates of impression contributions. An ordering of can-
didates by their scores then approximates their ordering by
impression contributions.

The score of each candidate is the product of two scores:
a frequency score and an incidence score. The former tries
to estimate the term freq(w) and the latter tries to estimate
the term incidence(x, w).

Frequency scores are computed using the suggestion ser-
vice. To this end, we developed a new low-cost procedure
for producing rough frequency estimates for many keywords
at bulk.

The incidence score of each candidate is the product of a
TF-score (term frequency score) with an IDF-score (inverse-
document frequency score). Here, we build on the com-
mon IR wisdom that terms of high TF-IDF values are more
‚Äúuniquely representative‚Äùof the document. Hence, the search
engine is more likely to match the document to such terms
when they (or combinations thereof) are being sent as queries.
To summarize the score of each candidate keyword w is

deÔ¨Åned as the product of three terms:

score(w) = fscore(w)Œ± ¬∑ tfscore(w)Œ≤ ¬∑ idfscore(w)Œ≥,

where Œ±, Œ≤, Œ≥ ‚â• 0 are weights that are used to properly bal-
ance among the three scores (we assume here 00 = 1, so if
some weight is 0, the corresponding score is ignored). In our
experiments we empirically optimized the setting of these
weights; see Section 5.

We describe below how frequency scores are calculated.
Since the calculation of the TF-scores and IDF-scores is
somewhat routine and due to lack of space we omit its de-
scription here. More details can be found in the full draft of
the paper.
Overview of frequency scoring. The frequency scorer
is given as input a keyword w and it outputs frequency es-
timates for all the children of w in the candidate tree. The
need to score all children of a candidate comes up in two
situations: (1) when the algorithm initializes the candidate
heap it scores all the single terms, which can be viewed as
the children of the root (= the empty string); (2) when the
algorithm expands a keyword it scores all its children.

Note that the number of children of w is the same as
the number of terms in the term pool. Hence, the scorer
needs to produce frequency estimates for up to thousands
of keywords. Running the expensive volume estimator (see
Section 2) requires a few thousands of suggest requests per
frequency estimation. Doing this for all the thousands of
candidates is infeasible. The procedure we introduce below
applies the cheap volume estimator to a small number of the
candidates and derives frequency estimates for all the candi-
dates. The produced frequency estimates are rough, but are
suÔ¨Écient to diÔ¨Äerentiate between high frequency candidates
and low frequency candidates.
The keyword TRIE. Let S be the set of keywords for
which the frequency scorer needs to compute frequency scores.
S consists of all the children of w, and all of these keywords
have the same preÔ¨Åx (= w). The frequency scorer starts by
constructing a character-level keyword TRIE T for S.

Recall that in order to estimate the frequency of a keyword
u, the frequency estimator needs to estimate the volume of

WWW 2009 MADRID!Track: Data Mining / Session: Graph Algorithms46the shortest exposing preÔ¨Åx of u. In other words, it needs
to Ô¨Ånd the top-most ancestor of u in T for which u is one of
the top suggestions.

Cheap volume estimations. As mentioned above, the
frequency scorer uses the cheap volume estimator to produce
volume estimates. This estimator has the useful property
that given a node r ‚àà T , it estimates as a byproduct not
only the volume of r but also the volumes of all the children
of r.

The frequency scorer is given a priori a budget M on the
number of invocations of the cheap volume estimator. Thus,
the frequency scorer can estimate the volumes of at most M
nodes of T and their children. Using this information the
scorer needs to somehow derive frequency estimates for all
the keywords in S.

We now need to address two questions: (a) how to se-
lect the nodes for volume estimation? (b) how to derive
frequency estimates from these volume estimates?

Selecting nodes for volume estimation. When the
scorer selects nodes for volume estimation, it tries to max-
imize the number of keywords for which it will be able to
later derive frequency estimates. If the scorer estimates the
volume of a node r ‚àà T , then it will be able to derive fre-
quency estimates for all the keywords that are descendants
of r. Hence, the guideline in choosing the nodes for vol-
ume estimation is to prefer ones that have many keyword
descendants.

The scorer calculates for each node r in the TRIE the num-
ber of keyword descendants, d(r), it has. The scorer employs
a max-heap, to which TRIE nodes will be inserted based
on their d(¬∑) values. The heap is initialized with the root
of the keyword TRIE (corresponding to the empty string).
The scorer iteratively estimates volumes of nodes from the
TRIE until the heap is empty or the budget for the number
of volume estimations is exhausted.

The scorer picks the top node r from the heap. This is the
node that has the most keyword descendants among all the
nodes in the TRIE whose child volumes have not yet been
estimated. The scorer estimates the volumes of r and of its
children by invoking the cheap volume estimator.

The scorer next considers the produced volume estimates
for the children of r. If the volume for a child r‚Ä≤ is 0 or 1, it
means that the suggestion service returns no suggestions for
r‚Ä≤ (except, maybe, for r‚Ä≤ itself). In particular, this means
that all the keyword descendants of r‚Ä≤ in the keyword TRIE
have 0 frequency in the query log. The scorer therefore
prunes the TRIE at r‚Ä≤; The frequency scores of the keyword
descendants of r‚Ä≤ are set 0.

If the volume of r‚Ä≤ is greater than 1, then it is still possible
that one or more of its keyword descendants has positive
frequency in the log and thus r‚Ä≤ is added to the heap.

A useful property of the above selection strategy is that if
a node u is selected for volume estimation, it is guaranteed
that that also all the ancestors of u have been selected. This
property will be used below in the frequency estimations.

Frequency estimation. Next, we address the question of
how to derive frequency estimates from the volume estimates
produced. Let u be some keyword in S. There are two
possible cases to consider.

The easy case is that u has some ancestor r in T that sat-
isÔ¨Åes the following: (a) the scorer estimated the volume of r;
(b) u is one of the top suggestions for r. In this case, let r‚Ä≤

be the top-most ancestor of u, for which u is one of the top
suggestions. Note that r‚Ä≤ = r or r is an ancestor of r and
thus the scorer has a volume estimate for r‚Ä≤ too. The scorer
now derives a frequency estimate for u from the volume es-
timate of r‚Ä≤ and from the position of u in the suggestions for
r‚Ä≤.

The harder case is that u has no known ancestor for which
it appears as one of the top suggestions. Let r be the lower-
most ancestor of u for which the scorer estimated volume
(at least one such ancestor always exists, because the scorer
estimates the volume of the root). Suppose that one or-
ders all the queries in the log of whom r is a preÔ¨Åx by their
frequencies. The top N of these are returned by the sug-
gestion service as suggestions for r. Let us call the rest the
suggestion tail of r. Since u does not show up at the top sug-
gestions, either it does not show up in this list at all (i.e.,
it does not show up in the query log) or it belongs to the
suggestion tail of r. Since the scorer cannot tell apart the
two cases, it will always assume the latter.

The scorer now estimates the frequency of u as follows.
First, the scorer estimates the total frequency mass of the
suggestion tail of r (i.e., the sum of the frequencies of all the
suggestions in the tail). This can be done using the assumed
power law distribution of query frequencies and using the
volume estimate for r. See more details in our previous
paper [1]. Let us denote this quantity F . Since the scorer
has no information about the position of u within this tail, it
simply assumes that u‚Äôs frequency is the average frequency
vol(r)‚àíN , where N is the number of top
in the tail, which is
suggestions for r.

F

In some instances the above frequency estimation leads to
anomalies. If the number of descendants of r in the keyword
TRIE that do not appear as top suggestions for r is bigger
than vol(r) ‚àí N , the total frequency mass they will receive
by the above estimate will be higher than F . In order to
avoid such anomalies, the scorer calculates the number n of
keyword descendants of r that are not top suggestions for r.
It then assigns the frequency score of u as follows:

fscore(u) = min{

F

vol(r) ‚àí N

,

F
n

}.

5. EXPERIMENTAL RESULTS

Quality metrics. We Ô¨Årst describe the quality metrics
we used for evaluating popular keyword extraction. Given a
document x, let N (x) be the set of all keywords that are in-
cident to x. Given a popular keyword extraction algorithm
A, let A(x) be the set of keywords that the algorithm returns
on x. Achieving a precision of 1 (i.e., A(x) ‚äÜ N (x)) is triv-
ial. Hence, our main quality metric is recall. We deÔ¨Åne two
variants of recall: (a) recallU (A, x) = |A(x)|/|N (x)|, which
measures the success the algorithm in extracting keywords
with no regard to their popularity; and (b) recallF (A, x) =
Pw‚ààA(x) freq(w)/ Pw‚ààN (x) freq(w), which measures the suc-
cess of the algorithm in extracting popular keywords.

The above two metrics measure the quality of the al-
gorithm on a speciÔ¨Åc document x. We deÔ¨Åne two corre-
sponding metrics that aggregate the quality of the algorithm
on all documents.
recallU (A) = E(recallU (A, X)), where
X is a random document chosen proportionally to its de-
gree in the impression graph, i.e., Pr(X = x) ‚àù |N (x)|.
recallF (A) = E(recallF (A, X)), where X is chosen propor-
tionally to ImpressionRank, i.e., Pr(X = x) ‚àù irank(x).

WWW 2009 MADRID!Track: Data Mining / Session: Graph Algorithms47The choice of these speciÔ¨Åc document distributions main-
tains the principle that in the former metric all keywords are
treated equally, while in the latter keywords are weighted by
their frequencies.

We use the following procedure to estimate the recall met-
rics: (1) Sample a random keyword w from the query log
either uniformly (for estimating recallU ) or proportionally
to its frequency (for estimating recallF ), using the sampling
algorithms from [1]. (2) Send w to the search engine. (3)
Select a uniformly chosen document x from the results of w.
(4) Compute A(x). (5) Return 1 if w belongs to A(x) and 0
otherwise. In the full draft of the paper we prove that this
procedure indeed estimates recallU (A) and recallF (A).

Note that the above recall measures are quite pessimistic,
as they measures recall relative to the entire list of key-
words that are incident to the target document. One could
deÔ¨Åne also more reÔ¨Åned measures, like recall@k, which con-
sider coverage of only the top keywords. These measures,
however, are harder to compute. We therefore focus in this
paper on the above recall measures and keep in mind that
they are lower bounds on the realistic recall.
Recall analysis.
In order to evaluate the eÔ¨Äectiveness of
our algorithm, we measured the above two recall measures
when running the algorithm on Google and on Yahoo!. We
report the results in Table 1 for diÔ¨Äerent settings of the pa-
rameters Œ±,Œ≤, and Œ≥, which determine the relative weight of
the frequency score, TF score, and IDF score in our algo-
rithm. The Ô¨Årst row in this table corresponds to the best
conÔ¨Åguration of the algorithm we found (we reached this
conÔ¨Åguration using a hill-climbing search on the space of
possible values for Œ±, Œ≤, Œ≥ with the average of the estimated
recallF values on Google and Yahoo! as the objective func-
tion). The results indicate that the algorithm achieves high
coverage on both Google and Yahoo!
(93% and 84%, re-
spectively, for the recallF measure). The algorithm is more
successful at extracting popular keywords than unpopular
keywords, as indicated by the diÔ¨Äerence between the mea-
sured recallF and recallU values.

The three other rows of the table demonstrate the signiÔ¨Å-
cance of each of the three scores we use in the algorithm. To
this end, we eliminated each of the scores, by setting their
corresponding weights to 0, and measured the resulting re-
call values. The results demonstrate that the TF score is the
most crucial for achieving high recall, especially on Google.
Without this score, recallF on Google drops down from 93%
to only 2%. The two other scores have marginal eÔ¨Äect on
recallF but have more signiÔ¨Åcant eÔ¨Äect on recallU .

Œ±, Œ≤, Œ≥

0.2, 1, 0.6
0, 1, 0.6
0.2, 0, 0.6
0.2, 1, 0

recallF ,
Google

0.93 ¬± 0.08
0.91 ¬± 0.09
0.02 ¬± 0.01
0.92 ¬± 0.08

recallF ,
Yahoo!

0.84 ¬± 0.14
0.80 ¬± 0.16
0.07 ¬± 0.08
0.82 ¬± 0.14

recallU ,
Google

0.62 ¬± 0.06
0.52 ¬± 0.06
0.24 ¬± 0.05
0.50 ¬± 0.06

recallU ,
Yahoo!

0.37 ¬± 0.05
0.27 ¬± 0.04
0.17 ¬± 0.04
0.17 ¬± 0.04

Table 1: Estimated recall values (together with mea-
sured standard deviations) for the popular keyword
extraction algorithm.

A more reÔ¨Åned breakdown of the recallF analysis is shown
in Figure 2. Here we split the random keywords selected by
the recall estimator into 5 equally sized buckets, based on
their frequency. We then measured the fraction of keywords
in each bucket that the algorithm managed to detect. The

results indicate that our algorithm performs much better on
more popular keywords. At the top bucket, for example, it
achieved a recall of almost 100% on both Google and Yahoo!.
The recall deteriorates as the frequency of keywords goes
down. This is to be expected, since our algorithm indeed
tries to Ô¨Ånd the most popular keywords. We note that in
26% of the cases the algorithm did not Ô¨Ånd some keyword on
Google and in 60% of the cases it did not Ô¨Ånd some keyword
on Yahoo!, the algorithm did Ô¨Ånd at least one other more
popular keyword.



         	 


!""#$%
&'("")




 



Figure 2: Recall as a function of keyword frequency.

The lower recall values measured for Yahoo! are due to
the fact that the Yahoo! suggestion service seems to expose
a larger set of low-frequency queries than Google. Hence,
when choosing a random keyword proportionally to frequency,
the chances to pick a low-frequency keyword in Yahoo! are
higher than the chances to pick a low-frequency keyword in
Google.

Cost analysis. To evaluate the eÔ¨Éciency of the algorithm,
we measured how quickly it Ô¨Ånds the keywords that it even-
tually outputs. We measure time in terms of the fraction of
the request budget used. Progress is measured in terms of
the fraction of the eventual keywords found, where keywords
are weighted by their frequency.

The results, depicted in Figure 3, show that the algorithm
Ô¨Ånds most of the keywords very early on. On Google, the
algorithm manages to Ô¨Ånd about 80% of the keywords after
consuming as little as 20% of the budget. On Yahoo! the
progress curve is a bit more moderate, again due to the
abundance of low-frequency keywords in the log.

We also measured the eÔ¨Äect of the frequency score, the TF
score, and the IDF score on the costs of our algorithm. We
found that when the frequency score was eliminated, the
amortized number of search engine requests per keyword
found grew by 64% on Google and 42% on Yahoo!. The
amortized number of suggestion requests grew by 45% on
Google, but did not change on Yahoo!. The TF score and
the IDF score had more variable eÔ¨Äects on the costs. See
the complete results in the full draft of this paper.

ImpressionRank estimations. We used our popular key-
word extraction algorithm to estimate the ImpressionRank
of several popular sites on Google and Yahoo!. Figures 4
and 5 shows our ImpressionRank estimates, as of January-
February 2009, for two sets of sites: news sites and travel

WWW 2009 MADRID!Track: Data Mining / Session: Graph Algorithms48567869::

*+,*+-*+.*+/*+0*+1*+2*+3*+4*+,**+

*+

Figure 3: Keyword extraction progress.

HIIJKL
MNOIIP
3*+
-*+
1*+
/*+
;<=><?@AB=<CAD=><EDFG<@DC<F

,**+

sites. Note that the ImpressionRank estimates are relative
within each search engine (the estimates for Google and Ya-
hoo! are not comparable to each other). For comparison,
we show traÔ¨Éc reports for the same sites as published by
Compete10, a Ô¨Årm that produces site traÔ¨Éc reports based
on ISP, panel, and toolbar information. (We chose to show
their reports, because they are the only traÔ¨Éc reporting Ô¨Årm
we found that freely provides actual numbers, as opposed to
unlabeled charts.)

Most of our estimates seem to be roughly consistent with
the data provided by Compete, where the measurements
on Google tend to demonstrate higher consistency. Note
that even if our algorithm would have worked perfectly, we
wouldn‚Äôt have expected absolute consistency with the Com-
pete data, because: (a) we measure impressions, not clicks,
which correspond to actual visits to the site; (b) we measure
potential traÔ¨Éc to the site coming from a search engine,
while the Compete data reÔ¨Çects the overall traÔ¨Éc to the
site, including traÔ¨Éc that does not originate from a search
engine.

^_`abcd_efgh_iicjk^akl

mmmnoppnoqr
mmmnpsturvwnoqr
mmmnmxwyupztqp{qwtnoqr
QRRSTU
ZR[\U]U
VWXRRY

Figure 4: ImpressionRank estimates for news sites.

Popular keyword extraction. Table 2 shows the most
popular keywords our algorithm found for several sites and
pages of interest. Some of the interesting observations we

10http://siteanalytics.compete.com

Figure 5: ImpressionRank estimates for travel sites.

¬â¬ä¬ã¬å¬ç¬é¬è¬ä¬ê¬ë¬í¬ì¬ä¬î¬î¬é¬ï¬ñ¬â¬å¬ñ¬ó

¬ò¬ò¬ò¬ô¬°¬£¬§¬û¬•¬¶¬ô¬†¬°¬¢
¬ò¬ò¬ò¬ô¬ö¬õ¬ú¬ö¬ù¬û¬ü¬ô¬†¬°¬¢
¬ò¬ò¬ò¬ô¬•¬£¬ü¬ß¬ö¬®¬°¬†¬û¬•¬©¬ô¬†¬°¬¢
|}}~¬Ä
¬Ö}¬Ü¬á¬Ä¬à¬Ä
¬Å¬Ç¬É}}¬Ñ

found are the following. While hard-core news issues, like
Barack Obama‚Äôs election and the war in Iraq, drive many
impressions to the news sites, these sites are also popular
due to other things, like weather (cnn), crossword puzzles
(New York Times), and comics (Washington Post). Travel
sites earn many of their impressions from people looking for
cheap Ô¨Çights, hotels, and car rentals. Expedia receives sig-
niÔ¨Åcant attention also from users looking for ski holidays,
cruises, and vacation packages. Popular keywords for peo-
ple‚Äôs homepages reveal what they are best known for. For
example, Kleinberg‚Äôs book with Tardos and his papers about
the small world phenomenon drive many impressions to his
homepage. Bill Gates, on the other hand, is popular among
searchers who wish to view pictures of his house.

Google

Yahoo!

www.cnn.com

cnn, election results, news,
obama, video, polls, health

weather, cnn, news of the world,

obama, cnn news, presidential election

www.nytimes.com

new york times, fashion,

new york times, ny times, crossword,

obama girl, crossword puzzles

the new york times, new times

www.washingtonpost.com

obama tax plan, washington post,

washington, post, the post

washington post, comics,

newspaper, iraq news

en.wikipedia.org/wiki/PageRank

page rank, ranking, google ranking,

pagerank, google algorithm,

google page rank, pagerank

google rank, page rank

expedia, travel, travel agents,

ski holidays, cruises, cruise deals

expedia, hotels, cheap hotels,

travel, vacation packages

www.expedia.com

www.orbitz.com

orbitz, car rental, cheap hotels,

hotels, cheap tickets, cheap flights,

flights, airline tickets,

orbitz, travel, flights

www.travelocity.com

travelocity, travel, flights,

hotels, cheap flights, travelocity,

flight tickets, travelocity flights,

travel, flights, airline tickets

www.cs.cornell.edu/home/kleinber

scientific american articles,
kleinberg, kleinberg tardos,

small world phenomenon

kleinberg, small world phenomenon,

jon kleinberg, world phenomenon,

robin ec08 parts

www.microsoft.com/BillGates

bill gates, bill gates home,

william gates, william gates iii

bill gates speeches,

gates, william gates,

william gates iii,

pictures of bill gates house

infolab.stanford.edu/‚àºsergey

sergey brin, favorite books,

sergey brin, brin sergey,

brin sergey, stanford sergey brin,

stanford home page, cs stanford,

data mining search engines

brin page, page brin

Table 2: Popular keywords for various sites and
pages, ordered by frequency.

6. RELATED WORK
Keyword extraction. Algorithms for keyword extrac-
tion have been studied quite extensively [13, 4, 7, 6, 14, 9].
A commonly used framework for keyword extraction is as

WWW 2009 MADRID!Track: Data Mining / Session: Graph Algorithms49follows. A set of candidate keywords (e.g., phrases) are ex-
tracted from the target document‚Äôs content. Each keyword
is associated with a set of features (e.g., a TF-IDF score,
the distance from the document‚Äôs beginning, capitalization,
etc). A classiÔ¨Åer is then used to decide which candidates are
returned as keywords and which are discarded. These works
are not applicable to our problem, because when they select
keywords they neither care whether a search engine returns
the target document on these keywords nor they care about
the popularity of these keywords.

Recent works [12, 5] on keyword suggestions for the pur-
pose of keyword-based online advertising, perform keyword
extraction similarly to previous methods, but use the popu-
larity of keywords as an additional feature in their classiÔ¨Åers.
However, these works rely on access to a private search query
log in order to estimate keyword frequencies. Similarly, the
Google AdWords Keyword Tool11 is a keyword suggestion
tool that presumably relies on access to Google‚Äôs query log.
Our algorithm is external and does not need access to pri-
vate data sources.

All the above mentioned keyword extraction techniques
perform an exhaustive search on all the candidate keywords
they generate in order to Ô¨Ånd the best keywords.
In our
setting, however, the number of candidates is very large and
thus brute-force search is not feasible. Our algorithm uses
best-Ô¨Årst search to zero in on the most promising candidates.

ImpressionRank estimation.
In our previous work [1]
we introduced the notion of ImpressionRank and presented
an algorithm for sampling random pages from a search en-
gine‚Äôs index proportionally to their ImpressionRank. This
sampling algorithm cannot be used to estimate the Impres-
sionRank of a given speciÔ¨Åc page. In [1] we also proposed a
technique for estimating the popularity of a given keyword
using the suggestion service. This technique is an essential
component in the algorithm we develop in this paper.

Google Trends for WebSites enables comparison of the
number of visitors to popular web sites over time and pro-
vides some additional information like related queries and
geographic visitation patterns. The data for this service
comes from internal Google data: search logs, Google An-
alytics data, etc. Similarly, comScore, Nielsen, Alexa, and
other traÔ¨Éc reporting tools collect their information from
private data sources.

7. CONCLUSIONS

In this work we introduced the popular keyword extrac-
tion and ImpressionRank estimation problems. We pre-
sented the Ô¨Årst external and local algorithm for these prob-
lems.

Our algorithm assumed that the search results we get at
the time we run the algorithm are the same as they were
during the time frame the query log underlying the sug-
gestion service has been collected. Obviously, this assump-
tion does not hold in practice as search engine indices are
constantly being updated. Our algorithm may produce un-
reliable results for pages corresponding to ‚Äúdynamic‚Äù top-
ics, where keyword popularity and/or search results change
rapidly.

The reliability of our algorithm depends on the quality of
the data set underlying the suggestion service. If the sug-
gestion service Ô¨Ålters many queries from its data set or if it

11https://adwords.google.com/select/KeywordToolExternal.

does not refresh the data set frequently enough, our algo-
rithm may produce biased/stale results. Another source of
bias in our algorithm is the suggestion-based volume esti-
mation of keywords, which as shown in our previous paper
may have some bias.

Our algorithm produces only relative estimates of the Im-
pressionRank of documents. Producing absolute estimates
would require knowledge of internal search engine data, like
the size of the query log.

We note that while generally suggestion services seem to
reÔ¨Çect query logs rather well, they deliberately Ô¨Ålter out
some negative keywords (e.g., porn or hate related). Our al-
gorithms therefore have diÔ¨Éculty in estimation the Impres-
sionRank of documents whose popular keywords are nega-
tive.

Despite the shortcomings highlighted above, we believe
our algorithms eÔ¨Äectively use the limited public data avail-
able, and produce useful results.

8. REFERENCES
[1] Z. Bar-Yossef and M. Gurevich. Mining search engine query

logs via suggestion sampling. In 34th VLDB, 2008.

[2] comScore. 61 billion searches conducted worldwide in

August.
www.comscore.com/press/release.asp?press=1802, 2008.

[3] J. Dean and S. Ghemawat. Mapreduce: simpliÔ¨Åed data

processing on large clusters. Commun. ACM,
51(1):107‚Äì113, 2008.

[4] E. Frank, G. W. Paynter, I. H. Witten, C. Gutwin, and

C. G. Nevill-Manning. Domain-speciÔ¨Åc keyphrase
extraction. In 16th IJCAI, pages 668‚Äì673, 1999.

[5] A. Fuxman, P. Tsaparas, K. Achan, and R. Agrawal. Using

the wisdom of the crowds for keyword generation. In 17th
WWW, pages 61‚Äì70, 2008.

[6] J. Goodman and V. R. Carvalho. Implicit queries for email.

In CEAS, July 2005.

[7] A. Hulth. Improved automatic keyword extraction given

more linguistic knowledge. In EMNLP, pages 216‚Äì223,
2003.

[8] T. Joachims, L. Granka, B. Pan, H. Hembrooke, and
G. Gay. Accurately interpreting clickthrough data as
implicit feedback. In 28th SIGIR, pages 154‚Äì161, 2005.
[9] D. Kelleher and S. Luz. Automatic hypertext keyphrase

detection. In 22nd IJCAI, 2005.

[10] S. J. Russell and P. Norvig. ArtiÔ¨Åcial Intelligence. A

Modern Approach. Prentice-Hall, 2nd edition, 2003.

[11] P. C. Saraiva, E. S. de Moura, R. C. Fonseca, W. M. Jr.,

B. A. Ribeiro-Neto, and N. Ziviani. Rank-preserving
two-level caching for scalable search engines. In 24th
SIGIR, pages 51‚Äì58, 2001.

[12] W. tau Yih, J. Goodman, and V. R. Carvalho. Finding

advertising keywords on web pages. In 15th WWW, pages
213‚Äì222, 2006.

[13] P. D. Turney. Learning algorithms for keyphrase extraction.

Inf. Retr., 2(4):303‚Äì336, 2000.

[14] P. D. Turney. Coherent keyphrase extraction via web

mining. In 20th IJCAI, pages 434‚Äì439, 2003.

[15] M. B. Valentine. Google drives 70 percent of traÔ¨Éc to most

web sites. http://searchengineoptimism.com/Google_
refers_70_percent.html.

[16] Y. Xie and D. R. O‚ÄôHallaron. Locality in search engine

queries and its implications for caching. In 21st
INFOCOM, 2002.

WWW 2009 MADRID!Track: Data Mining / Session: Graph Algorithms50