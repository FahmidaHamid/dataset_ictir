Exploiting Web Search to Generate Synonyms for Entities

Surajit Chaudhuri

Venkatesh Ganti

Dong Xin

Microsoft Research
Redmond, WA 98052

{surajitc, vganti, dongxin}@microsoft.com

ABSTRACT
Tasks recognizing named entities such as products, people
names, or locations from documents have recently received
signiÔ¨Åcant attention in the literature. Many solutions to
these tasks assume the existence of reference entity tables.
An important challenge that needs to be addressed in the
entity extraction task is that of ascertaining whether or not
a candidate string approximately matches with a named en-
tity in a given reference table. Prior approaches have relied
on string-based similarity which only compare a candidate
string and an entity it matches with. In this paper, we ex-
ploit web search engines in order to deÔ¨Åne new similarity
functions. We then develop eÔ¨Écient techniques to facilitate
approximate matching in the context of our proposed simi-
larity functions. In an extensive experimental evaluation, we
demonstrate the accuracy and eÔ¨Éciency of our techniques.

Categories and Subject Descriptors
H.2.8 [Database Applications]: Data Mining

General Terms
Algorithms

Keywords
Synonym Generation, Entity Extraction, Similarity Mea-
sure, Web Search

1.

INTRODUCTION

Tasks relying on recognizing entities have recently received
signiÔ¨Åcant attention in the literature [10, 12, 2, 14, 11, 9].
Many solutions to these tasks assume the existence of exten-
sive reference entity tables. For instance, extracting named
entities such as products and locations from a reference en-
tity table is important for several applications. A typical
application is the business analytics and reporting system
which analyzes user sentiment of products. The system peri-
odically obtains a few review articles (e.g., feeds from review
website and online forums), and aggregates user reviews for
a reference list of products (e.g., products from certain man-
ufacturers, or products in certain categories). Such a report-
ing application requires us to eÔ¨Äectively identify mentions of
those reference products in the review articles.
Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2009, April 20‚Äì24, 2009, Madrid, Spain.
ACM 978-1-60558-487-4/09/04.

Consider another application. The entity matching task
identiÔ¨Åes entity pairs, one from a reference entity table and
the other from an external entity list, matching with each
other. An example application is the oÔ¨Äer matching sys-
tem which consolidates oÔ¨Äers (e.g., listed price for products)
from multiple retailers. This application needs to accurately
match product names from various sources to those in the
system‚Äôs reference table, and to provide a uniÔ¨Åed view for
each product.

At the core of the above two applications, the task is to
check whether or not a candidate string (a sub-string from
a review article or an entry from an oÔ¨Äer list) matches with
a member of a reference table. This problem is challenge
because users often like to use phrases, which are not mem-
ber of the reference table, to refer to some entities. These
phrases can be an individual‚Äôs preferred description of an en-
tity, and the description is diÔ¨Äerent from the entity‚Äôs conven-
tional name included in a reference table. For instance, con-
sider a product entity ‚ÄúLenovo ThinkPad X61 Notebook‚Äù.
In many reviews, users may just refer to ‚ÄúLenovo ThinkPad
X61 Notebook‚Äù by writing ‚ÄúLenovo X61‚Äù, or simply ‚ÄúX61‚Äù.
Exact match techniques, which insist that sub-strings in re-
view articles match exactly with entity names in the refer-
ence table, drastically limit their applicability in our scenar-
ios.

To characterize whether or not a candidate string matches
with a reference entity string, an alternative approach is
to compute the string similarity score between the candi-
date and the reference strings [10, 6]. For example, the
(unweighted) Jaccard similarity1 function comparing a can-
didate string ‚ÄúX61‚Äù and the entity ‚ÄúLenovo ThinkPad X61
Notebook‚Äù would observe that one out of four distinct tokens
(using a typical white space delimited tokenizer) are com-
mon between the two strings and thus measures similarity
to be quite low at 1
4 . On the other hand, a candidate string
‚ÄúLenovo ThinkPad Notebook‚Äù has three tokens which are
shared with ‚ÄúLenovo ThinkPad X61 Notebook‚Äù, and thus
the Jaccard similarity between them is 3
4 . However, from
the common knowledge, we all know that ‚ÄúX61‚Äù does refer to
‚ÄúLenovo ThinkPad X61 Notebook‚Äù, and ‚ÄúLenovo ThinkPad
Notebook‚Äù does not because there are many models in the
ThinkPad series. We observe a similar problem with other
similarity functions as well. Therefore, the string-based sim-
ilarity does not often reÔ¨Çect the ‚Äúcommon knowledge‚Äù that
users generally have for the candidate string in question.

1These similarity functions also use token weights, say IDF
weights, which may in turn depend on token frequencies in
a corpus or a reference table.

WWW 2009 MADRID!Track: Data Mining / Session: Web Mining151In this paper, we address the above limitation. We ob-
serve that the ‚Äúcommon knowledge‚Äù is often incorporated
in documents within which a candidate string is mentioned.
For instance, the candidate string ‚ÄúX61‚Äù is very highly cor-
related with the tokens in the entity ‚ÄúLenovo ThinkPad X61
Notebook‚Äù. And, many documents which contain the tokens
‚ÄúX61‚Äù also mention within its vicinity the remaining tokens
in the entity. This provides a stronger evidence that ‚ÄúX61‚Äù
matches with ‚ÄúLenovo ThinkPad X61 Notebook‚Äù. In this pa-
per, we observe that such ‚Äúcorrelation‚Äù between a candidate
string œÑ and an entity e is seen across multiple documents
and exploit it. We propose new document-based similarity
measures to quantify the similarity in the context of multi-
ple documents containing œÑ . However, the challenge is that
it is quite hard to obtain a large number of documents con-
taining a string œÑ unless a large portion of the web is crawled
and indexed as done by search engines. Most of us do not
have access to such crawled document collections from the
web. Therefore, we exploit a web search engine and iden-
tify a small set of very relevant documents (or even just
their snippets returned by a web search engine) containing
the given candidate string œÑ . We rely on these small set
of highly relevant documents to measure the correlation be-
tween œÑ and the target entity e.

Note that our criteria matching œÑ and e needs the web
search results for œÑ to obtain a highly relevant set of docu-
ments or snippets containing œÑ . Hence, evaluating the simi-
larity between a candidate string with entities in a reference
table in general may not be applicable. Our approach here
is to Ô¨Årst identify a set of ‚Äúsynonyms‚Äù for each entity in the
reference table. Once such synonyms are identiÔ¨Åed, our task
of approximately matching a candidate string with a refer-
ence entity is now reduced to match exactly with synonym
or original entity names, i.e., the (sub)set of tokens in the
candidate string is equal to the token set of either a syn-
onym or of an original entity name. Methods which only
support exact match between candidate strings and entities
in a reference table are signiÔ¨Åcantly faster (e.g., [3]).

In this paper, we focus on a class of synonyms where each
synonym for an entity e is an identifying set of tokens, which
when mentioned contiguously (or within a small window) re-
fer to e with high probability. We refer to these identifying
token sets as IDTokenSets. We only consider IDTokenSets
for an entity e which consist of a subset of the tokens in e
for two reasons. First, the reference entity tables are often
provided by authoritative sources; hence, each entity name
generally does contain the most important tokens required
to identify an entity exactly but may also contain redun-
dant tokens which are not required for identifying the entity.
Therefore, it is suÔ¨Écient to isolate the identifying subset of
tokens for each entity as an IDTokenSet. The IDTokenSets
of an entity can be considered as keys that uniquely refer
to the original entity. Second, our target applications are
mainly entity extraction from documents. These documents
are mainly drawn from the web such as blogs, forums, re-
views, queries, etc, where it is often observed that users like
to represent a possibly long entity name by a subset of iden-
tifying tokens (e.g., 1-3 keywords).

The main technical challenge in identifying IDTokenSets
for an entity e is that the number of all token subsets of e
could be fairly large. For example, the entity ‚ÄúCanon EOS
Digital Rebel XTI SLR Camera‚Äù has 127 subsets. Directly
evaluating whether or not each subset œÑe of e matches with

In other words, if œÑe ‚äÇ œÑ(cid:48)

e would require a web search query to be issued. Therefore,
the main challenge is to reduce the number of web search
queries issued to identify IDTokenSets for an entity. Our
main insight in addressing this challenge is that for most
entities, if the set œÑe ‚äÇ e of tokens identify an entity e then
e where œÑe ‚äÇ œÑ(cid:48)
e ‚äÇ e also identiÔ¨Åes e (i.e., subset-
a set œÑ(cid:48)
e ‚äÇ e,
superset monotonicity).
then œÑ(cid:48)
e is more correlated to e. This is reminiscent of the
‚Äúapriori‚Äù property in the frequent itemset mining [1, 13],
where a superset is frequent only if its subsets are frequent.
We assume that the subset-superset monotonicity is true
in general and develop techniques which signiÔ¨Åcantly reduce
the number of web search queries issued. For example, sup-
pose ‚ÄúCanon XTI‚Äù identiÔ¨Åes ‚ÄúCanon EOS Digital Rebel XTI
SLR Camera‚Äù uniquely. Hence we assume that any superset
say ‚ÄúCanon EOS XTI‚Äù also identiÔ¨Åes e1 uniquely. There-
fore, if we eÔ¨Éciently determine the ‚Äúborder‚Äù of IDTokenSets
whose supersets are all IDTokenSets and whose subsets are
not, then we can often signiÔ¨Åcantly reduce the number of
web search queries per entity. In this paper, we develop ef-
Ô¨Åcient techniques to determine the border eÔ¨Éciently. We
further extend these techniques for multiple entities by tak-
ing advantage of entities which are structurally similar.

In summary, our contributions in this paper are as follows.

1. We consider a new class of similarity functions be-
tween candidate strings and reference entities. These
similarity functions are more accurate than previous
string-based similarity functions because they aggre-
gate evidence from multiple documents, and exploit
web search engines in order to measure similarity.

2. We develop eÔ¨Écient algorithms for generating IDTo-

kenSets of entities in a reference table.

3. We thoroughly evaluate our techniques on real datasets

and demonstrate their accuracy and eÔ¨Éciency.

The remainder of the paper is organized as follows. We
deÔ¨Åne problem in Section 2. We develop several eÔ¨Écient al-
gorithms for generating IDTokenSets in Section 3, and dis-
cuss some extensions in Section 4. We present a case study
that uses IDTokenSets for entity extraction in Section 5. In
Section 6, we discuss the experimental results. In Section 7,
we review the related work. Finally, we conclude in Section
8.

2. PROBLEM DEFINITION
We Ô¨Årst deÔ¨Åne the notation used in the paper. Let E
denote the set of entities in a reference table. For each e ‚àà E,
let T ok(e) denote the set of tokens in e. For simplicity, we
use e to denote T ok(e). We use the notation œÑe to denote a
subset of tokens of e. That is, œÑe ‚äÜ e.

Recall that we focus on identifying token sets which are
subsets of the token set of the entity. That is, an IDTokenSet
of an entity e consists of a subset œÑe of tokens in e.
In
the following, we formally deÔ¨Åne IDTokenSets. As discussed
earlier in Section 1, to characterize an IDTokenSet, we rely
on a set of documents and analyze correlations between the
candidate subset œÑe and the target entity e.
If a subset
œÑe identiÔ¨Åes e, then a large fraction, say Œ∏, of documents
mentioning œÑe is likely to contain the remaining tokens in
e ‚àí œÑe. We Ô¨Årst deÔ¨Åne the notion of a document mentioning
a token subset.

WWW 2009 MADRID!Track: Data Mining / Session: Web Mining152Definition 1. Let d be a document and œÑe be a set of
tokens. We say that d mentions œÑe if there exists a sub-
string s of d such that T ok(s) = œÑe.

We are now ready to deÔ¨Åne the aggregated correlation be-
tween œÑe and e with respect to a document set W (œÑe). Infor-
mally, the aggregated correlation is the aggregated evidence
that œÑe refers to e from all documents mentioning œÑe.

ID Document
d1 The All-New, 2009 Ford F150 takes on ...
d2
Sony Vaio F150 is...business notebook...
d3 An overview of the Ford F150 Pickup...

Table 1: A set of documents

corr(œÑe, e, W (œÑe)) =

Example 1. For instance, the document d2 in Table 1
mentions the subset {V aio, F 150}, and the documents d1
and d3 mention the subset {F ord, F 150}

For each document that mentions a subset œÑe, we check
whether the document also contains the remaining tokens in
e‚àí œÑe. In the ideal case, a large fraction of these documents
mention tokens in e‚àí œÑe next to the mention of œÑe. However,
this may be too constraining. Hence, we relax this notion
in two ways. First, we want to parameterize the context
window size p within which we expect to observe all tokens
in e‚àíœÑe. Second, it may be good enough to Ô¨Ånd a signiÔ¨Åcant
fraction of tokens in e‚àí œÑe within the context window of the
mention of œÑe; the size of the fraction quantiÔ¨Åes the evidence
that œÑe refers to e.

Definition 2. (p-window context) Let M = {m} be a
set of mentions of œÑe in a document d = t1, . . . , tn. For each
mention m, let c(m, p) be the sequence of tokens by including
(at most) p tokens before and after m. The p-window context
of œÑe in d is C(œÑe, d, p) =

(cid:83)

m‚ààM c(m, p).

Example 2. For example, the 1-window context of ‚ÄúF150‚Äù
in document d2 of Table 1 is {Vaio, F150, is} and that in d1
and d3 are {Ford, F150, takes} and {F ord, F 150, P ickup},
respectively.

We now deÔ¨Åne the measure to quantify the evidence that
œÑe refers to e in a document d. We Ô¨Årst deÔ¨Åne the stricter
notion of evidence g1, where all tokens in e‚àí œÑe are required
to be present in the p-window context of œÑe.

(cid:189)

g1(œÑe, e, d) =

if e ‚äÜ C(œÑe, d, p)
otherwise

1
0

We now deÔ¨Åne a relaxed notion of evidence g2 of a doc-
ument referring to an entity e, which is quantiÔ¨Åed by the
fraction of tokens in e‚àí œÑe that are present in the p-window
context of œÑe.

(cid:80)

(cid:80)

g2(œÑe, e, d) =

t‚ààC(œÑe,d,p)‚à©e w(t)

t‚ààe w(t)

(2)

where w(t) is the weight (e.g., IDF weight [5]) of the token
t.

The IDTokenSets problem is to generate for a given entity
e all its IDTokenSets with respect to a document collection
D. In the ideal case, this set corresponds to a large collection
of documents on the web which requires us to have access
to a crawled repository of the web. Since this is hard to
have access to in the scenarios we focus on, we exploit the
web search engines to provide us a small set W (œÑe) of very
relevant document snippets which are highly relevant for œÑe.

Definition 3. (Correlation) Given e, œÑe, a search en-

gine W , we deÔ¨Åne the aggregated correlation corr(œÑe, e, W (œÑe))
as follows.

(cid:80)

g(œÑe, e, d)
d‚ààW (œÑe),d mentions œÑe
|{d|d ‚àà W (œÑe), d mentions œÑe}|

Given a correlation threshold Œ∏, we say that œÑe is an ID-

TokenSet of e if corr(œÑe, e, W (œÑe)) ‚â• Œ∏.

Example 3. Let e =‚ÄúSony Vaio F150 Laptop‚Äù be the tar-
get entity, and œÑe = {F 150} be the candidate subset. Sup-
pose documents in Table 1 are obtained snippets from W (œÑe).
Each document mentions {F 150}. In order to validate whether
œÑe = {F 150} is an IDTokenSet of e, we compute:
g1(œÑe, e, d1) = 0, g1(œÑe, e, d2) = 1, g1(œÑe, e, d3) = 0.
Thus, corr(œÑe, e, W (œÑe)) = 1
Suppose the token weights of {Sony, V aio, F 150, Laptop} are
{6.8, 9.5, 10.5, 6.5}. Using g2, we have:
g2(œÑe, e, d1) = 0.29, g2(œÑe, e, d2) = 0.80, g2(œÑe, e, d3) = 0.29.
Thus, corr(œÑe, e, W (œÑe)) = 1.38

3 = 0.33.

3 = 0.46.

Definition 4. (IDTokenSets Problem) Given an en-
tity e, a search engine W , and the correlation threshold Œ∏,
the IDTokenSets problem is to identify the set Se of all sub-
sets such that for each œÑe ‚àà Se, corr(œÑe, e, W (œÑe)) ‚â• Œ∏.

Using the above similarity function, adapting techniques
which measure similarity between candidate strings and en-
tities from reference tables directly is an expensive approach.
Therefore, we pre-process the reference entity table and ex-
pand the original entities with their IDTokenSets. By gen-
erating accurate IDTokenSets oÔ¨Ä-line, we transform the ap-
proximate match against the reference entity table problem
to an exact match over the set of IDTokenSets, thus signiÔ¨Å-
cantly improving the eÔ¨Éciency and accuracy of the approx-
imate lookup task.

(1)

3. GENERATING IDTOKENSETS

We now describe our techniques for eÔ¨Éciently generating
IDTokenSets of a given set of entities. We Ô¨Årst discuss the
optimization criterion and the complexity of the optimal so-
lution for generating IDTokenSets of a single entity. We then
outline an algorithmic framework, under which, we develop
two algorithms. We then extend these techniques to gen-
erate IDTokenSets for a set of entities, and take advantage
of entities which are structurally similar. We show that one
of the discussed algorithms is within a factor of the optimal
solution.
3.1 Optimization Criterion

The input to our system is a set E of entities, and a search
interface W . For each entity e, all subsets of e consist of the
candidate space. For each subset œÑe of entity e, we want to
validate whether œÑe is an IDTokenSet of e, using the mea-
sure in DeÔ¨Ånition 3. SpeciÔ¨Åcally, the general framework to
process an entity e is to validate each of its subsets œÑe, which
consists of the following two steps:

WWW 2009 MADRID!Track: Data Mining / Session: Web Mining1531. Send œÑe as a query term to W , and retrieve W (œÑe)
(title, URL and snippets) as the relevant documents;

2. Evaluate corr(œÑe, e, W (œÑe)), and report œÑe is an IDTo-

kenSet if corr(œÑe, e, W (œÑe)) ‚â• Œ∏;

We consider the whole process to validate œÑe as an atomic
operator, and notate it as validate(œÑe). Furthermore, we as-
sume the cost of validate(œÑe) for diÔ¨Äerent œÑe is roughly same
since the most expensive part of validate(œÑe) is sending œÑe
to W . Thus, in order to eÔ¨Éciently generate all IDTokenSets
of an entity e, we need to reduce the number of web search
queries we issued. The optimization is mainly based on the
intuition that removing some tokens from a subset œÑe weak-
ens the correlation between œÑe and e. On the other hand,
adding more tokens (belong to e) to œÑe enhances the corre-
lation between œÑe and e. This is formally characterized as
the subset-superset monotonicity in DeÔ¨Ånition 5.

Definition 5. (subset-superset monotonicity) Given
e be two subsets of e, and œÑe ‚äÇ œÑ(cid:48)
e.
e is also an IDTokenSet

an entity e, let œÑe and œÑ(cid:48)
If œÑe is an IDTokenSet of e, then œÑ(cid:48)
of e.

e (œÑe ‚äÇ œÑ(cid:48)
e (œÑ(cid:48)

Based on the subset-superset monotonicity, if œÑe is an ID-
e ‚äÜ e) are IDTokenSets
TokenSet of e, all subsets œÑ(cid:48)
of e, and thus can be pruned for validation.
If œÑe is not
e ‚äÇ œÑe) are not IDTo-
an IDTokenSet of e, all subsets œÑ(cid:48)
kenSets of e, and thus can be pruned for validation. There-
fore, by appropriately schedule the order in which subsets
œÑe are submitted for validation, we can reduce the number
of web search queries. Before we present the detailed algo-
rithms, we Ô¨Årst discuss the optimal solution.
3.2 Complexity of the Optimal Algorithm

In order to exploit the subset-superset monotonicity, we
use the lattice structure to model the partial order between
all subsets. An example of subset-lattice of entity ‚ÄúSony
Vaio F150 Laptop‚Äù is shown in Figure 1.

Figure 1: Subset-lattice of ‚ÄúSony Vaio F150 Laptop‚Äù

The optimal algorithm is built upon the notion of mini-
mal positive subset and maximal negative subset, as deÔ¨Åned
below.

Definition 6. Given an entity e, a subset œÑe is a mini-
mal positive subset if validate(œÑe) = true and for all subsets
e ‚äÇ œÑe, validate(œÑ(cid:48)
œÑ(cid:48)
e) = f alse. Similarly, a subset œÑe is a
maximal negative subset if validate(œÑe) = f alse and for all
subsets œÑ(cid:48)

e such that œÑe ‚äÇ œÑ(cid:48)

e ‚äÜ e, validate(œÑ(cid:48)

e) = true.

We use the notation Cut(e) to denote the set of all min-
imal positive and maximal negative subsets. We now illus-
trate it with an example.

Example 4. Given the entity ‚ÄúSony Vaio F150 Laptop‚Äù,
e = {sony, vaio, laptop} is not an IDTokenSet
the subset œÑ 1
since there are models other than F150 in the vaio series.
Consequently, all subsets of {sony, vaio, laptop} are not ID-
TokenSets. Because F150 is a popular ford truck, œÑ 2
e =
{F 150} is not an IDTokenSet either. However, œÑ 3
e = {sony,
e = {F 150, laptop} are all
F 150}, œÑ 4
IDTokenSets. These Ô¨Åve subsets constitute the cut. One
can easily verify that all other subsets are either supersets of
œÑ 3
e , œÑ 4

e = {vaio, F 150} and œÑ 5

e or subsets of œÑ 1

e , œÑ 2
e .

e , œÑ 5

Consider a special case where all subsets with

|e|
2 tokens
(suppose |e| is even) are not IDTokenSets and all subsets
|e|
with
2 + 1 tokens are IDTokenSets. One can easily verify
|e|
2 + 1 tokens constitute Cut(e),
that all subsets with
and |Cut(e)| is exponential to |e|. For a given entity, a
scheduling algorithm is optimal if it validates the minimal
number of subsets. One can easily verify that any subset in
the cut can not be pruned by other subsets, and thus has
to be validated. The following lemma shows the connection
between optimal solution and Cut(e).

|e|
2 or

Lemma 1. Given an entity e, the optimal scheduling algo-
rithm validates and only validates subsets in Cut(e). In the
worst case, the number of subsets in Cut(e) is exponential
to |e|.
3.3 Algorithmic Framework

As shown in the previous subsection, given an entity e,
it is suÔ¨Écient to validate those subsets that are in Cut(e).
However, both the maximal negative and minimal positive
subsets are not known beforehand.
In this paper, we use
a greedy algorithmic framework that iteratively probes a
subset, validates it and prunes other subsets (if applicable).
The algorithm stops when no subset is left undetermined.
The framework is outlined in Algorithm 1. Let Pe be the
set of all subsets of e. Our task is to determine for all subsets
in Pe, whether they are IDTokenSets of e. The algorithm
maintains a set of candidate subsets in Le. Initially, Le =
Pe. As soon as a subset œÑe ‚àà Le is validated or pruned, œÑe is
removed from Le. The algorithm repeats the following two
steps until Le = œÜ.

1. validate-and-prune (Line 4 to Line 9): It validates a
If œÑe is an IDTokenSet, all œÑe‚Äôs supersets
subset œÑe.
are determined to be IDTokenSets, and will be pruned
from Le for further validation. If œÑe is not an IDTo-
kenSet, all œÑe‚Äôs subsets are determined to be not ID-
TokenSets, and will be pruned from Le as well.

2. getnext (Line 3): It determines which subset to visit
next. We discuss various strategies for implementing
getnext in this section.

3.4 Single-Entity Scheduling

We now describe two strategies to implement getnext. The
depth-Ô¨Årst scheduling starts with the maximal (or minimal)
subset, and schedules subsets for validation by following the
edges on the lattice structure. The max-beneÔ¨Åt scheduling
considers all subsets simultaneously: for each remaining sub-
set, it computes the potential beneÔ¨Åt for each subset, and
picks the one with the maximal beneÔ¨Åt.

 	
 	
  	
 	
   	
  	
 	
    	
 WWW 2009 MADRID!Track: Data Mining / Session: Web Mining154Algorithm 1 Generating IDTokenSets for an Entity

Input: An entity: e, Search interface: W
the size of the context window: p,
number of top documents: k,
threshold for IDTokenSet: Œ∏
1: Let Le = Pe; //all subsets of e;
2: while (Le is not empty)
3:
4:
5:
6:
7:
8:
9:
10: return

œÑe = getnext(Le);
Submit œÑe to W , and retrieve W (œÑe);
if (corr(œÑe, e, W (œÑe)) ‚â• Œ∏) // œÑe is an IDTokenSet
Report œÑe and all its supersets as IDTokenSets;
Remove œÑe and all its supersets from Le;
Remove œÑe and its subsets from Le;

else//œÑe is not an IDTokenSet

3.4.1 Depth-Ô¨Årst Scheduling
Given an entity e, all its subsets constitute a lattice (see
Figure 1). The main idea of depth-Ô¨Årst strategy is to start
with a top root node (it can start at the bottom node as
well) and recursively traverse the lattice structure. Suppose
the algorithm reaches a node corresponding to a subset œÑe
at some stage. The getnext step determines which subset to
validate next. It consists of three steps:

1. Let œÑ c

e be a child of œÑe. If ‚àÉœÑ(cid:48)

e could be œÑ c

e (note œÑ(cid:48)
œÑ c
scheduling. That is, there is a descendent œÑ(cid:48)
status is unknown;

e itself), œÑ c

e ‚àà Le such that œÑ(cid:48)

e ‚äÜ
e is a candidate for
e whose

2. If step 1 did not Ô¨Ånd a candidate œÑ c
e ‚àà Le such that œÑ(cid:48)

the algorithm looks for the siblings of œÑe. Let œÑ s
sibling of œÑe. If ‚àÉœÑ(cid:48)
e , œÑ s
candidate for scheduling;

e for scheduling,
e be a
e ‚äÜ œÑ s
e is a

3. If neither step 1 nor step 2 Ô¨Ånd a candidate for schedul-
e , and

ing, the algorithm goes back to œÑe‚Äôs parent œÑ p
restarts the step 1 on œÑ p
e .

When multiple subsets (such as multiple children of œÑe or
multiple siblings of œÑe) are available for scheduling, we rely
on the intuition that the higher string similarity between the
subset and e, the higher the possibility that this subset is an
IDTokenSet. Since the depth-Ô¨Årst scheduling starts from e,
we expect to quickly Ô¨Ånd a subset that is not an IDTokenSet
in order to prune all its descendent subsets. Therefore, we
pick the candidate subset with the lowest string similarity
(e.g., the Jaccard similarity as deÔ¨Åned in Section 2). Simi-
larly, if the traversal starts from the bottom node, we will
pick the candidate subset with the highest string similarity.
Theorem 1 gives a performance guarantee by the depth-
Ô¨Årst scheduling algorithm. The main insight is that using
the depth-Ô¨Årst scheduling, we will validate at most |e| ‚àí 1
subsets before we hit a subset belonging to the cut. We omit
the detailed proof here.

Theorem 1. Given an entity e, let DF S(e) be the num-
ber of validations (e.g., web search queries) performed by
the depth-Ô¨Årst scheduling, and let OP T (e) be the number of
validation performed by the optimal scheduling. We have
DF S(e) ‚â§ |e|OP T (e).

3.4.2 Max-BeneÔ¨Åt Scheduling
DiÔ¨Äerent from the depth-Ô¨Årst scheduling, the max-beneÔ¨Åt
scheduling does not conÔ¨Åne to the lattice structure. Instead,
at any stage, all subsets in Le are under consideration, and
the one with the maximum estimated beneÔ¨Åt will be picked.
The getnext step in the max-beneÔ¨Åt scheduling works as
follows. For each subset œÑe ‚àà Le, we can beneÔ¨Åt in two ways
from validating œÑe: pos benef it(œÑe) if validate(œÑe) = true
or neg benef it(œÑe) if validate(œÑe) = f alse. The beneÔ¨Åt is
simply computed by the number of subsets in Le that are
expected to be pruned. If validate(œÑe) = true, the beneÔ¨Åt
of validating œÑe is deÔ¨Åned as follows.

pos benef it(œÑe) = |{œÑ

e|œÑe ‚äÜ œÑ
(cid:48)

e ‚äÜ e and œÑ
(cid:48)

e ‚àà Le}|
(cid:48)

(3)

If validate(œÑe) = f alse, the beneÔ¨Åt of validating œÑe is

deÔ¨Åned as follows.

neg benef it(œÑe) = |{œÑ

e|œÑ
(cid:48)

e ‚äÜ œÑe and œÑ
(cid:48)

e ‚àà Le}|
(cid:48)

(4)

We consider three aggregate beneÔ¨Åt formulation: max,

min and avg, as deÔ¨Åned as follows.

max(œÑe) = max{pos benef it(œÑe), neg benef it(œÑe)}
min(œÑe) = min{pos benef it(œÑe), neg benef it(œÑe)}
avg(œÑe) = 1
2 (pos benef it(œÑe) + neg benef it(œÑe))

Intuitively, max is an aggressive aggregate which always
aims for the best; min is a conservative aggregate which
guarantees for the worst scenario; and avg is in between the
above two. For each of the aggregate option, the getnext
step picks a œÑe with the maximum aggregated beneÔ¨Åt.
3.5 Multiple-Entity Scheduling

In this subsection, we discuss the techniques for scheduling
subset validation when the input consists of multiple enti-
ties, and our goal is to generate IDTokenSets for all entities.
We Ô¨Årst note that our techniques in this section improve
the eÔ¨Éciency and the results are still correct. That is, the
result would be the same as that of processing each entity
independently, and taking the union of the results.

The intuition is as follows. Often, names of entities follow
an implicit structure. The IDTokenSets of such structurally
similar entities are also likely to follow the implicit structure.
By exploring such structured information across entities, our
scheduling strategy can be more eÔ¨Écient. Suppose there is a
group of entities e1, e2, . . . , en, which are structurally similar
to each other. After the Ô¨Årst i entities are processed, we may
have a better idea on which subsets of ei+1 are IDTokenSets
and which are not. For instance, both ‚Äúlenovo thinkpad
T41‚Äù and ‚Äúlenovo thinkpad T60‚Äù belong to the thinkpad se-
ries from Lenovo. After processing ‚Äúlenovo thinkpad T41‚Äù,
one may identify that {T 41} is an IDTokenSet of ‚Äúlenovo
thinkpad T41‚Äù, and {T 41} belongs to the cut. By observing
the structural similarity across entities, we may Ô¨Årst validate
{T 60} in its lattice structure. Depending on the outcome
of validation, the scheduling algorithm may terminate early
or proceed further.

In order to build the connection across multiple entities,
we Ô¨Årst group together entities that are structurally similar.
For each group, we create a group proÔ¨Åle, which aggregates
statistics from entities in the group processed so far. Our
new beneÔ¨Åt estimation function for any subset exploits the
the statistics on the group proÔ¨Åle. We continue to apply

WWW 2009 MADRID!Track: Data Mining / Session: Web Mining155Algorithm 1 on each individual entity with a new getnext
that leverages the group proÔ¨Åle statistics.
3.5.1 ProÔ¨Åle Grouping
Observe that our single entity scheduling algorithms op-
erate on the subset lattice structure obtained by tokenizing
an input entity.
In order to share statistics for improved
scheduling across entities, the statistics also have to be on
the same subset lattice structure. Otherwise, it would be
much harder to exploit them. Therefore, the main constraint
on grouping multiple entities together for statistics collection
is that we should be able to easily aggregate statistics across
entity lattices.

In this paper, we take an approach of normalizing entity
names based on ‚Äútoken level‚Äù regular expressions. That is,
each of these normalization rules takes as input a single to-
ken and maps it to a more general class, all of which are
accepted by the regular expression. The outcome is that
entities which share the same normal form (characterized
by a sequence of token level regular expressions) may all be
grouped together. More importantly, they would share the
same subset lattice structure. We further denote the nor-
malized form shared by all entities in the group as group
proÔ¨Åle. Some example token level regular expressions are as
follows.

‚Ä¢ Regular expressions: [0 ‚àí 9]+ ‚Üí P U RE N U M BER,

[A ‚àí Z][0 ‚àí 9]+ ‚Üí CHAR N U M BER

‚Ä¢ Synonyms: {red, green, blue, white, . . .} ‚Üí COLOR,
{standard, prof essional, enterprise} ‚Üí V ERSION

Formally, we deÔ¨Åne the group and its proÔ¨Åle as follows.

An example is given in Example 5.

Definition 7. Let e1, e2, . . . , en be n entities. Let N =
{rule1, rule2, . . . , ruler} be a set of single token normaliza-
tion rules. We denote ei as the normalized form of ei after
applying rules in N . A set {e1, e2, . . . , en} form a group if
e1 = e2 = . . . = en. The group proÔ¨Åle is the normalized
entity ei (i = 1, . . . , n).

Example 5. Suppose the normalization rule is [A‚àíZ][0‚àí
9]+ ‚Üí CHAR N U M BER. Given two entities e1=‚Äúlenovo
thinkpad T41‚Äù and e2=‚Äúlenovo thinkpad T60‚Äù, their normal-
ized forms are both e1 = e2=‚Äúlenovo thinkpad CHAR NU-
MBER‚Äù. Therefore, e1 and e2 form a group, and the group
proÔ¨Åle is ‚Äúlenovo thinkpad CHAR NUMBER‚Äù.

Based on the normalized rules, all input entities are parti-
tioned into disjoint groups. Note that the normalized form
of some entities may be the same as the original entity.
This occurs when no normalization rules apply on the en-
tity. Each entity where no normalization rules apply forms
its own group and the multi-entity scheduling reduces to
single-entity scheduling strategy.
3.5.2 ProÔ¨Åle-Based Scheduling
After grouping entities into multiple partitions, we process
entities one group at a time.
In each group, we process
entities one by one. Let e1, e2, . . . , en be entities in a group,
and let ep be the group proÔ¨Åle. For any subset œÑei from ei,
there is a corresponding subset œÑep from ep.

Assume the entities are processed in the order of e1, . . . , en.
In the beginning, there are no statistics on ep. We will use

the single-entity scheduling algorithm (as shown in the pre-
vious subsection) to process e1. Suppose the Ô¨Årst i entities
have been processed, and the next entity is ei+1. The algo-
rithm Ô¨Årst updates the statistics on ep using the validation
results of ei. For each subset œÑep in ep, we keep two coun-
ters: œÑep .positive and œÑep .negative. For each subset œÑei in
ei, if œÑei is an IDTokenSet of ei, we increment œÑep .positive
by 1. Otherwise, we increment œÑep .negative by 1. After ep
has accumulated statistics over a number of entities (e.g.,
i > 1), we use the proÔ¨Åle information to process ei+1.
Similar to the max-beneÔ¨Åt search, among all remaining
subsets œÑei+1 in Lei+1 , we will pick the one with the maxi-
mum beneÔ¨Åt. The idea is that if a subset œÑei+1 has higher
probability to be an IDTokenSet of ei+1, that is, œÑep .positive >
œÑep .negative, we estimate its beneÔ¨Åt using pos benef it(œÑei+1 )
(e.g., Equation 3). If œÑei+1 has higher probability to be in-
valid, we estimate its beneÔ¨Åt using neg benef it(œÑei+1 ) (e.g.,
Equation 4).
If there is a tie between positive count and
negative count, we do not consider œÑei+1 . If all subsets tie
on the positive and negative counts, we switch to the single
entity scheduling algorithm as we did for the Ô¨Årst entity e1.
The beneÔ¨Åt of a subset œÑei+1 is formally deÔ¨Åned as follows.

Ô£±Ô£¥Ô£¥Ô£¥Ô£≤Ô£¥Ô£¥Ô£¥Ô£≥

benef it(œÑei+1 ) =

pos benef it(œÑei+1 )

if œÑep .postive > œÑep .negative

neg benef it(œÑei+1 )

if œÑep .postive < œÑep .negative

0 Otherwise

Observe that if for any entity ei+1, where (i > 0), if the
proÔ¨Åle ep correctly accumulates the statistics such that for
any œÑei+1 , œÑei+1 is an IDTokenSet of ei+1 iÔ¨Ä œÑep .positive >
œÑep .negative, then the proÔ¨Åle-based scheduling algorithm for
ei+1 is optimal. That is, we directly validate the subsets on
the Cut(ei+1), thus mimicking the optimal algorithm would.
In our experiments, we observe that the simple beneÔ¨Åt func-
tion as deÔ¨Åned above works well. Our algorithmic frame-
work is able to take other beneÔ¨Åt functions, and we intend
to further explore them in the future.

4. EXTENSIONS

In this section, we discuss two extensions to our approach.
In the Ô¨Årst extension, we show how to incorporate additional
constraints to prune candidate subsets for generating IDTo-
kenSet. Such constraints are especially meaningful for enti-
ties with a large number of tokens. In the second extension,
we relax the deÔ¨Ånition of mention (DeÔ¨Ånition 1) to further
enrich the applicability of our techniques.
4.1 Constrained IDTokenSet Generation

In the above section, we assume all subsets are candidates
for generating IDTokenSet.
In some scenarios, users may
have additional constraints that can be applied to prune
some subset candidates.
It is especially beneÔ¨Åcial if the
constraint can be evaluated before the subset is validated.
Thus, we can save the cost of validation. To incorporate
constraints into Algorithm 1, we simply replace Le (the can-
didate space) by the set of candidates which satisfy the con-
straints. The rest of the algorithm remains the same.
4.2 Gap Mentions

In Example 3, {Sony, F 150} is an IDTokenSet of ‚ÄúSony
Vaio F150 Laptop‚Äù. However, a document may mention

WWW 2009 MADRID!Track: Data Mining / Session: Web Mining156‚ÄúSony PCG F150‚Äù instead of ‚ÄúSony F150‚Äù. In DeÔ¨Ånition 1,
we require that all tokens in a mention be contiguous within
a document. Therefore, we would not identify the mention
‚ÄúSony PCG F150‚Äù. We now relax the deÔ¨Ånition of document
mentioning a candidate by relaxing the requirement that a
document sub-string consist of a contiguous set of tokens
in a document. Instead, we may consider all sets of tokens
which are ‚Äúclose‚Äù to each other within a window. Informally,
we consider w-gap token sets from the document where the
maximum gap (i.e., number of intervening tokens) between
neighboring tokens in the subset is no more than w. A w-gap
token set that exactly matches with an IDTokenSet is called
w-gap mention. w controls the proximity of the tokens, w =
0 means that the token set is a contiguous token set in the
document d, w = ‚àû means any subsets of tokens in d may
be a valid mention. Typically, one may set w = 0 for longer
documents such as web pages, and set w = ‚àû for short
documents such as queries, titles or snippets.

5. CASE STUDY: ENTITY EXTRACTION

Here we describe a case study that uses IDTokenSets to
facilitate fast and accurate entity extraction. We assume
the extraction task is based on a reference entity table, as
demonstrated by the example applications in Section 1. The
system architecture, which is outlined in Figure 2, consists
of two phases: the oÔ¨Ñine phase and the online phase.

Figure 2: System Framework for Entity Extraction

The oÔ¨Ñine phase generates the IDTokenSets for each ref-
erence entity and constructs an exact lookup structure over
the IDTokenSets. The online phase extracts sub-strings as
candidates from query documents and checks them against
the lookup structure for exact match. We observe that users
often use diÔ¨Äerent token order in mentioning an entity. For
instance, both ‚Äúsony F150 notebook‚Äù and ‚Äúsony notebook
F150‚Äù refer to the same entity. Therefore, we apply the set-
based exact match criterion (or, the Jaccard similarity with
threshold 1) between candidates from query documents and
IDTokenSets. In doing this, we re-order tokens in each ID-
TokenSet according to a global order (say, lexicographic).
The tokens in the candidates are also re-ordered according
to the same order.

In order to eÔ¨Éciently extract candidates from query doc-
uments, we apply the optimization techniques used in [6].
First, we create a token table which keeps all distinct tokens
appearing in the generated IDTokenSets. Given a query
document, we Ô¨Årst check each token against the token ta-
ble, and only keep those hit-tokens (i.e., tokens appearing in

the table). We denote a set of contiguous hit-tokens as hit-
sequence. Suppose we derive h hit-sequences from a query
document. The second optimization exploits the concept of
strong-token. From each IDTokenSet, we identify the token
with the least frequency over the corpus. For instance, from
the IDTokenSet {sony, F 150, notebook}, one may extract
F 150 as the strong-token. Since we enforce exact match, a
strong-token has to be matched between any candidate and
its matched IDTokenSet. We put strong-tokens from all ID-
TokenSets into a strong-token table. For each hit-sequence
derived from the Ô¨Årst step, we check whether it contains
a strong token. A hit-sequence is pruned immediately if it
does not contain a strong-token. For all the remaining hit-
sequences, we will enumerate sub-strings with length up to
L (suppose the longest IDTokenSet has L tokens), while en-
suring that each of which contains at least one strong-token.
We then lookup each of these sub-strings against the lookup
structure.

6. PERFORMANCE STUDY

We now present the results of an extensive empirical study
to evaluate the techniques described in this paper. We use
real data sets for the experiments. The reference entity table
is a collection of 200k product names (e.g., consumer and
electronics, bicycles, shoes, etc). The number of tokens in
the product names varies from 2 to 10, with 3.6 tokens on
average.

The major Ô¨Åndings of our study can be summarized as

follows:

1. High quality IDTokenSets:

the document-based
measure performs signiÔ¨Åcantly better than the tradi-
tional string-based similarity measure in determining
IDTokenSets;

2. Manageable size: the number of IDTokenSets that
we generated is within reasonable size (i.e., generally
2-4 times of the original entity number);

3. EÔ¨Écient generation: our proposed algorithms are
able to prune more than 80% of the web queries in
generating IDTokenSet;

4. Fast entity extraction: the case study on entity ex-
traction shows that using IDTokenSets, our system is
able to process 200 documents per second, where each
document contains 3, 689 tokens on average;

In the remaining of this section, we show the experimental
results in terms of quality of IDTokenSets, cost of material-
izing IDTokenSets and the number of IDTokenSets. We also
report the performance of the entity extraction application.
We use Live Search API 2 to retrieve web documents.
6.1 Quality of IDTokenSets

To examine the quality of the IDTokenSets, we compare
our proposed document-based measures with the traditional
string-based similarity measure (e.g., weighted Jaccard sim-
ilarity). We use Live Search to retrieve top-10 results. Since
we only consider title, url and snippets, which are succinct in
general, we set w = ‚àû in determining w-gap mentions (Sec-
tion 4.2), and p = ‚àû in determining p-window context (Def-
inition 2). We notate Corr1 for the document-based mea-
sure with g1 (Equation 1), Corr2 for the document-based
2http://dev.live.com/livesearch/

 	
Pre-Compute !"#$	%	
&Exact Set-based MatchPre-ComputeIDTokenSetsOnline Lookup'((()&&&(')*&*)'((()&&DocumentsEntity Mentions&&WWW 2009 MADRID!Track: Data Mining / Session: Web Mining157Figure 3: Precision-Recall on Bicycle
Synonyms

Figure 4: Precision-Recall on Laptop
Synonyms

Figure 5: Precision-Recall on Shoe
Synonyms

Figure 6: Generating IDTokenSet for
10k Product Names

Figure 7: Generating IDTokenSet for
10k Product Names

Figure 8: Number of Web Queries
w.r.t. Entity Number

measure with g2 (Equation 2), and Jaccard for the string-
based weighted Jaccard Similarity. The experiments are
conducted on three diÔ¨Äerence categories: bicycles, shoes and
laptops.
In each category, we sample 100 product names,
and manually label all subsets which are IDTokenSets. By
varying the threshold in (0.3, 0.5, 0.7, 0.9) for Corr1, (0.6,
0.7, 0.8, 0.9) for both Corr2 and Jaccard, we plot the precision-
recall curves in Figures 3-5.

We observe that in all three categories, the document-
based measures are signiÔ¨Åcantly better than the string-based
measure. Among two variants of document-based measures,
Corr1 performs better than Corr2. We notice that Corr1
is a fairly strict measure since it requires all tokens (in the
original entity) to be presenting in at least 50% of the doc-
uments (assuming threshold is 0.5). When the threshold is
0.5, it achieves 80% recall and more than 97% precision.

The recall drops when we increase the threshold. This is
expected since not all documents mention all tokens in the
context. To improve the recall, we found it is important
to recognize token-synonyms in matching tokens. For in-
stance, the token in the product title may be ‚Äùbicycle‚Äù, and
in the document, users may write ‚Äùbike‚Äù. Given the prior
knowledge that we are processing bicycle names, ‚Äùbike‚Äù can
be matched to ‚Äùbicycle‚Äù.
In this experiment, we only use
the exact token matching, and the preliminary results is al-
ready very promising. We leave the robust token matching
as future work.

6.2 Cost of Materializing IDTokenSets

Here we compare the computational performance using
various strategies for generating IDTokenSets, as discussed
in Section 3. The computation time is roughly proportional
to the number of web queries. Since the time needed for
a web query relies on network traÔ¨Éc condition and server
loading status, we compare the number of web queries in-
stead.

We use Corr1 as the measure, and Ô¨Åx the threshold to
0.5. The reference entity table consists of 10k electronic and
consumer product names. Figure 6 shows the number of web
queries used by the following methods: dfs-topdown (DT),
the depth-Ô¨Årst scheduling starting from e for all entities e
in the reference table; dfs-bottomup (DB), the depth-Ô¨Årst
scheduling starting from œÜ for all e; max-max (MAX), the
max-beneÔ¨Åt scheduling using max beneÔ¨Åt function; max-
min (MIN): the max-beneÔ¨Åt scheduling using min beneÔ¨Åt
function; max-avg (AVG), the max-beneÔ¨Åt scheduling using
avg beneÔ¨Åt function; multi-entity (ME): the multiple entity
scheduling algorithm; upper-bound (UB), the total number
of subsets; and lower-bound (LB), the total number of sub-
sets in the cut (deÔ¨Åned in Section 3.2).

We observe that multi-entity performs the best. It is close
to the optimal scheduling in that the number of web queries
is only 1.34 times of that in the lower bound. Compar-
ing to the upper bound, it prunes more than 80% of web
queries. Within the single entity scheduling, the depth-Ô¨Årst

 0.4 0.6 0.8 1 0.4 0.6 0.8 1PrecisionRecallCorr1Corr2Jaccard 0.2 0.4 0.6 0.8 1 0.2 0.4 0.6 0.8 1PrecisionRecallCorr1Corr2Jaccard 0.2 0.4 0.6 0.8 1 0.2 0.4 0.6 0.8 1PrecisionRecallCorr1Corr2Jaccard 50000 100000 150000 200000 250000 300000 350000UBLBMEDTDBMAXMINAVG#Web QueriesMethods#queries(cid:10) 10000 20000 30000 40000 50000 60000 70000 80000 90000 100000UBMEDTDBMAXMINAVG#Web QueriesMethods#queries(cid:10) 100 200 300 400 50020406080100#Web Queries (K)Entity Number (K)multi-entitybfs-topdown(cid:10)WWW 2009 MADRID!Track: Data Mining / Session: Web Mining158Figure 9: Number of IDTokenSet
w.r.t. Entity Size

Figure 10: Number of IDTokenSet
w.r.t. Entity Number

Figure 11: Query Performance

scheduling performs better than the max-beneÔ¨Åt scheduling.
Within the depth-Ô¨Årst scheduling, the top-down scheduling
strategy is better than the bottom-up scheduling strategy.
This matches the intuition that IDTokenSets generally share
more tokens with the original entities.

We also impose an additional constraint that each subset
has to appear as a contiguous substring in some documents
(as we discussed in Section 4.1). In order to evaluate the con-
straint, we still leverage the web search engine to retrieve
the relevant documents. However, sending each subset to
the web search engine is equally expensive as validating the
subset. Instead, we using the following heuristic to evalu-
ate the constraint. For each entity e, we will Ô¨Årst send e as
a query to the web search engine, and retrieve top-K doc-
uments (i.e., title, url and snippets), where K could be a
large number. We then evaluate the constraint by checking
whether a subsets of e appears contiguously in at least one
of these documents.

Figure 7 shows the performance by diÔ¨Äerent methods,
which is similar to that in the complete lattice case. Notice
that diÔ¨Äerent from the complete lattice case where we can
compute the lower-bound of the web API calls, we are not
able to obtain the lower-bound in the constrained case where
candidate subsets form a partial lattice structure. This is be-
cause the optimal solution may pick any subsets in the com-
plete lattice in order to prune subsets in the partial lattice.
The optimal solution is essentially a set covering problem,
which is NP-hard.

The last experiment in this subsection is to show the scal-
ability of the algorithms. We Ô¨Åx a mixed strategy such that
for all entities with no more than 5 tokens, we apply the
complete lattice model; and for entities with more than 5
tokens, we enforce the same constraint in Figure 7. Figure
8 shows the number of web queries by multi-entity and dfs-
topdown, with respect to diÔ¨Äerent number of input entities.
We observe that both methods are linearly scalable to the
number of input entities. The performance gain of multi-
entity is not signiÔ¨Åcant in our experiment. This is because
our entity database is rather diversiÔ¨Åed. However, it shows
a clear trend that multi-entity beneÔ¨Åts more by increasing
the entity number. By increasing the number of entities, the
grouping method is able to identify more entities which have
similar structure to each other. Hence, the multi-entity has
better chance to locate subsets in the Cut in scheduling.

6.3 Number of IDTokenSets

We pre-compute the IDTokenSet to support eÔ¨Écient ap-
proximate match on-the-Ô¨Çy.
It is important to show that
the IDTokenSets are within the manageable size. Figure 9
reports the average number of IDTokenSets per entity with
respect to diÔ¨Äerent entity size (e.g., number of tokens in the
entity). In each size group, we compute the number of IDTo-
kenSets for both the complete lattice case and partial lattice
case (using the same constraint in Figure 7). We observe
that the constraint is quite eÔ¨Äective in reducing the number
of IDTokenSets. Actually, the constraint is also meaningful
since not all IDTokenSets are conventionally used by users.
Figure 10 shows the number of IDTokenSets by increasing
the number of entities, using the same experimental conÔ¨Åg-
uration in Figure 8. In general, the number of IDTokenSets
is about 2-4 times of the entity number.
6.4 Performance on Entity Extraction

In the last experiment, we brieÔ¨Çy show the performance
on entity extraction, using the generated IDTokenSets. The
input string is a collection of 10, 000 documents. On aver-
age, each document contains 3, 689 tokens. The reference
entity table includes 200k entities, from which, we gener-
ate 592k IDTokenSets. We extract all substrings from docu-
ments with length up to 10, and apply set based exact match
(using a hash-table) over the IDTokenSets. We use the Ô¨Ålter
ideas discussed in Section 5 to prune non-interested sub-
strings. The experiments are conducted on a 2.4GHz Intel
Core 2 Duo PC with 4GB RAM, and the execution time is
reported in Figure 11. Our system is fairly eÔ¨Écient in that
it is able to process around 200 documents per second.

7. RELATED WORK

A number of techniques have been proposed for using dic-
tionary information in entity extraction. Cohen and Sarawagi
[10] exploited external dictionaries to improve the accuracy
of named entity recognition. Agrawal et al. [2] introduced
the ‚Äúad-hoc‚Äù entity extraction task where entities of interest
are constrained to be from a list of entities that is speciÔ¨Åc to
the task, and have also considered approximate match based
on similarity functions.

Approximate-match based dictionary lookup was stud-
ied under the context of string similarity search in appli-
cation scenarios such as data cleaning and entity extraction

 1 10 1002-45-78-10IDToken NumberEntity Size (#token)completepartial(cid:10) 50 100 150 200 250 30020406080100IDToken Number (K)Entity Number (K)synonym 0.01 0.1 1 10 10010100100010000Execution Time (Seconds)Document NumberLookupTime(cid:10)WWW 2009 MADRID!Track: Data Mining / Session: Web Mining159(e.g., [7, 8, 4]). All these techniques rely on similarity func-
tions which only use information from the input string and
the target entity it is supposed to match. In contrast, we de-
velop a new similarity scheme which exploits evidence from
a collection of documents.

Our work is related to synonym detection, which is the
problem of identifying when diÔ¨Äerent references (i.e., sets of
attribute values) in a dataset correspond to the same real
entity. Synonym detection has received signiÔ¨Åcant attention
in the literature [14]. Most previous literature assumed ref-
erences having a fair number of attributes. While additional
attributes or linkage structures may help provide extra evi-
dence, in this paper, we assume we are only given the list of
entity names which is usually the case.

An alternative approach for generating IDTokenSets could
be to segment the original entities into attributes. Con-
sider the entity ‚ÄúLenovo ThinkPad X61 Notebook‚Äù, where
the tokens can be tagged as follows: Lenovo (Brand Name),
ThinkPad (Product Line), X61 (Product Model) and Note-
book (Product Type). A rule based approach then may sug-
gest that Product Model is speciÔ¨Åc enough to refer to a prod-
uct, and hence ‚ÄúX61‚Äù is a valid IDTokenSet. This rule based
approach has two limitations. First, not all entities have a
clear schema for segmentation. For instance, one may not
be able to segment a movie name. Second, even for some en-
tities that can be segmented, the rule based approach is not
robust. Consider another product entity ‚ÄúSony Vaio F150
Laptop‚Äù, F150 will be tagged as product model. Hence the
rule based approach may conclude that ‚ÄúF150‚Äù is an IDTo-
kenSet of ‚ÄúSony Vaio F150 Laptop‚Äù. While actually, from
the common knowledge, ‚ÄúF150‚Äù is better known as a Ford
vehicle. In contrast, our techniques do not assume that each
entity is segmentable into attribute values, and we do not
assume the availability of a robust segmentation technique.
Turney [15] introduced a simple unsupervised learning al-
gorithm that exploits web documents for recognizing syn-
onyms. Given a problem word and a set of alternative words,
the task is to choose the member from the set of alternative
words that is most similar in meaning to the problem word.
In this paper, we focus on eÔ¨Éciently generating IDTokenSets
for a large collection of entities. The search space is signif-
icantly larger than that in [15], and we focus on methods
that minimize the generation cost.

The subset-superset monotonicity exploited in this paper
is related to the ‚Äúapriori‚Äù property used in many frequent
itemset mining algorithms [1, 13]. The diÔ¨Äerence is that the
subset-superset monotonicity prunes computation in two di-
rections. That is, if œÑ is a valid IDTokenSet, all œÑ ‚Äôs supersets
are pruned for validation; if œÑ is not a valid IDTokenSet,
all œÑ ‚Äôs subsets are pruned for validation. While in frequent
itemset mining, the ‚Äúapriori‚Äù property only prunes compu-
tation in one direction (e.g., if an itemset œÑ is not frequent,
only its supersets are pruned).

8. DISCUSSION AND CONCLUSIONS

In order to support fast and accurate approximate entity
match, we propose a new solution by exploiting IDTokenSet.
Our approach diÔ¨Äers from many previous methods in two
folders: Ô¨Årst, we pre-compute a list of IDTokenSets for each
entity. SpeciÔ¨Åcally, we use the document-based similarity
measure to validate IDTokenSets, and leverage web search
to retrieve related documents; and Second, we apply exact
set-based match over the IDTokenSets at matching phase.

This paper mainly focuses on the quality of the IDTokenSets,
as well as eÔ¨Écient algorithms in generating IDTokenSets.
We show the document-based measure is signiÔ¨Åcantly better
than the traditional string-based similarity measure. Several
algorithms are proposed to eÔ¨Éciently generate IDTokenSets
by pruning majority number of web queries.

We plan to explore several directions in future work. First,
we studied a document-based similarity measure in this pa-
per. One extension is to build a classiÔ¨Åer with features de-
rived from multiple documents.
Second, we will consider
an alternative computation model where the evidence doc-
uments are available for direct access (e.g., scan all docu-
ments). SpeciÔ¨Åcally, we will exploit batched processing by
simultaneously generating IDTokenSets for all entities.

9. REFERENCES
[1] R. Agrawal, T. Imielinski, and A. Swami. Mining

association rules between sets of items in large
databases. In SIGMOD Conference, pages 207‚Äì216,
1993.

[2] S. Agrawal, K. Chakrabarti, S. Chaudhuri, and

V. Ganti. Scalable ad-hoc entity extraction from text
collections. In VLDB, 2008.

[3] A. V. Aho and M. J. Corasick. EÔ¨Écient string

matching: An aid to bibliographic search. Commun.
ACM, 18(6):333‚Äì340, 1975.

[4] A. Arasu, V. Ganti, and R. Kaushik. EÔ¨Écient exact
set-similarity joins. In VLDB, pages 918‚Äì929, 2006.
[5] R. A. Baeza-Yates and B. A. Ribeiro-Neto. Modern
Information Retrieval. ACM Press/Addison-Wesley,
1999.

[6] K. Chakrabarti, S. Chaudhuri, V. Ganti, and D. Xin.

An eÔ¨Écient Ô¨Ålter for approximate membership
checking. In SIGMOD Conference, pages 805‚Äì818,
2008.

[7] A. Chandel, P. C. Nagesh, and S. Sarawagi. EÔ¨Écient

batch top-k search for dictionary-based entity
recognition. In ICDE, page 28, 2006.

[8] S. Chaudhuri, V. Ganti, and R. Kaushik. A primitive

operator for similarity joins in data cleaning. In ICDE,
page 5, 2006.

[9] T. Cheng, X. Yan, and K. C.-C. Chang. Entityrank:
Searching entities directly and holistically. In VLDB,
pages 387‚Äì398, 2007.

[10] W. W. Cohen and S. Sarawagi. Exploiting dictionaries

in named entity extraction: combining semi-markov
extraction processes and data integration methods. In
KDD, pages 89‚Äì98, 2004.

[11] X. Dong, A. Y. Halevy, and J. Madhavan. Reference

reconciliation in complex information spaces. In
SIGMOD Conference, pages 85‚Äì96, 2005.

[12] V. Ganti, A. C. K¬®onig, and R. Vernica. Entity

categorization over large document collections. In
KDD, pages 274‚Äì282, 2008.

[13] J. Han and M. Kamber. Data Mining: Concepts and

Techniques. Morgan Kaufmann, 2001.

[14] N. Koudas, S. Sarawagi, and D. Srivastava. Record

linkage: similarity measures and algorithms. In
SIGMOD Conference, pages 802‚Äì803, 2006.

[15] P. D. Turney. Mining the web for synonyms: Pmi-ir

versus lsa on toeÔ¨Ç. CoRR, cs.LG/0212033, 2002.

WWW 2009 MADRID!Track: Data Mining / Session: Web Mining160