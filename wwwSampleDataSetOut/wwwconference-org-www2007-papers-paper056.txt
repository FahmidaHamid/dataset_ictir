Robust Methodologies for Modeling Web Click

Distributions

Kamal Ali, Yahoo!
701 First Avenue

Sunnyvale, CA USA

kamal3@yahoo.com

ABSTRACT
Metrics such as click counts are vital to online businesses but
their measurement has been problematic due to inclusion of
high variance robot traÔ¨Éc. We posit that by applying sta-
tistical methods more rigorous than have been employed to
date that we can build a robust model of the distribution
of clicks following which we can set probabilistically sound
thresholds to address outliers and robots. Prior research
in this domain has used inappropriate statistical method-
ology to model distributions and current industrial prac-
tice eschews this research for conservative ad-hoc click-level
thresholds. Prevailing belief is that such distributions are
scale-free power law distributions but using more rigorous
statistical methods we Ô¨Ånd the best description of the data is
instead provided by a scale-sensitive Zipf-Mandelbrot mix-
ture distribution. Our results are based on ten datasets
from various verticals in the Yahoo domain. Since mixture
models can overÔ¨Åt the data we take care to use the BIC log-
likelihood method which penalizes overly complex models.
Using a mixture model in the web activity domain makes
sense because there are likely multiple classes of users. In
particular, we have noticed that there is a signiÔ¨Åcantly large
set of ‚Äúusers‚Äù that visit the Yahoo portal exactly once a day.
We surmise these may be robots testing internet connectiv-
ity by pinging the Yahoo main website.

Backing up our quantitative analysis is graphical analysis
in which empirical distributions are plotted against theoret-
ical distributions in log-log space using robust cumulative
distribution plots. This methodology has two advantages:
plotting in log-log space allows one to visually diÔ¨Äerentiate
the various exponential distributions and secondly, cumula-
tive plots are much more robust to outliers. We plan to use
the results of this work for applications for robot removal
from web metrics business intelligence systems.

Categories and Subject Descriptors
G.3 [Mathematics of Computing]: Probability and Sta-
tistics

General Terms
Measurement, Theory

Copyright is held by the International World Wide Web Conference Com-
mittee (IW3C2). Distribution of these papers is limited to classroom use,
and personal use by others.
WWW 2007, May 8‚Äì12, 2007, Banff, Alberta, Canada.
ACM 978-1-59593-654-7/07/0005.

Mark Scarr, Yahoo!

701 First Avenue

Sunnyvale, CA USA

mscarr@yahoo-inc.com

1.

INTRODUCTION

Modern businesses rely on accurate counts of web page-
views and clicks to calculate growth rates and market share.
Per-user page-views in the millions (per month) and other
suspicious statistics lead to the belief that a signiÔ¨Åcant amount
of traÔ¨Éc originates from robots. Failure to remove robots
can mislead businesses about their growth metrics. Fur-
thermore, we see higher temporal (month to month or day
to day) variance from the robot population so failure to re-
move them also degrades statistical power [20] in comparing
Ô¨Åelded systems to their beta counterparts.

In order to remove robots on a more principled basis, we
need to have a better characterization of the distribution of
click behavior. One method of removing robots is to iden-
tify them with outliers and remove outliers. Outlier removal
using distributional methods proceeds by Ô¨Åtting a model to
the observed distribution and then selecting a tail probabil-
ity (say 0.1%) to use as a deÔ¨Ånition of an outlier. Using the
model, we can then translate that probability into a statisti-
cally founded threshold of clicks and remove all ‚Äúusers‚Äù that
exceed that threshold. Currently, businesses use very con-
servative ad-hoc thresholds for robot removal. Knowing the
distribution more precisely would allow them to be more ag-
gressive in removing robots and thus produce more accurate
and stable metrics for business.

Most previous work in web traÔ¨Éc distribution modeling
has been done for network caching and relay applications [7,
3] - little to none has been done for web analytics. The work
that has been done did not appear to test a wide variety of
distribution families and some authors used continuous dis-
tributions [9, 1], which are not appropriate for discrete (click
and pageview) distributions. Finally, the work appears not
to have used rigorous statistical methodology. For instance,
many authors simply plot the observed distribution in log-
log space (log of frequency of x versus log(x)) and then pro-
ceed to Ô¨Åt a straight line, using Pearson‚Äôs correlation coef-
Ô¨Åcient [16] to measure goodness of Ô¨Åt. This is completely
incorrect because typically the points corresponding to low
pageview and click counts represent millions of users and
other points may only represent a single user. Thus, at the
very least, a weighted regression is called for. Even so, out-
liers can have large impact on regression Ô¨Åtting methods and
no indication was given that robust regression methods were
used. Instead of regression, we use the maximum likelihood
method (MLE) which is much more robust.

In our methodology, we try to Ô¨Åt a large number of dis-
tribution families to the data using the robust maximum-
likelihood estimation [16] (MLE) method. Then we evalu-

ate the goodness of Ô¨Åt using the (log) likelihood of the data
given the Ô¨Åtted distribution. Since some models have more
parameters that can overÔ¨Åt the data, we use the Bayesian In-
formation Criterion (BIC, [17]) correction to log-likelihood.
Essentially this measure requires that more complex mod-
els ‚Äúpay for their complexity‚Äù by providing a better Ô¨Åt to
the data.
In the case where we have nested models the
likelihood-ratio test [16] is also used to see what other distri-
butions are statistically indistinguishable from the winning
one - thereby forming a set of winners. Although inappropri-
ate for count data we also use two continuous distributions
(Inverse Gaussian [9], log-normal [10]) that have been used
in prior work so as to compare their scores to the theoreti-
cally correct discrete distributions.

Graphical methods (plotting the data) are a great aid in
statistics because in two dimensions the human eye is great
at pattern detection. However,
in distribution modeling
where some points have drastically diÔ¨Äerent weight than oth-
ers, the plots can be quite misleading. Nevertheless, proper-
ties such as curvature can be discerned from plots and this
is important for this paper since if the observed distribution
has a curved form in log-log space, it favors scale-sensitive
models over scale-free power law distributions.

The rest of the paper is organized as follows: section 2
summarizes previous work in modeling distributions for the
web and other data sets with various discrete distributions
(ZipÔ¨Åan, Poisson, Negative Binomial) and simple mixture
models. Section 3 gives density functions for the candidate
distributions we use in this paper. Section 4 spells out our
approach for parameter estimation and model comparison,
section 5 describes the ten datasets and section 6 presents
results for the various Ô¨Åtted distributions.

2. PREVIOUS WORK

Power-laws, Zipf distributions and Pareto distributions
have become somthing of a fad recently, being very pop-
ular in explaining all manner of data (city sizes, galaxy sizes
[10], words: [23], incomes [21]). We begin by carefully dis-
tinguishing these. The term ‚Äúpower law‚Äù is inappropriately
general for our application since it goes well beyond distrib-
utions to describe any functional relationship between y and
x where y = axk, hence we will not use this term. Zipf dis-
tributions are discrete distributions over a Ô¨Ånite set (1...N )
with probability mass function f (x; s, N ) = ‚Ñ¶(N ) 1
xs where
‚Ñ¶(N ) = 1/PN
1
is is a normalizing constant. For s > 1 this
distribution is normalizable even if N = ‚àû and becomes the
Zeta distribution [19]. Some authors restrict the term ‚ÄúZipf
distribution‚Äù for the case s = 1 and call other cases ‚ÄúZipf-
like‚Äù or ‚ÄúZipÔ¨Åan‚Äù. Zipf distributions (s = 1) such as word
distributions, have the scale-free property:

i=1

Scale-free: A distribution f is scale free if for
all values of x, the probability of 2x is half the
probability of x.
If a distribution is not scale-
free, we will term it as scale-sensitive.

Applied to word rankings this means that for all ranks,
the probability of seeing a word at that rank is twice the
probability of seeing a word with twice the rank (Zipf‚Äôs law
[23]). Pareto [21] distributions are continuous analogues of
the Zipf, with density given by f (x; k) = k 1

Previous work can be classiÔ¨Åed according to the types of
distributions that were tried and also more subtly, the mean-
ing of the x axis: in some work, the x axis refers to the value

xk+1 .

of a random variate (e.g. number of clicks) whereas in others
it refers to a rank of that variable. Table 1 summarizes pre-
vious work with respect to this classiÔ¨Åcation. Laherrere and
Sornette [10] take yet another approach, modeling rank as
a function of the random variate. The Inverse-Gaussian [9],
Log-Normal [10] and Weibull [10] distributions are continu-
ous distributions yet they are being used to model discrete
data.

The paper of Huberman et al.

[9] claims to have dis-
covered a ‚Äústrong law of surÔ¨Ång‚Äù: that the distribution of
clicks is distributed as Inverse Gaussian (IG). However, we
believe this claim to be too strong. In particular, for our
datasets, we have found other distributions to oÔ¨Äer statis-
tically signiÔ¨Åcantly better Ô¨Åt to the data than the Inverse
Gaussian. We doubt there is a single distribution that will
Ô¨Åt all kinds of web surÔ¨Ång let alone constitute a ‚Äúlaw of
surÔ¨Ång‚Äù. Huberman also claims the Inverse Gaussian has
theoretical motivation and argues that the utility of a web
surfer mirrors that of economic options whose prices are
known to follow an Inverse-Gaussian distribution. They also
show that the page-view distribution of URLs is Ô¨Åtted by an
Inverse-Gaussian. Their approach seems not to have tried
a lot of distributions, let alone discrete ones - it seems only
that they have considered the Inverse Gaussian because of
its theoretical underpinning and then proceeded to see if it
gives a good enough Ô¨Åt to the data.

Huberman‚Äôs work is the only other we are aware of that
points out the curvature of the distribution in log-log space
thus deprecating the scale-free power law distributions. Pre-
vious authors did not rule out better Ô¨Åts by scale-sensitive
curved forms in log-log space: they only demonstrated that
they got a good-enough Ô¨Åt by a line in log-log space and
thus concluded the distribution must be power-law.

Laherrere and Sornette justify their choice of Weibull be-
cause ‚Äútails of pdfs of products of a Ô¨Ånite number of random
variables is generically a stretched exponential‚Äù [6]. How-
ever, it appears to us that there is a serious problem with
their methodology. They evalute goodness of Ô¨Åt in log-log
space using Pearson‚Äôs correlation coeÔ¨Écient which is incor-
rect since the plotted points with lower rank represent many
more points than those with higher ranks.

Finally, a note on previous work in mixture and zero-
adjusted models. User web behavior can be naturally parti-
tioned based on the presence or absence of a click, in other
words zero click vs. non-zero click behavior. The zero click
class might be thought of as primarily containing robot traf-
Ô¨Åc, and the non-zero click class principally ‚Äúhuman‚Äù traÔ¨Éc.
At the very least, if we see that a model Ô¨Åts the data well
except at the zero point, we can conclude that the excess
of users with zero clicks may be due to a secondary phe-
nomenon: that of robots. This is the approach of mixture
models in which the zero class is modeled by a (single-valued
degenerate) distribution and the positive clicks are modeled
by a discrete positive-valued distribution. Such models are
called zero-altered or zero-inÔ¨Çated and have been used in
numerous applications to model data with an excess of ze-
roes or where the zero-class of the underlying process has
special meaning (number of defects [11], number of dental
cavities [14], and number of car crashes [12]). In all of these
domains, zero has a special meaning in that it is usually the
default scenario indicating lack of an accident or problem.

Best-Ô¨Åt distribution
Zipf k = 1

Measure
Frequency
Population Weibull
Page-views Zipf-like, various k

Entity
Word
City
URL
Search Queries Frequency Weibull
URL
Session
Session
User

Page-views
Clicks
Clicks
Page-views ZAZM

X-axis
Rank
Population Rank
Page-views Frequency Breslau [3]
Rank
Page-views Frequency Huberman et al. [9]
Inverse Gaussian
Frequency Huberman et al. [9]
Inverse Gaussian
Clicks
ZAZM: Zero-altered Zipf-Mandelbrot Clicks
Frequency
Page-views Frequency

Y-axis
Frequency Zipf [23]

Frequency Abdullah [1]

Scarr, Ali (this paper)
Scarr, Ali (this paper)

Laherrere, Sornette [10]

Author

Table 1: Prior work in distribution modeling can be partitioned into those modeling the random variate x
versus those modeling the rank of x. The web papers diÔ¨Äer subtly in that some are modeling clicks per user
or clicks per session or searches per query.

3. DISTRIBUTION THEORY

Since this paper is concerned with modeling count data,
in the form of user clicks, we limit our attention to discrete
distributions; in particular the Negative Binomial [2], Zipf
[23], Zipf-Mandelbrot [13] (of which Zipf is a special case),
Logarithmic Series [22] and Yule-Simon [18]. The Poisson
distribution which best Ô¨Åts data when its mean and variance
are equal is not used due to the fact our click data (table 2)
have variance-to-mean ratios far in excess of 1. The Inverse
Gaussian [9], Weibull [10] and Log-Normal [10] distributions,
which are continuous, are only included for comparison as
they have been used to model web click behavior in the past.
In addition a class of simple mixture models are also con-
sidered, motivated by the fact that zero-click users may be-
have diÔ¨Äerently to non-zero click users. A number of ‚Äúusers‚Äù
are actually robots that ping the Yahoo site every day to test
if they are connected to the internet. This is a diÔ¨Äerent gen-
erative phenomenon than that of regular human search and
hence justiÔ¨Åes using a mixture model. Zero-altered or zero-
inÔ¨Çated models [8, 11, 14] for count data are mixture models
that assume with probability p the only possible observation
is 0 and with probability 1‚àí p a discrete random variable is
observed.

The zero-altered model mixes a degenerate distribution
with point mass of 1 at zero with a truncated count distri-
bution for example truncated Poisson, truncated Negative
Binomial or any discrete distribution bounded below by 1.
On the other hand the zero-inÔ¨Çated model accounts for some
of the zeros through the non-degenerate distribution ( e.g.
Poisson, Negative Binomial, etc...) and some through the
degenerate (zero) distribution.

We now present formulae for the probability mass and
density functions of the various distributions discussed above,
by considering a random variable X.

Zero-Altered (ZA):
The probability mass or density function for a zero-altered

mixture model is deÔ¨Åned as:

fX (x; p, Œ∏Œ∏Œ∏) = 8<
:

0
x < 0
p
x = 0
(1 ‚àí p)fY (x; Œ∏Œ∏Œ∏) x > 0

(1)

where 0 ‚â§ p ‚â§ 1, Œ∏Œ∏Œ∏ = (Œ∏1, Œ∏2, . . . , Œ∏k) is a vector of para-
meters and fY (x; Œ∏Œ∏Œ∏) is any valid probability distribution on
[1,‚àû). From the above, the random variable X takes the
value zero with probability p and values greater than zero
with probabilities (1 ‚àí p)fY (x; Œ∏Œ∏Œ∏).

Zero-InÔ¨Çated (ZI):
The probability mass or density function for a zero-inÔ¨Çated

mixture model is deÔ¨Åned as:

fX (x; p, Œ∏Œ∏Œ∏) = 8<
:

0
x < 0
p + (1 ‚àí p)fY (0; Œ∏Œ∏Œ∏) x = 0
(1 ‚àí p)fY (x; Œ∏Œ∏Œ∏)
x > 0

(2)

is any valid probability distribution on [0,‚àû).

where 0 ‚â§ p ‚â§ 1, Œ∏Œ∏Œ∏ is a vector of parameters and fY (x; Œ∏Œ∏Œ∏)
Negative Binomial (NB) (discrete): X ‚àº N B(k, ¬µ),

the probability mass function is:

k

(

Œì(x + k)

Œì(k)Œì(x + 1)

)k(1‚àí k

)x, x ‚â• 0 (3)
f (x; k, ¬µ) =
where k > 0 and ¬µ ‚â• 0. To Ô¨Åt a ‚Äòzero-altered‚Äù Negative
Binomial model (ZANB) the probability mass function of a
zero-truncated Negative Binomial distribution is required,
this is deÔ¨Åned as:

¬µ + k

¬µ + k

f0(x; k, ¬µ) =

f (x; k, ¬µ)
1 ‚àí f (0; k, ¬µ)

, x > 0

(4)

From equations (1, 3, 4) the Zero-Altered Negative Bino-

mial ZAN B(p, k, ¬µ) probability mass function is:

f (x; p, k, ¬µ) = 8><
>:

0
p
(1 ‚àí p)

¬µ+k )k(1‚àí k
Œì(x+k)( k
Œì(k)Œì(x+1)(1‚àí( k

¬µ+k )x
¬µ+k )k)

x < 0
x = 0

x > 0

(5)
Zipf (Z) (discrete): X ‚àº Z(s, N ), the probability

mass function is:

f (x; s, N ) = ‚Ñ¶(N )

1

xs , x ‚àà (0, N ]

(6)

normalising constant.

where s > 0, N is Ô¨Ånite and ‚Ñ¶(N ) = 1/PN
1
is is a
Zipf-Mandelbrot (ZM) (discrete): X ‚àº ZM (s, q, N ),

i=1

the probability mass function is:

1

f (x; s, q, N ) = ‚Ñ¶(q, N )

(x + q)s , x ‚àà [0, N ]
where s > 0, q ‚â• 0, N is Ô¨Ånite and ‚Ñ¶(q, N ) = 1/PN

is a normalising constant. Clearly, setting q = 0 yields the
Zipf distribution in equation (6) so non-zero q indicates cur-
vature in log-log space.
Logarithmic-Series (LS) (continuous): X ‚àº LS(k),

(7)

1

(i+q)s

i=1

the probability mass function is:

f (x; k) =

(8)
Yule-Simon (YS) (discrete): X ‚àº Y S(k), the prob-

log(1 ‚àí k)

, x > 0, k ‚àà (0, 1)

‚àí1

kx
x

ability mass function is:

f (x; œÅ) = œÅB(x, œÅ + 1), x > 0, œÅ > 0

(9)

where B(, ) is the Beta function.
Inverse-Gaussian (IG) (continuous): X ‚àº IG(Œª, ¬µ),

the probability density function is:

f (x; Œª, ¬µ) = r Œª

2œÄx3 exp(

‚àíŒª(x ‚àí ¬µ)2

2¬µ2x

), x ‚â• 0

(10)

with , Œª, ¬µ > 0.
Log-Normal (LN) (continuous): X ‚àº LN (¬µ, œÉ), the

probability density function is:

xœÉ

2œÉ2

‚àö
1
2œÄ

exp‚àí (log(x) ‚àí ¬µ)2

 , x ‚àà R (11)
f (x; ¬µ, œÉ) =
with ¬µ ‚àà R and œÉ > 0. The Log-Normal distribution is
the probability distribution of any random variable whose
logarithm possesses a Normal or Gaussian distribution. In
other words if Z ‚àº N (¬µ, œÉ2) is a Normally distributed ran-
dom variable then X = eZ has a Log-Normal distribution.
Weibull (W) (continuous): X ‚àº W (Œ±, Œ≤), the prob-

ability density function is:

f (x; Œ±, Œ≤) =

Œ±
Œ≤

(

x
Œ≤

)Œ±‚àí1 exp(‚àí(

x
Œ≤

)Œ±), x ‚â• 0, Œ±, Œ≤ > 0 (12)

Zero-altered or zero-inÔ¨Çated models can be constructed
using equations (1, 2) and any of the probability mass or
density functions above. Since we are particularly interested
in the zero class, zero-altered models only are used for those
distributions where x > 0, such as Zipf, Logarithmic series
etc...

4. METHODOLOGY

We now describe the method used to Ô¨Åt the distributions
proposed in section 3 to our observed data. In addition we
also discuss how to compare diÔ¨Äerent Ô¨Åtted models. Maxi-
mum likelihood estimation e.g. [16, 5], a standard statistical
modeling technique, is used to Ô¨Åt the models. It possesses a
number of desirable properties and is a widely used parame-
ter estimation tool. Once various models have been Ô¨Åtted
they can be compared using the Bayesian Information Cri-
terion (BIC) [17] and graphical methods such as Cumulative
Distribution plots.
4.1 Maximum Likelihood Estimation

Suppose we have an independant and identically distrib-
uted (i.i.d.) sample XXX = (X1, X2, . . . , Xn) with joint prob-
ability density or mass function f (xxx|Œ∏Œ∏Œ∏), where the data xxx =
(x1, x2, . . . , xn) and the parameter vector Œ∏Œ∏Œ∏ = (Œ∏1, Œ∏2, . . . , Œ∏k).
Given observed values Xi = xi, i = 1, . . . , n the likelihood
is:

L(Œ∏Œ∏Œ∏|xxx) =

f (xi|Œ∏Œ∏Œ∏)

n

Yi=1

(13)

In the case of discrete data the likelihood measures the
probability of observing the given data as a function of Œ∏Œ∏Œ∏.
The maximum likelihood estimate (MLE) ÀÜŒ∏ÀÜŒ∏ÀÜŒ∏, is the value of Œ∏Œ∏Œ∏
that maximises the likelihood i.e. makes the observed data
‚Äúmost likely‚Äù. In practice it is usually easier to equivalently
maximize the log-likelihood:

l(Œ∏Œ∏Œ∏|xxx) =

n

Xi=1

log{f (xi|Œ∏Œ∏Œ∏)}

(14)

As an example, consider an i.i.d. sample from a Poisson

distribution with parameter Œª, then:

f (xi|Œª) =

Œªxi e‚àíŒª

xi!

‚àÄi = 1, . . . , n

From equation (13) the likelihood is:

L(Œª|xxx) =

Œªxi e‚àíŒª

xi!

n

Yi=1

(15)

(16)

Since it is easier to work with the log-likelihood, from equa-

tion (14) we have:

l(Œª|xxx) = log(Œª)

n

Xi=1

xi ‚àí nŒª ‚àí n
Xi=1

log(xi!)

(17)

DiÔ¨Äerentiating the log-likelihood in equation (17) with re-

spect to Œª and setting to zero gives:

0

l

(Œª) =

1
Œª

n

Xi=1

xi ‚àí n = 0

(18)

Expressing equation (18) in terms of Œª gives the familiar

MLE ÀÜŒª = x.

In the above example, we maximized the log-likelihood
with respect to a single parameter Œª. More generally, the
model may contain several parameters, in which case we
compute partial derivatives and set each in turn to zero. De-
pending on the particular distribution used in the likelihood,
a closed form solution may or may not exist. For the Pois-
son distribution the MLE of Œª has an algebraic solution that
is the sample mean of the data. In cases where there is no
closed form solution an iterative method is employed using a
modiÔ¨Åcation of the Broyden, Fletcher, Goldfarb and Shanno
quasi-Newton algorithm [4] within the statistical software
package R [15].

Although not discussed in detail here it is also possible
to compute variances based on the Fisher Information [16]
and hence conÔ¨Ådence intervals for the maximum likelihood
estimates described above.
4.2 Comparing Models

The log-likelihood in equation (14) can be computed for
the various parametric models of interest and used as a ba-
sis for model comparison. This makes sense as the model
that has the largest log-likelihood is considered to be the
most ‚Äúlikely‚Äù given the observed data. However since we
are investigating models with diÔ¨Äering numbers of parame-
ters, rather than comparing log-likelihoods directly we use
the Bayesian Information Criterion (BIC) [17]. The BIC
penalizes models with more parameters so that to ‚Äúwin‚Äù in

Property
X-axis
Number records
Mean
Median
Variance
Variance to Mean Ratio
Minimum #Clicks
Maximum #Clicks

Session-level data User-level data
Clicks/session
1,411,298
2.78
1
22.94798
8.24
0
499

Clicks/user
22,913,854
2.29
0
75.84359
34.03
0
5,428

Table 2: Summary of our data sets.

a BIC-sense, the extra parameter needs to justify its addi-
tion with a commensurate increase in log-likelihood.

The Bayesian Information Criterion (BIC) [17] is deÔ¨Åned

as:

BIC = ‚àí2l(Œ∏Œ∏Œ∏|xxx) + k log(n)

(19)
where l(Œ∏Œ∏Œ∏|xxx) is the maximized log-likelihood, n is the num-
ber of observations and k = |Œ∏Œ∏Œ∏| is the number of model pa-
rameters. We wish to minimize the BIC with respect to the
estimated model parameters. As can be seen from equation
(19) the BIC attaches a penalty to the addition of extra pa-
rameters, forcing it to prefer lower order models especially
for large n.
4.3 Graphical Methods

In the related work section we argued why linear Ô¨Åtting
in log-log space where one axis denotes frequency is poor
methodology. In this section we describe the beneÔ¨Åts and
pitfalls of the Cumulative-Probability (Cum-Prob) plot.

Cum-Prob plots:

A Cumulative-Probability (cum-
prob) plot is related to a QQ plot. To compute the x coordi-
nate of the i‚Äôth point, the area under the histogram ( e.g. of
clicks) up to and including the i‚Äôth distinct value is divided
by the full area under the histogram. In other words, x val-
ues are empirical CDF (Cumulative Distribution Function)
values. The y values are obtained by Ô¨Årst doing a MLE best-
Ô¨Åt but this time referring to the theoretical CDF. Let ci be
the i‚Äôth distinct click value over the set C of all possible click
values, then CDF is a function taking random click values
to the zero-one set: CDF : C ‚Üí [0,1]. Thus both x and y
values are normed to the [0,1] interval.

5. DESCRIPTION OF DATASETS

We show results for our two primary datasets: their basic
parameters are in table 2. The Ô¨Årst dataset is for our main
web search engine: a random sample of sessions collected
from a week‚Äôs worth of data. A session is terminated by
the standard 30 minutes of inactivity. The second dataset
is a random sample of per user clicks, integrated over one
month of activity for that user on another Yahoo website.
Thus dataset 2 consists of a mixture of sessions.

Figures 1 and 2 show histograms of the data. We have
zoomed in to the top 10 click values since the rest of the
histogram has very low values. Note that for the per-session
web search dataset, one-click occurs more frequently than
any other value and in particular, more than zero clicks. A
zero-click session is possible since a session can have pageviews
but zero clicks. If one were to use an exponential discrete
model, one might expect the zero probability to be higher

Figure 1: Sessionized data: frequency of web-search
clicks per-session (top 10 values).

Figure 2: User-level data: frequency of Yahoo web-
site clicks per-user (top 10 values).

than the probability for one click. Thus the data indicate
immediately the need to try for a mixture model with a
special component at zero - so-called Zero-Adjusted mix-
ture models. For the per-user month-level dataset (Ô¨Ågure
2), zero clicks are more common than one click. Note that
the user-level data is from another website (not web search)
so it is not as if multiple sessions from Ô¨Ågure 1 are being
included in Ô¨Ågure 2.

Ten other datasets from various other Yahoo verticals1
were also analyzed to see if our results are generalizable. In
order to get verticals that span the gamut of user behavior

1Verticals are websites such as mail.yahoo, travel.yahoo etc.

012345678910Histogram of observed data, top 10 groups0   e+002   e+054   e+05012345678910Histogram of observed data, top 10 groups0.0 e+004.0 e+068.0 e+061.2 e+07we sorted all the websites at Yahoo by volume of traÔ¨Éc and
then randomly picked a website in each decile of the sorted
list.

6. RESULTS

We begin by examining both datasets in log-log space (Ô¨Åg-
ures 3 and 4). Log-log space brings out diÔ¨Äerences in the
various exponential distribution families that are just not
apparent in usual histograms because all such distributions
have long tails. The other advantage of log-log space is
that curvature can be easily visually spotted and this usu-
ally indicates the data is not from a scale-free distribution
(scale-free has linear form in log-log space).

Figure 4: Per-user clicks (integrated over 1 month)
for another large Yahoo website exhibit curvature
in log-log space.

The main three results of our paper are supported by these

results:

‚Ä¢ Curvature:

In log-log space our data is curved as
indicated by the fact that the best Ô¨Åtting distribution,
Zipf-Mandelbrot, by theory has a curved form in log-
log space. This is also visually conÔ¨Årmed. Another
way to quantify the amount of curvature is to examine
the conÔ¨Ådence interval around the MLE (maximum
likelihood estimate) of the q parameter in the Zipf-
Mandelbrot distribution. For web search, we obtained
q = 2.0¬±0.01 and for the per-user dataset we obtained
q = 0.9 ¬± 0.002. Thus in both cases, q = 0 (which
would indicate no curvature) is emphatically excluded.
Curved forms in log-log space do not have the scale-
free property of pure ZipÔ¨Åan (power-law) distributions
- they have a natural, distinguished scale. ( e.g. [10]).
‚Ä¢ ZAZM: The particular model form with best BIC Ô¨Åt
is the ZAZM (Zero-Adjusted Zipf-Mandelbrot) model
for both datasets.

‚Ä¢ Zero-Altered Mixture: Zero-altered mixture mod-
els do better than their non-mixture counterparts, ir-
respective of the distribution type. This is consistent
with our hypothesis that a diÔ¨Äerent generative compo-
nent (robots) are major contributors to the zero-click
observations.

6.1 Results on ten other websites

Next, we examine the ten other datasets (table 3). First
note the (Log-Likelihood) LL column: this shows that the
best Ô¨Åt for each website was obtained by the Zero-Altered
Zipf-Mandelbrot mixture - not any other mixture. Second,

Figure 3: Per-session clicks for search engine results
exhibit curvature in log-log space. Observations are
dark asterisks; Ô¨Åtted models are curves or lines.

The Ô¨Ågures show the distribution of data, a non-mixture
Zipf-Mandelbrot, a mixture Zipf-Mandelbrot and a mixture
Zipf - we cannot Ô¨Åt a non-mixture Zipf since it is undeÔ¨Åned
for x = 0. The Ô¨Ågures also show a ‚Äúlinear Ô¨Åt‚Äù regression line.
This line is obtained by doing linear regression on the plot-
ted points. Note how it provides a poor Ô¨Åt for points near
log(clicks) = 0 which are just the points that represent mil-
lions of users (or sessions). This is the fundamental problem
with doing line-Ô¨Åtting of the plotted points: all points are
equally treated whereas in reality some points correspond to
millions of users and others may correspond to a single user.
Although visual examination of the data seems to indi-
cate curvature, a more rigorous conÔ¨Årmation would be to
Ô¨Åt models assessed by BIC values (Ô¨Ågures 5 and 6).
If
the winning model has, by theory, a curved form in log-
log space, then it would conÔ¨Årm the visual observation. The
Ô¨Ågures show the smallest (best Ô¨Åtting) BIC values consis-
tently come from the zero-altered mixture models, with zero-
altered Zipf-Mandelbrot (ZAZM) being the best Ô¨Åt for our
data.

**********************************************************0123402468Log‚àílog plot of observed dataZAZ(red), ZM(blue), ZAZM(green)log( #clicks )log(freq)**********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************02468051015Log‚àílog plot of observed data with ZAZ, ZM, ZAZM and Linear fitted modelslog( #clicks )log(freq)(ZAZ)(ZM)(ZAZM)Linear FitDecile Best parsimonious Best Ô¨Åt

1
2
3
4
6
7
9

Ô¨Åt (BIC)
ZAZM
ZAZ
ZAZM
ZAZ
ZAZ
ZAZ
ZAZ

Sample
size

(LL)
ZAZM 22,913,854
ZAZM 239,520
ZAZM 160,137
ZAZM 21,033
ZAZM 6,461
ZAZM 6,724
ZAZM 294

Table 3: Best Ô¨Åtting models for Yahoo websites cho-
sen to span all Yahoo websites in terms of user vol-
ume. Even though deciles 5, 8 and 10 had plenty of
data, they had too few distinct values for number of
clicks to allow Ô¨Åtting of parameterized models.

is an adjusted measure of Ô¨Åt) one would never lose (Ô¨Åtting-
wise) by using ZAZM (this is clear when one recalls that the
ZAZ model is just a special case of the ZAZM model).
6.2 Graphical Results

Having established that, at least for our datasets, the Zipf-
Mandelbrot (and its Zero-Altered counterpart) oÔ¨Äer a very
good Ô¨Åt2, we now examine supporting graphical methods:
the Quantile-Quantile (QQ) plot and Cumulative-Probability
(Cum-Prob) plot.

Figure 7 shows the cum-prob plot for the winning ZAZM
model on the web per-session data. The cumulative distrib-
ution plots are less vulnerable to outliers because they plot
areas rather than values of the random variates themselves.
An outlier has a large value but typically does not constitute
a large percentage of the overall mass (clicks summed across
all users). Hence its eÔ¨Äect is reduced in the cum-prob plot.
We can re-examine graphically our comparison between
the best BIC scoring distribution (ZAZM - Figure 7) and
one that was not competitive (Inverse Gaussian - Figure 8).
One can clearly see the ZAZM provides a good Ô¨Åt between
theory and observation whereas the Inverse Gaussian touted
by [9] as providing a strong law of surÔ¨Ång does not hold for
Yahoo web-search data.

Figures 9 through 14 provide graphical conÔ¨Årmation of the
LL/BIC results. ZAZM is the best Ô¨Åt with ZAZ being the
runner-up and the other doing much less well. The inappro-
priate continuous distributions (Inverse Gaussian, Weibull
and Log-Normal) are included here only because they have
been used in prior literature [9, 10, 1], which did not in-
vestigate Zipf-Mandelbrot or mixtures. Negative-Binomial
(NB) is included because clicks are discrete counts and NB
is often used for over-dispersed (variance-to-mean ratio > 1)
data. Log-Normal is shown because it is the Ô¨Årst distribu-
tion people think of when they think of skewed data (and
it also has a curved form in log-log space). However, it is
not discrete and it did not perform nearly as well as the
Zipf-Mandelbrot.

2Note we can never say for sure that the data originate from
a particular form of distribution, only that of the distribu-
tions we have tried, such and such a distribution provides
the best Ô¨Åt.

Figure 5: Sessionized web data BIC Ô¨Åt values (low
values are better): Zero-alteration makes a big dif-
ference and within that class, the Zipf-Mandelbrot
is the best.

Figure 6: User-level BIC values (lower is better):
Zero-altered models (ZA..) do better than zero-
inÔ¨Çated (ZI..) or unadjusted (e.g. ZM). Within the
zero-adjusted set, Zipf-Mandelbrot does the best.

note that for many of these minor websites, BIC indicates
the addition of a parameter in going from Zipf to Zipf-
Mandelbrot is not ‚Äúworth it‚Äù. In other words, some of these
websites do have a linear form in log-log space. However,
even in these cases, the best Ô¨Åt per se (measured by LL) is
provided by ZAZM. And since LL measures pure Ô¨Åt (BIC

Bayesian Information Criteria (BIC) for different models(BIC)Modelzawbnbzizmzmzalnzaigzazzayszanbzalszazm5800000600000062000006400000Bayesian Information Criteria (BIC) for different models(BIC)Modelzawbzalnnbzaigzanbzalszazzizmzmzayszazm7.6 e+077.8 e+078.0 e+078.2 e+07Figure 7: Web session data: Cumulative distri-
bution plot. Winning model: Zero-Altered Zipf-
Mandelbrot.

Figure 9: Second dataset: Cumulative distribu-
tion plot. Winning model: Zero-Altered Zipf-
Mandelbrot.

Figure 8: Web session data: Cumulative distribu-
tion plot. Losing model from strong law of surÔ¨Ång:
Zero-Altered Inverse Gaussian.

Figure 10: Second dataset: Drop the Mandelbrot
correction (Zero-Altered Zipf ).

************************************************************************************************************************************************************************************0.20.40.60.81.00.20.40.60.81.0Observed vs. expected cumulative probs ZAZM p, (q,s) independent(the line y=x included for comparison)ObservedExpected************************************************************************************************************************************************************************************0.20.40.60.81.00.20.40.60.81.0Observed vs. expected cumulative probs ZAIG (the line y=x included for comparison)ObservedExpected***********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************0.60.70.80.91.00.60.70.80.91.0Observed vs. expected cumulative probs ZAZM p, (q,s) independent(the line y=x included for comparison)ObservedExpected***********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************0.60.70.80.91.00.60.70.80.91.0Observed vs. expected cumulative probs ZAZ(the line y=x included for comparison)ObservedExpectedFigure 11:
inappropriate
Gaussian.
man‚Äôs strong law of surÔ¨Ång.

Second dataset: A losing model:
continuous Zero-Altered Inverse
Inverse Gaussian was used in Huber-

Figure 13: Second dataset: A losing continuous
model: Zero-Altered Weibull. Weibull was used by
Laherrere et al..

Figure 12: Second dataset: A losing discrete model
with over-dispersion (Zero-Altered Negative Bino-
mial). Negative-Binomial is a reasonable guess since
it is discrete and has long tail.

Figure 14: Second dataset: Another losing contin-
uous model: Zero-Altered Log-normal. Log-normal
is one of the simplest models that has curved form
in log-log space but it is continuous.

***********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************0.60.70.80.91.00.60.70.80.91.0Observed vs. expected cumulative probs ZAIG (the line y=x included for comparison)ObservedExpected***********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************0.60.70.80.91.00.60.70.80.91.0Observed vs. expected cumulative probs ZANB p, m independent(the line y=x included for comparison)ObservedExpected***********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************0.60.70.80.91.00.60.70.80.9Observed vs. expected cumulative probs ZAWB(the line y=x included for comparison)ObservedExpected***********************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************************0.60.70.80.91.00.60.70.80.9Observed vs. expected cumulative probs ZALN(the line y=x included for comparison)ObservedExpected7. CONCLUSIONS

Prevailing wisdom is that the distribution of web clicks
and pageviews follows a scale-free power law distribution.
However, we have found that a statistically signiÔ¨Åcantly
better description of the data is the scale-sensitive Zipf-
Mandelbrot distribution and that mixtures thereof further
enhances the Ô¨Åt. Previous analyses have three disadvan-
tages: they have used a small set of candidate distributions,
analyzed out-of-date user web behavior (circa 1998) and
used questionable statistical methodologies. Although we
cannot preclude that a better Ô¨Åtting distribution may not
one day be found, we can say for sure that the scale-sensitive
Zipf-Mandelbrot distribution provides a statistically signiÔ¨Å-
cantly stronger Ô¨Åt to the data than the scale-free power-law
or Zipf on a variety of verticals from the Yahoo domain.
The distribution has a deÔ¨Ånite curved form in log-log space
which in turn indicates it is not scale free.

Secondly, we have shown that better results are obtain-
able using a mixture model which treats the zero-class as
special. This is warranted because the generative process
of zero clicks might contain a signiÔ¨Åcant proportion of ro-
bot ‚Äúusers‚Äù and thus would be diÔ¨Äerent than the generative
process for non-zero clicks (containing mostly human users).
Since we have compared zero-adjusted mixture models to
non-mixture models we have taken care to use the BIC log-
likelihood scoring method since it makes some adjustments
for varying complexity of the models.

Finally, we have argued that the practice of Ô¨Åtting plotted
points in log-log space is incorrect methodology and is sen-
sitive to outliers. We instead propose using the Cumulative-
Probability plots which plot empirical cumulative distribu-
tions against theoretical cumulative distributions. We plan
to use the thresholds resulting from these methods to set
probabilistically founded threshold levels for removing out-
liers and robots and thus to enjoy more stable and accurate
metrics.

8. REFERENCES
[1] G. Abdulla. Analysis and Modeling of World Wide

Web TraÔ¨Éc. PhD thesis, Virginia Tech, 1998.

[2] A. Agresti. Categorical Data Analysis. Wiley Series in

Probability and Statistics, 2002.

[3] L. Breslau, P. Cao, L. Fan, G. Phillips, and

S. Shenker. Web caching and zipf-like distributions:
Evidence and implications. In INFOCOM (1), pages
126‚Äì134, 1999.

[4] R. H. Byrd, J. Nocedal, and C. Zhu. A limited

memory algorithm for bound constrained
optimization. Journal of ScientiÔ¨Åc Computing
(SIAM), 16:1190‚Äì1208, 1995.

[5] G. Casella and R. L. Berger. Statistical Inference.

Duxbury Press, 1990.

[6] U. Frisch and D. Sornette. Extreme deviation and

applications. J. Phys. I France 7, 7:1155‚Äì1171, 1997.

[7] S. Glassman. A caching relay for the World Wide

Web. Computer Networks and ISDN Systems,
27(2):165‚Äì173, 1994.

[8] D. C. Heilbron. Zero-altered and other regression

models for count data with added zeroes. Biometrics,
36:531‚Äì547, 1994.

[9] B. A. Huberman, P. L. T. Pirolli, J. E. Pitkow, and
R. M. Lukose. Strong regularities in world wide web
surÔ¨Ång. Science, 280:95‚Äì97, 1998.

[10] J. Laherrere and D. Sornette. Stretched exponential

distributions in nature and economy: ‚Äúfat tails‚Äù with
characteristic scales. The European Physical Journal
B, 2:525, 1998.

[11] D. Lambert. Zero-inÔ¨Çated poisson regression, with an

application to defects in manufacturing.
Technometrics, 34:1‚Äì14, 1992.

[12] D. Lord, S. P. Washington, and J. N. Ivan. Poisson,

poisson-gamma and zero-inÔ¨Çated regression models of
motor vehicle crashes: balancing statistical Ô¨Åt and
theory. Accident Analysis and Prevention, 37:35‚Äì46,
2005.

[13] B. Mandelbrot. An informational theory of the

statistical structure of language. In W. Jackson,
editor, Communication Theory. Betterworths, 1953.

[14] S. M. Mwalili, E. LesaÔ¨Äre, and D. Declerck. The

zero-inÔ¨Çated negative binomial regression model with
correction for misclassiÔ¨Åcation: An example in caries
research. Technical Report TR0462, IAP Statistics
Network, 2005.

[15] R Development Core Team. R: A language and

environment for statistical computing. R Foundation
for Statistical Computing, Vienna, Austria, 2004.
ISBN 3-900051-00-3.

[16] J. A. Rice. Mathematical Statistics and Data Analysis.

Wadsworth & Brooks/Cole, 1988.

[17] G. Schwarz. Estimating the dimension of a model. The

Annals of Statistics, 6:461‚Äì464, 1978.

[18] H. A. Simon. On a class of skew distribution

functions. Biometrika, 42:425‚Äì440, 1955.

[19] E. C. Titchmarsh. The Theory of the Riemann Zeta

Function, 2nd ed. Oxford Science Publications,
Clarendon Press, Oxford, 1986.

[20] D. G. Uitenbroek. SISA Pairwise tests.

http://home.clara.net/ sisa/ pairwhlp.htm, 1997.

[21] D. von Seggern. CRC Standard Curves and Surfaces.

CRC Press, 1993.

[22] J. R. Wilson. Logarithmic series distribution and its
use in analyzing discrete data. In Proceedings of the
Survey Research Methods Section, American
Statistical Association, pages 275‚Äì280, 1988.

[23] G. K. Zipf. Human Behaviour and the Principle of

Least-EÔ¨Äort. Addison-Wesley, Cambridge, MA, 1949.

