metrics such as click counts are vital to online businesses but their measurement has been problematic due to inclusion of high variance robot tra c. we posit that by applying statistical methods more rigorous than have been employed to date that we can build a robust model of the distribution of clicks following which we can set probabilistically sound thresholds to address outliers and robots. prior research in this domain has used inappropriate statistical methodology to model distributions and current industrial practice eschews this research for conservative ad-hoc click-level thresholds. prevailing belief is that such distributions are scale-free power law distributions but using more rigorous statistical methods we  nd the best description of the data is instead provided by a scale-sensitive zipf-mandelbrot mixture distribution. our results are based on ten datasets from various verticals in the yahoo domain. since mixture models can over t the data we take care to use the bic log-likelihood method which penalizes overly complex models. using a mixture model in the web activity domain makes sense because there are likely multiple classes of users. in particular, we have noticed that there is a signi cantly large set of  users  that visit the yahoo portal exactly once a day. we surmise these may be robots testing internet connectivity by pinging the yahoo main website. backing up our quantitative analysis is graphical analysis in which empirical distributions are plotted against theoretical distributions in log-log space using robust cumulative distribution plots. this methodology has two advantages plotting in log-log space allows one to visually di erentiate the various exponential distributions and secondly, cumulative plots are much more robust to outliers. we plan to use the results of this work for applications for robot removal from web metrics business intelligence systems.
