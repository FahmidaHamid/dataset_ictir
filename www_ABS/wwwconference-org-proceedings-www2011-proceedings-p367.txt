we investigate the problem of learning to rank for document retrieval from the perspective of learning with multiple objective functions. we present solutions to two open problems in learning to rank  rst, we show how multiple measures can be combined into a single graded measure that can be learned. this solves the problem of learning from a  scorecard  of measures by making such scorecards comparable, and we show results where a standard web relevance measure (ndcg) is used for the top-tier measure, and a relevance measure derived from click data is used for the second-tier measure; the second-tier measure is shown to sig-ni cantly improve while leaving the top-tier measure largely unchanged. second, we note that the learning-to-rank problem can itself be viewed as changing as the ranking model learns for example, early in learning, adjusting the rank of all documents can be advantageous, but later during training, it becomes more desirable to concentrate on correcting the top few documents for each query. we show how an analysis of these problems leads to an improved, iteration-dependent cost function that interpolates between a cost function that is more appropriate for early learning, with one that is more appropriate for late-stage learning. the approach results in a signi cant improvement in accuracy with the same size models. we investigate these ideas using lambdamart, a state-of-the-art ranking algorithm.
