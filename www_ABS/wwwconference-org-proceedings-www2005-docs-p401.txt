search engines are the primary gateways of information access on the web today. behind the scenes, search engines crawl the web to populate a local indexed repository of web pages, used to answer user search queries. in an aggregate sense, the web is very dynamic, causing any repository of web pages to become out of date over time, which in turn causes query answer quality to degrade. given the considerable size, dynamicity, and degree of autonomy of the web as a whole, it is not feasible for a search engine to maintain its repository exactly synchronized with the web. in this paper we study how to schedule web pages for selective (re)downloading into a search engine repository. the scheduling objective is to maximize the quality of the user experience for those who query the search engine. we begin with a quantitative characterization of the way in which the discrepancy between the content of the repository and the current content of the live web impacts the quality of the user experience. this characterization leads to a user-centric metric of the quality of a search engine s local repository. we use this metric to derive a policy for scheduling web page (re)downloading that is driven by search engine usage and free of exterior tuning parameters. we then focus on the important subproblem of scheduling refreshing of web pages already present in the repository, and show how to compute the priorities e ciently. we provide extensive empirical comparisons of our user-centric method against prior web page refresh strategies, using real web data. our results demonstrate that our method requires far fewer resources to maintain same search engine quality level for users, leaving substantially more resources available for incorporating new web pages into the search repository.
