a crucial step in adding structure to unstructured data is to identify references to entities and disambiguate them. such disambiguated references can help enhance readability and draw similarities across different pieces of running text in an automated fashion. previous research has tackled this problem by  rst forming a catalog of entities from a knowledge base, such as wikipedia, and then using this catalog to disambiguate references in unseen text. however, most of the previously proposed models either do not use all text in the knowledge base, potentially missing out on discriminative features, or do not exploit word-entity proximity to learn high-quality catalogs. in this work, we propose topic models that keep track of the context of every word in the knowledge base; so that words appearing within the same context as an entity are more likely to be associated with that entity. thus, our topic models utilize all text present in the knowledge base and help learn high-quality catalogs. our models also learn groups of co-occurring entities thus enabling collective disambiguation. unlike most previous topic models, our models are non-parametric and do not require the user to specify the exact number of groups present in the knowledge base. in experiments performed on an extract of wikipedia containing almost 60,000 references, our models outperform svm-based baselines by as much as 18% in terms of disambiguation accuracy translating to an increment of almost 11,000 correctly disambiguated references.
