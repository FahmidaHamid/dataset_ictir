computing the degree of semantic relatedness of words is a key functionality of many language applications such as search, clustering, and disambiguation. previous approaches to computing semantic relatedness mostly used static language resources, while essentially ignoring their temporal aspects. we believe that a considerable amount of relatedness information can also be found in studying patterns of word usage over time. consider, for instance, a newspaper archive spanning many years. two words such as  war  and  peace  might rarely co-occur in the same articles, yet their patterns of use over time might be similar. in this paper, we propose a new semantic relatedness model, temporal semantic analysis (tsa), which captures this temporal information. the previous state of the art method, explicit semantic analysis (esa), represented word semantics as a vector of concepts. tsa uses a more re ned representation, where each concept is no longer scalar, but is instead represented as time series over a corpus of temporally-ordered documents. to the best of our knowledge, this is the  rst attempt to incorporate temporal evidence into models of semantic relatedness. empirical evaluation shows that tsa provides consistent improvements over the state of the art esa results on multiple benchmarks.
