we describe a system for synchronization and organization of user-contributed content from live music events. we start with a set of short video clips taken at a single event by multiple contributors, who were using a varied set of capture devices. using audio  ngerprints, we synchronize these clips such that overlapping clips can be displayed simultaneously. furthermore, we use the timing and link structure generated by the synchronization algorithm to improve the  ndability and representation of the event content, including identifying key moments of interest and descriptive text for important captured segments of the show. we also identify the preferred audio track when multiple clips overlap. we thus create a much improved representation of the event that builds on the automatic content match. our work demonstrates important principles in the use of content analysis techniques for social media content on the web, and applies those principles in the domain of live music capture.
