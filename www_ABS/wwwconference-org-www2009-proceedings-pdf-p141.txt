there are many online settings in which users publicly express opinions. a number of these offer mechanisms for other users to evaluate these opinions; a canonical example is amazon.com, where reviews come with annotations like  26 of 32 people found the following review helpful.  opinion evaluation appears in many offline settings as well, including market research and political campaigns. reasoning about the evaluation of an opinion is fundamentally different from reasoning about the opinion itself rather than asking,  what did y think of x? , we are asking,  what did z think of y s opinion of x?  here we develop a framework for analyzing and modeling opinion evaluation, using a large-scale collection of amazon book reviews as a dataset. we  nd that the perceived helpfulness of a review depends not just on its content but also but also in subtle ways on how the expressed evaluation relates to other evaluations of the same product. as part of our approach, we develop novel methods that take advantage of the phenomenon of review  plagiarism  to control for the effects of text in opinion evaluation, and we provide a simple and natural mathematical model consistent with our  ndings. our analysis also allows us to distinguish among the predictions of competing theories from sociology and social psychology, and to discover unexpected differences in the collective opinion-evaluation behavior of user populations from different countries.
