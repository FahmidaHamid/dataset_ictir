gradient boosted regression trees (gbrt) are the current state-of-the-art learning paradigm for machine learned web-search ranking   a domain notorious for very large data sets. in this paper, we propose a novel method for par-allelizing the training of gbrt. our technique parallelizes the construction of the individual regression trees and operates using the master-worker paradigm as follows. the data are partitioned among the workers. at each iteration, the worker summarizes its data-partition using histograms. the master processor uses these to build one layer of a regression tree, and then sends this layer to the workers, allowing the workers to build histograms for the next layer. our algorithm carefully orchestrates overlap between communication and computation to achieve good performance. since this approach is based on data partitioning, and requires a small amount of communication, it generalizes to distributed and shared memory machines, as well as clouds. we present experimental results on both shared memory machines and clusters for two large scale web search ranking data sets. we demonstrate that the loss in accuracy induced due to the histogram approximation in the regression tree creation can be compensated for through slightly deeper trees. as a result, we see no signi cant loss in accuracy on the yahoo data sets and a very small reduction in accuracy for the microsoft letor data. in addition, on shared memory machines, we obtain almost perfect linear speedup with up to about 48 cores on the large data sets. on distributed memory machines, we get a speedup of 25 with 32 processors. due to data partitioning our approach can scale to even larger data sets, on which one can reasonably expect even higher speedups.
